{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "6SDvJoPLupUl"
      },
      "outputs": [],
      "source": [
        "#tesi\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random as rnd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import classification_report\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "\n",
        "\n",
        "#caricamento dati\n",
        "data = np.load(\"C:/Users/gabri/Desktop/uni/tesi/csgo_archive/legit/legit.npy\")\n",
        "data_cheater = np.load(\"C:/Users/gabri/Desktop/uni/tesi/csgo_archive/cheaters/cheaters.npy\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2D4jAqBrxlmp",
        "outputId": "7a177793-b5c5-49ec-ed78-175deaa330bc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cpu\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "impementazione LCSMT\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(1000, 10, 96, 5)\n"
          ]
        }
      ],
      "source": [
        "total = np.concatenate((data, data_cheater), axis=0)\n",
        "labels_legit = np.zeros(len(data))\n",
        "labels_cheater=np.ones(len(data_cheater))\n",
        "labels = np.concatenate((labels_legit, labels_cheater))\n",
        "\n",
        "#reshape dei dati per la SVM\n",
        "X_train, X_test, y_train, y_test = train_test_split(total, labels, test_size=0.20, random_state=42, stratify=labels)\n",
        "\n",
        "#rescaling dei dati\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = X_train\n",
        "X_test_scaled = X_test\n",
        "\n",
        "X_train_small = X_train_scaled[:1000, :10, 96:192]\n",
        "y_train_small = y_train[:1000]\n",
        "X_test_small = X_test_scaled[:200, :10, 96:192]\n",
        "y_test_small = y_test[:200]\n",
        "\n",
        "print(X_train_small.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "cnn implementazione"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32).permute(0, 3, 1, 2)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).to(device)\n",
        "X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32).permute(0, 3, 1, 2)\n",
        "y_test_tensor = torch.tensor(y_test, dtype=torch.float32).to(device)\n",
        "\n",
        "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/100], Step [10/150], Loss: 14.4392\n",
            "Epoch [1/100], Step [20/150], Loss: 16.2040\n",
            "Epoch [1/100], Step [30/150], Loss: 16.8964\n",
            "Epoch [1/100], Step [40/150], Loss: 15.9535\n",
            "Epoch [1/100], Step [50/150], Loss: 16.4191\n",
            "Epoch [1/100], Step [60/150], Loss: 16.7294\n",
            "Epoch [1/100], Step [70/150], Loss: 16.7056\n",
            "Epoch [1/100], Step [80/150], Loss: 16.8244\n",
            "Epoch [1/100], Step [90/150], Loss: 16.7606\n",
            "Epoch [1/100], Step [100/150], Loss: 16.6470\n",
            "Epoch [1/100], Step [110/150], Loss: 16.7104\n",
            "Epoch [1/100], Step [120/150], Loss: 16.6590\n",
            "Epoch [1/100], Step [130/150], Loss: 16.5674\n",
            "Epoch [1/100], Step [140/150], Loss: 16.4890\n",
            "Epoch [1/100], Step [150/150], Loss: 16.5355\n",
            "Epoch [1/100], Loss: 16.5355\n",
            "Test Accuracy: 83.33%\n",
            "Epoch [2/100], Step [10/150], Loss: 17.8125\n",
            "Epoch [2/100], Step [20/150], Loss: 16.9531\n",
            "Epoch [2/100], Step [30/150], Loss: 16.7708\n",
            "Epoch [2/100], Step [40/150], Loss: 16.4844\n",
            "Epoch [2/100], Step [50/150], Loss: 16.6250\n",
            "Epoch [2/100], Step [60/150], Loss: 16.3542\n",
            "Epoch [2/100], Step [70/150], Loss: 16.2277\n",
            "Epoch [2/100], Step [80/150], Loss: 16.3086\n",
            "Epoch [2/100], Step [90/150], Loss: 16.2326\n",
            "Epoch [2/100], Step [100/150], Loss: 16.2500\n",
            "Epoch [2/100], Step [110/150], Loss: 16.3636\n",
            "Epoch [2/100], Step [120/150], Loss: 16.4714\n",
            "Epoch [2/100], Step [130/150], Loss: 16.4303\n",
            "Epoch [2/100], Step [140/150], Loss: 16.5402\n",
            "Epoch [2/100], Step [150/150], Loss: 16.6667\n",
            "Epoch [2/100], Loss: 16.6667\n",
            "Test Accuracy: 83.33%\n",
            "Epoch [3/100], Step [10/150], Loss: 15.3125\n",
            "Epoch [3/100], Step [20/150], Loss: 16.4062\n",
            "Epoch [3/100], Step [30/150], Loss: 16.5625\n",
            "Epoch [3/100], Step [40/150], Loss: 16.9531\n",
            "Epoch [3/100], Step [50/150], Loss: 16.8438\n",
            "Epoch [3/100], Step [60/150], Loss: 16.8490\n",
            "Epoch [3/100], Step [70/150], Loss: 16.3839\n",
            "Epoch [3/100], Step [80/150], Loss: 16.4648\n",
            "Epoch [3/100], Step [90/150], Loss: 16.4410\n",
            "Epoch [3/100], Step [100/150], Loss: 16.5312\n",
            "Epoch [3/100], Step [110/150], Loss: 16.5341\n",
            "Epoch [3/100], Step [120/150], Loss: 16.5234\n",
            "Epoch [3/100], Step [130/150], Loss: 16.7067\n",
            "Epoch [3/100], Step [140/150], Loss: 16.6853\n",
            "Epoch [3/100], Step [150/150], Loss: 16.6667\n",
            "Epoch [3/100], Loss: 16.6667\n",
            "Test Accuracy: 83.33%\n",
            "Epoch [4/100], Step [10/150], Loss: 18.2812\n",
            "Epoch [4/100], Step [20/150], Loss: 17.1875\n",
            "Epoch [4/100], Step [30/150], Loss: 16.8750\n",
            "Epoch [4/100], Step [40/150], Loss: 17.2656\n",
            "Epoch [4/100], Step [50/150], Loss: 17.0938\n",
            "Epoch [4/100], Step [60/150], Loss: 17.0052\n",
            "Epoch [4/100], Step [70/150], Loss: 16.8304\n",
            "Epoch [4/100], Step [80/150], Loss: 16.9336\n",
            "Epoch [4/100], Step [90/150], Loss: 16.9792\n",
            "Epoch [4/100], Step [100/150], Loss: 17.1562\n",
            "Epoch [4/100], Step [110/150], Loss: 17.1023\n",
            "Epoch [4/100], Step [120/150], Loss: 16.9401\n",
            "Epoch [4/100], Step [130/150], Loss: 16.9231\n",
            "Epoch [4/100], Step [140/150], Loss: 16.9308\n",
            "Epoch [4/100], Step [150/150], Loss: 16.6667\n",
            "Epoch [4/100], Loss: 16.6667\n",
            "Test Accuracy: 83.33%\n",
            "Epoch [5/100], Step [10/150], Loss: 16.2500\n",
            "Epoch [5/100], Step [20/150], Loss: 16.4844\n",
            "Epoch [5/100], Step [30/150], Loss: 17.0833\n",
            "Epoch [5/100], Step [40/150], Loss: 16.5234\n",
            "Epoch [5/100], Step [50/150], Loss: 16.2812\n",
            "Epoch [5/100], Step [60/150], Loss: 16.2760\n",
            "Epoch [5/100], Step [70/150], Loss: 16.4955\n",
            "Epoch [5/100], Step [80/150], Loss: 16.5625\n",
            "Epoch [5/100], Step [90/150], Loss: 16.6667\n",
            "Epoch [5/100], Step [100/150], Loss: 16.6875\n",
            "Epoch [5/100], Step [110/150], Loss: 16.6193\n",
            "Epoch [5/100], Step [120/150], Loss: 16.6406\n",
            "Epoch [5/100], Step [130/150], Loss: 16.5385\n",
            "Epoch [5/100], Step [140/150], Loss: 16.6518\n",
            "Epoch [5/100], Step [150/150], Loss: 16.6667\n",
            "Epoch [5/100], Loss: 16.6667\n",
            "Test Accuracy: 83.33%\n",
            "Epoch [6/100], Step [10/150], Loss: 14.3750\n",
            "Epoch [6/100], Step [20/150], Loss: 15.7812\n",
            "Epoch [6/100], Step [30/150], Loss: 16.0938\n",
            "Epoch [6/100], Step [40/150], Loss: 16.2891\n",
            "Epoch [6/100], Step [50/150], Loss: 16.8125\n",
            "Epoch [6/100], Step [60/150], Loss: 16.8229\n",
            "Epoch [6/100], Step [70/150], Loss: 16.6964\n",
            "Epoch [6/100], Step [80/150], Loss: 16.7188\n",
            "Epoch [6/100], Step [90/150], Loss: 16.5799\n",
            "Epoch [6/100], Step [100/150], Loss: 16.6562\n",
            "Epoch [6/100], Step [110/150], Loss: 16.6193\n",
            "Epoch [6/100], Step [120/150], Loss: 16.7057\n",
            "Epoch [6/100], Step [130/150], Loss: 16.7188\n",
            "Epoch [6/100], Step [140/150], Loss: 16.4621\n",
            "Epoch [6/100], Step [150/150], Loss: 16.6667\n",
            "Epoch [6/100], Loss: 16.6667\n",
            "Test Accuracy: 83.33%\n",
            "Epoch [7/100], Step [10/150], Loss: 13.4375\n",
            "Epoch [7/100], Step [20/150], Loss: 13.9844\n",
            "Epoch [7/100], Step [30/150], Loss: 14.5833\n",
            "Epoch [7/100], Step [40/150], Loss: 15.0000\n",
            "Epoch [7/100], Step [50/150], Loss: 15.7812\n",
            "Epoch [7/100], Step [60/150], Loss: 16.1458\n",
            "Epoch [7/100], Step [70/150], Loss: 16.2054\n",
            "Epoch [7/100], Step [80/150], Loss: 16.5820\n",
            "Epoch [7/100], Step [90/150], Loss: 16.5972\n",
            "Epoch [7/100], Step [100/150], Loss: 16.5000\n",
            "Epoch [7/100], Step [110/150], Loss: 16.6051\n",
            "Epoch [7/100], Step [120/150], Loss: 16.6016\n",
            "Epoch [7/100], Step [130/150], Loss: 16.5986\n",
            "Epoch [7/100], Step [140/150], Loss: 16.5960\n",
            "Epoch [7/100], Step [150/150], Loss: 16.6667\n",
            "Epoch [7/100], Loss: 16.6667\n",
            "Test Accuracy: 83.33%\n",
            "Epoch [8/100], Step [10/150], Loss: 18.2812\n",
            "Epoch [8/100], Step [20/150], Loss: 17.8125\n",
            "Epoch [8/100], Step [30/150], Loss: 17.6562\n",
            "Epoch [8/100], Step [40/150], Loss: 17.4219\n",
            "Epoch [8/100], Step [50/150], Loss: 17.0625\n",
            "Epoch [8/100], Step [60/150], Loss: 17.3177\n",
            "Epoch [8/100], Step [70/150], Loss: 17.3661\n",
            "Epoch [8/100], Step [80/150], Loss: 17.0117\n",
            "Epoch [8/100], Step [90/150], Loss: 17.1528\n",
            "Epoch [8/100], Step [100/150], Loss: 17.1406\n",
            "Epoch [8/100], Step [110/150], Loss: 16.9034\n",
            "Epoch [8/100], Step [120/150], Loss: 16.8750\n",
            "Epoch [8/100], Step [130/150], Loss: 16.7909\n",
            "Epoch [8/100], Step [140/150], Loss: 16.8304\n",
            "Epoch [8/100], Step [150/150], Loss: 16.6667\n",
            "Epoch [8/100], Loss: 16.6667\n",
            "Test Accuracy: 83.33%\n",
            "Epoch [9/100], Step [10/150], Loss: 15.6250\n",
            "Epoch [9/100], Step [20/150], Loss: 16.1719\n",
            "Epoch [9/100], Step [30/150], Loss: 15.5729\n",
            "Epoch [9/100], Step [40/150], Loss: 16.3672\n",
            "Epoch [9/100], Step [50/150], Loss: 16.0000\n",
            "Epoch [9/100], Step [60/150], Loss: 15.9115\n",
            "Epoch [9/100], Step [70/150], Loss: 15.9152\n",
            "Epoch [9/100], Step [80/150], Loss: 16.0547\n",
            "Epoch [9/100], Step [90/150], Loss: 16.1632\n",
            "Epoch [9/100], Step [100/150], Loss: 16.1562\n",
            "Epoch [9/100], Step [110/150], Loss: 16.1364\n",
            "Epoch [9/100], Step [120/150], Loss: 16.4193\n",
            "Epoch [9/100], Step [130/150], Loss: 16.4663\n",
            "Epoch [9/100], Step [140/150], Loss: 16.6629\n",
            "Epoch [9/100], Step [150/150], Loss: 16.6667\n",
            "Epoch [9/100], Loss: 16.6667\n",
            "Test Accuracy: 83.33%\n",
            "Epoch [10/100], Step [10/150], Loss: 18.4375\n",
            "Epoch [10/100], Step [20/150], Loss: 17.2656\n",
            "Epoch [10/100], Step [30/150], Loss: 17.5521\n",
            "Epoch [10/100], Step [40/150], Loss: 17.4609\n",
            "Epoch [10/100], Step [50/150], Loss: 17.4375\n",
            "Epoch [10/100], Step [60/150], Loss: 17.2656\n",
            "Epoch [10/100], Step [70/150], Loss: 17.2098\n",
            "Epoch [10/100], Step [80/150], Loss: 16.9141\n",
            "Epoch [10/100], Step [90/150], Loss: 16.9792\n",
            "Epoch [10/100], Step [100/150], Loss: 16.9375\n",
            "Epoch [10/100], Step [110/150], Loss: 16.7188\n",
            "Epoch [10/100], Step [120/150], Loss: 16.8359\n",
            "Epoch [10/100], Step [130/150], Loss: 16.7548\n",
            "Epoch [10/100], Step [140/150], Loss: 16.6406\n",
            "Epoch [10/100], Step [150/150], Loss: 16.6667\n",
            "Epoch [10/100], Loss: 16.6667\n",
            "Test Accuracy: 83.33%\n",
            "Epoch [11/100], Step [10/150], Loss: 15.3125\n",
            "Epoch [11/100], Step [20/150], Loss: 17.1094\n",
            "Epoch [11/100], Step [30/150], Loss: 16.5104\n",
            "Epoch [11/100], Step [40/150], Loss: 16.5625\n",
            "Epoch [11/100], Step [50/150], Loss: 16.4062\n",
            "Epoch [11/100], Step [60/150], Loss: 16.2240\n",
            "Epoch [11/100], Step [70/150], Loss: 16.2946\n",
            "Epoch [11/100], Step [80/150], Loss: 16.1914\n",
            "Epoch [11/100], Step [90/150], Loss: 16.2847\n",
            "Epoch [11/100], Step [100/150], Loss: 16.2188\n",
            "Epoch [11/100], Step [110/150], Loss: 16.4489\n",
            "Epoch [11/100], Step [120/150], Loss: 16.4062\n",
            "Epoch [11/100], Step [130/150], Loss: 16.4423\n",
            "Epoch [11/100], Step [140/150], Loss: 16.5737\n",
            "Epoch [11/100], Step [150/150], Loss: 16.6667\n",
            "Epoch [11/100], Loss: 16.6667\n",
            "Test Accuracy: 83.33%\n",
            "Epoch [12/100], Step [10/150], Loss: 16.7188\n",
            "Epoch [12/100], Step [20/150], Loss: 16.6406\n",
            "Epoch [12/100], Step [30/150], Loss: 16.7188\n",
            "Epoch [12/100], Step [40/150], Loss: 17.1875\n",
            "Epoch [12/100], Step [50/150], Loss: 17.7812\n",
            "Epoch [12/100], Step [60/150], Loss: 17.2656\n",
            "Epoch [12/100], Step [70/150], Loss: 17.2991\n",
            "Epoch [12/100], Step [80/150], Loss: 17.2461\n",
            "Epoch [12/100], Step [90/150], Loss: 17.2049\n",
            "Epoch [12/100], Step [100/150], Loss: 17.2656\n",
            "Epoch [12/100], Step [110/150], Loss: 16.9744\n",
            "Epoch [12/100], Step [120/150], Loss: 17.0703\n",
            "Epoch [12/100], Step [130/150], Loss: 16.9591\n",
            "Epoch [12/100], Step [140/150], Loss: 16.8638\n",
            "Epoch [12/100], Step [150/150], Loss: 16.6667\n",
            "Epoch [12/100], Loss: 16.6667\n",
            "Test Accuracy: 83.33%\n",
            "Epoch [13/100], Step [10/150], Loss: 15.3125\n",
            "Epoch [13/100], Step [20/150], Loss: 17.1094\n",
            "Epoch [13/100], Step [30/150], Loss: 18.1250\n",
            "Epoch [13/100], Step [40/150], Loss: 17.3438\n",
            "Epoch [13/100], Step [50/150], Loss: 16.9062\n",
            "Epoch [13/100], Step [60/150], Loss: 17.0573\n",
            "Epoch [13/100], Step [70/150], Loss: 16.9420\n",
            "Epoch [13/100], Step [80/150], Loss: 16.6406\n",
            "Epoch [13/100], Step [90/150], Loss: 16.6319\n",
            "Epoch [13/100], Step [100/150], Loss: 16.6094\n",
            "Epoch [13/100], Step [110/150], Loss: 16.8182\n",
            "Epoch [13/100], Step [120/150], Loss: 16.6536\n",
            "Epoch [13/100], Step [130/150], Loss: 16.5745\n",
            "Epoch [13/100], Step [140/150], Loss: 16.6853\n",
            "Epoch [13/100], Step [150/150], Loss: 16.6667\n",
            "Epoch [13/100], Loss: 16.6667\n",
            "Test Accuracy: 83.33%\n",
            "Epoch [14/100], Step [10/150], Loss: 17.0312\n",
            "Epoch [14/100], Step [20/150], Loss: 16.5625\n",
            "Epoch [14/100], Step [30/150], Loss: 16.6667\n",
            "Epoch [14/100], Step [40/150], Loss: 16.7188\n",
            "Epoch [14/100], Step [50/150], Loss: 17.0000\n",
            "Epoch [14/100], Step [60/150], Loss: 16.7188\n",
            "Epoch [14/100], Step [70/150], Loss: 17.0536\n",
            "Epoch [14/100], Step [80/150], Loss: 16.8164\n",
            "Epoch [14/100], Step [90/150], Loss: 16.8403\n",
            "Epoch [14/100], Step [100/150], Loss: 16.6562\n",
            "Epoch [14/100], Step [110/150], Loss: 16.7756\n",
            "Epoch [14/100], Step [120/150], Loss: 16.6927\n",
            "Epoch [14/100], Step [130/150], Loss: 16.6827\n",
            "Epoch [14/100], Step [140/150], Loss: 16.6964\n",
            "Epoch [14/100], Step [150/150], Loss: 16.6667\n",
            "Epoch [14/100], Loss: 16.6667\n",
            "Test Accuracy: 83.33%\n",
            "Epoch [15/100], Step [10/150], Loss: 17.1875\n",
            "Epoch [15/100], Step [20/150], Loss: 15.3125\n",
            "Epoch [15/100], Step [30/150], Loss: 15.8854\n",
            "Epoch [15/100], Step [40/150], Loss: 16.2109\n",
            "Epoch [15/100], Step [50/150], Loss: 16.0000\n",
            "Epoch [15/100], Step [60/150], Loss: 15.9635\n",
            "Epoch [15/100], Step [70/150], Loss: 16.2723\n",
            "Epoch [15/100], Step [80/150], Loss: 16.3867\n",
            "Epoch [15/100], Step [90/150], Loss: 16.5972\n",
            "Epoch [15/100], Step [100/150], Loss: 16.5781\n",
            "Epoch [15/100], Step [110/150], Loss: 16.6903\n",
            "Epoch [15/100], Step [120/150], Loss: 16.7448\n",
            "Epoch [15/100], Step [130/150], Loss: 16.5625\n",
            "Epoch [15/100], Step [140/150], Loss: 16.5848\n",
            "Epoch [15/100], Step [150/150], Loss: 16.6667\n",
            "Epoch [15/100], Loss: 16.6667\n",
            "Test Accuracy: 83.33%\n",
            "Epoch [16/100], Step [10/150], Loss: 16.5625\n",
            "Epoch [16/100], Step [20/150], Loss: 16.0938\n",
            "Epoch [16/100], Step [30/150], Loss: 16.0417\n",
            "Epoch [16/100], Step [40/150], Loss: 16.0547\n",
            "Epoch [16/100], Step [50/150], Loss: 16.5625\n",
            "Epoch [16/100], Step [60/150], Loss: 16.7188\n",
            "Epoch [16/100], Step [70/150], Loss: 16.8973\n",
            "Epoch [16/100], Step [80/150], Loss: 16.8164\n",
            "Epoch [16/100], Step [90/150], Loss: 16.7361\n",
            "Epoch [16/100], Step [100/150], Loss: 16.5156\n",
            "Epoch [16/100], Step [110/150], Loss: 16.4205\n",
            "Epoch [16/100], Step [120/150], Loss: 16.4453\n",
            "Epoch [16/100], Step [130/150], Loss: 16.6707\n",
            "Epoch [16/100], Step [140/150], Loss: 16.7411\n",
            "Epoch [16/100], Step [150/150], Loss: 16.6667\n",
            "Epoch [16/100], Loss: 16.6667\n",
            "Test Accuracy: 83.33%\n",
            "Epoch [17/100], Step [10/150], Loss: 17.8125\n",
            "Epoch [17/100], Step [20/150], Loss: 16.4844\n",
            "Epoch [17/100], Step [30/150], Loss: 16.5104\n",
            "Epoch [17/100], Step [40/150], Loss: 16.2500\n",
            "Epoch [17/100], Step [50/150], Loss: 16.0938\n",
            "Epoch [17/100], Step [60/150], Loss: 16.0677\n",
            "Epoch [17/100], Step [70/150], Loss: 16.2723\n",
            "Epoch [17/100], Step [80/150], Loss: 16.3086\n",
            "Epoch [17/100], Step [90/150], Loss: 16.4236\n",
            "Epoch [17/100], Step [100/150], Loss: 16.5312\n",
            "Epoch [17/100], Step [110/150], Loss: 16.5057\n",
            "Epoch [17/100], Step [120/150], Loss: 16.5495\n",
            "Epoch [17/100], Step [130/150], Loss: 16.6947\n",
            "Epoch [17/100], Step [140/150], Loss: 16.7299\n",
            "Epoch [17/100], Step [150/150], Loss: 16.6667\n",
            "Epoch [17/100], Loss: 16.6667\n",
            "Test Accuracy: 83.33%\n",
            "Epoch [18/100], Step [10/150], Loss: 14.5312\n",
            "Epoch [18/100], Step [20/150], Loss: 15.0781\n",
            "Epoch [18/100], Step [30/150], Loss: 15.2604\n",
            "Epoch [18/100], Step [40/150], Loss: 15.5078\n",
            "Epoch [18/100], Step [50/150], Loss: 16.0938\n",
            "Epoch [18/100], Step [60/150], Loss: 16.3802\n",
            "Epoch [18/100], Step [70/150], Loss: 16.5848\n",
            "Epoch [18/100], Step [80/150], Loss: 16.5430\n",
            "Epoch [18/100], Step [90/150], Loss: 16.8056\n",
            "Epoch [18/100], Step [100/150], Loss: 16.8438\n",
            "Epoch [18/100], Step [110/150], Loss: 16.8324\n",
            "Epoch [18/100], Step [120/150], Loss: 16.6927\n",
            "Epoch [18/100], Step [130/150], Loss: 16.6587\n",
            "Epoch [18/100], Step [140/150], Loss: 16.7411\n",
            "Epoch [18/100], Step [150/150], Loss: 16.6667\n",
            "Epoch [18/100], Loss: 16.6667\n",
            "Test Accuracy: 83.33%\n",
            "Epoch [19/100], Step [10/150], Loss: 17.9688\n",
            "Epoch [19/100], Step [20/150], Loss: 17.1875\n",
            "Epoch [19/100], Step [30/150], Loss: 17.1875\n",
            "Epoch [19/100], Step [40/150], Loss: 16.6406\n",
            "Epoch [19/100], Step [50/150], Loss: 16.8125\n",
            "Epoch [19/100], Step [60/150], Loss: 16.4844\n",
            "Epoch [19/100], Step [70/150], Loss: 16.4955\n",
            "Epoch [19/100], Step [80/150], Loss: 16.5430\n",
            "Epoch [19/100], Step [90/150], Loss: 16.7361\n",
            "Epoch [19/100], Step [100/150], Loss: 16.7031\n",
            "Epoch [19/100], Step [110/150], Loss: 16.6335\n",
            "Epoch [19/100], Step [120/150], Loss: 16.6016\n",
            "Epoch [19/100], Step [130/150], Loss: 16.5264\n",
            "Epoch [19/100], Step [140/150], Loss: 16.5513\n",
            "Epoch [19/100], Step [150/150], Loss: 16.6667\n",
            "Epoch [19/100], Loss: 16.6667\n",
            "Test Accuracy: 83.33%\n",
            "Epoch [20/100], Step [10/150], Loss: 17.5000\n",
            "Epoch [20/100], Step [20/150], Loss: 17.2656\n",
            "Epoch [20/100], Step [30/150], Loss: 17.0312\n",
            "Epoch [20/100], Step [40/150], Loss: 17.6172\n",
            "Epoch [20/100], Step [50/150], Loss: 17.0312\n",
            "Epoch [20/100], Step [60/150], Loss: 16.9792\n",
            "Epoch [20/100], Step [70/150], Loss: 16.8527\n",
            "Epoch [20/100], Step [80/150], Loss: 16.7773\n",
            "Epoch [20/100], Step [90/150], Loss: 16.7014\n",
            "Epoch [20/100], Step [100/150], Loss: 16.7188\n",
            "Epoch [20/100], Step [110/150], Loss: 16.7756\n",
            "Epoch [20/100], Step [120/150], Loss: 16.8099\n",
            "Epoch [20/100], Step [130/150], Loss: 16.7067\n",
            "Epoch [20/100], Step [140/150], Loss: 16.6071\n",
            "Epoch [20/100], Step [150/150], Loss: 16.6667\n",
            "Epoch [20/100], Loss: 16.6667\n",
            "Test Accuracy: 83.33%\n",
            "Epoch [21/100], Step [10/150], Loss: 15.9375\n",
            "Epoch [21/100], Step [20/150], Loss: 16.1719\n",
            "Epoch [21/100], Step [30/150], Loss: 16.2500\n",
            "Epoch [21/100], Step [40/150], Loss: 16.6016\n",
            "Epoch [21/100], Step [50/150], Loss: 16.3750\n",
            "Epoch [21/100], Step [60/150], Loss: 16.5885\n",
            "Epoch [21/100], Step [70/150], Loss: 16.5625\n",
            "Epoch [21/100], Step [80/150], Loss: 16.5039\n",
            "Epoch [21/100], Step [90/150], Loss: 16.6319\n",
            "Epoch [21/100], Step [100/150], Loss: 16.5000\n",
            "Epoch [21/100], Step [110/150], Loss: 16.6051\n",
            "Epoch [21/100], Step [120/150], Loss: 16.6406\n",
            "Epoch [21/100], Step [130/150], Loss: 16.6346\n",
            "Epoch [21/100], Step [140/150], Loss: 16.6071\n",
            "Epoch [21/100], Step [150/150], Loss: 16.6667\n",
            "Epoch [21/100], Loss: 16.6667\n",
            "Test Accuracy: 83.33%\n",
            "Epoch [22/100], Step [10/150], Loss: 16.8750\n",
            "Epoch [22/100], Step [20/150], Loss: 15.6250\n",
            "Epoch [22/100], Step [30/150], Loss: 16.1979\n",
            "Epoch [22/100], Step [40/150], Loss: 16.0547\n",
            "Epoch [22/100], Step [50/150], Loss: 16.1250\n",
            "Epoch [22/100], Step [60/150], Loss: 16.3281\n",
            "Epoch [22/100], Step [70/150], Loss: 16.2723\n",
            "Epoch [22/100], Step [80/150], Loss: 16.3867\n",
            "Epoch [22/100], Step [90/150], Loss: 16.6146\n",
            "Epoch [22/100], Step [100/150], Loss: 16.7031\n",
            "Epoch [22/100], Step [110/150], Loss: 16.6903\n",
            "Epoch [22/100], Step [120/150], Loss: 16.7839\n",
            "Epoch [22/100], Step [130/150], Loss: 16.6466\n",
            "Epoch [22/100], Step [140/150], Loss: 16.7188\n",
            "Epoch [22/100], Step [150/150], Loss: 16.6667\n",
            "Epoch [22/100], Loss: 16.6667\n",
            "Test Accuracy: 83.33%\n",
            "Epoch [23/100], Step [10/150], Loss: 20.6250\n",
            "Epoch [23/100], Step [20/150], Loss: 18.3594\n",
            "Epoch [23/100], Step [30/150], Loss: 17.7083\n",
            "Epoch [23/100], Step [40/150], Loss: 16.6016\n",
            "Epoch [23/100], Step [50/150], Loss: 16.9375\n",
            "Epoch [23/100], Step [60/150], Loss: 16.7969\n",
            "Epoch [23/100], Step [70/150], Loss: 16.8080\n",
            "Epoch [23/100], Step [80/150], Loss: 16.7969\n",
            "Epoch [23/100], Step [90/150], Loss: 16.6667\n",
            "Epoch [23/100], Step [100/150], Loss: 16.5625\n",
            "Epoch [23/100], Step [110/150], Loss: 16.6477\n",
            "Epoch [23/100], Step [120/150], Loss: 16.7188\n",
            "Epoch [23/100], Step [130/150], Loss: 16.7067\n",
            "Epoch [23/100], Step [140/150], Loss: 16.7076\n",
            "Epoch [23/100], Step [150/150], Loss: 16.6667\n",
            "Epoch [23/100], Loss: 16.6667\n",
            "Test Accuracy: 83.33%\n",
            "Epoch [24/100], Step [10/150], Loss: 18.7500\n",
            "Epoch [24/100], Step [20/150], Loss: 17.4219\n",
            "Epoch [24/100], Step [30/150], Loss: 16.3542\n",
            "Epoch [24/100], Step [40/150], Loss: 16.5625\n",
            "Epoch [24/100], Step [50/150], Loss: 16.6562\n",
            "Epoch [24/100], Step [60/150], Loss: 16.4323\n",
            "Epoch [24/100], Step [70/150], Loss: 16.3839\n",
            "Epoch [24/100], Step [80/150], Loss: 16.2305\n",
            "Epoch [24/100], Step [90/150], Loss: 16.5799\n",
            "Epoch [24/100], Step [100/150], Loss: 16.7344\n",
            "Epoch [24/100], Step [110/150], Loss: 16.6477\n",
            "Epoch [24/100], Step [120/150], Loss: 16.6797\n",
            "Epoch [24/100], Step [130/150], Loss: 16.7428\n",
            "Epoch [24/100], Step [140/150], Loss: 16.8750\n",
            "Epoch [24/100], Step [150/150], Loss: 16.6667\n",
            "Epoch [24/100], Loss: 16.6667\n",
            "Test Accuracy: 83.33%\n",
            "Epoch [25/100], Step [10/150], Loss: 17.9688\n",
            "Epoch [25/100], Step [20/150], Loss: 17.1875\n",
            "Epoch [25/100], Step [30/150], Loss: 17.3958\n",
            "Epoch [25/100], Step [40/150], Loss: 17.2266\n",
            "Epoch [25/100], Step [50/150], Loss: 16.8750\n",
            "Epoch [25/100], Step [60/150], Loss: 17.0833\n",
            "Epoch [25/100], Step [70/150], Loss: 17.1652\n",
            "Epoch [25/100], Step [80/150], Loss: 16.9922\n",
            "Epoch [25/100], Step [90/150], Loss: 16.9792\n",
            "Epoch [25/100], Step [100/150], Loss: 17.0469\n",
            "Epoch [25/100], Step [110/150], Loss: 16.9176\n",
            "Epoch [25/100], Step [120/150], Loss: 16.7188\n",
            "Epoch [25/100], Step [130/150], Loss: 16.6587\n",
            "Epoch [25/100], Step [140/150], Loss: 16.5960\n",
            "Epoch [25/100], Step [150/150], Loss: 16.6667\n",
            "Epoch [25/100], Loss: 16.6667\n",
            "Test Accuracy: 83.33%\n",
            "Epoch [26/100], Step [10/150], Loss: 16.8750\n",
            "Epoch [26/100], Step [20/150], Loss: 16.8750\n",
            "Epoch [26/100], Step [30/150], Loss: 15.6771\n",
            "Epoch [26/100], Step [40/150], Loss: 16.2109\n",
            "Epoch [26/100], Step [50/150], Loss: 16.0938\n",
            "Epoch [26/100], Step [60/150], Loss: 16.1979\n",
            "Epoch [26/100], Step [70/150], Loss: 16.2500\n",
            "Epoch [26/100], Step [80/150], Loss: 16.0352\n",
            "Epoch [26/100], Step [90/150], Loss: 16.3194\n",
            "Epoch [26/100], Step [100/150], Loss: 16.4531\n",
            "Epoch [26/100], Step [110/150], Loss: 16.7472\n",
            "Epoch [26/100], Step [120/150], Loss: 16.7969\n",
            "Epoch [26/100], Step [130/150], Loss: 16.5144\n",
            "Epoch [26/100], Step [140/150], Loss: 16.4062\n",
            "Epoch [26/100], Step [150/150], Loss: 16.6667\n",
            "Epoch [26/100], Loss: 16.6667\n",
            "Test Accuracy: 83.33%\n",
            "Epoch [27/100], Step [10/150], Loss: 15.6250\n",
            "Epoch [27/100], Step [20/150], Loss: 14.5312\n",
            "Epoch [27/100], Step [30/150], Loss: 15.6771\n",
            "Epoch [27/100], Step [40/150], Loss: 15.8594\n",
            "Epoch [27/100], Step [50/150], Loss: 16.2500\n",
            "Epoch [27/100], Step [60/150], Loss: 16.1979\n",
            "Epoch [27/100], Step [70/150], Loss: 15.9821\n",
            "Epoch [27/100], Step [80/150], Loss: 16.2500\n",
            "Epoch [27/100], Step [90/150], Loss: 16.3368\n",
            "Epoch [27/100], Step [100/150], Loss: 16.3594\n",
            "Epoch [27/100], Step [110/150], Loss: 16.4631\n",
            "Epoch [27/100], Step [120/150], Loss: 16.4453\n",
            "Epoch [27/100], Step [130/150], Loss: 16.6346\n",
            "Epoch [27/100], Step [140/150], Loss: 16.7746\n",
            "Epoch [27/100], Step [150/150], Loss: 16.6667\n",
            "Epoch [27/100], Loss: 16.6667\n",
            "Test Accuracy: 83.33%\n",
            "Epoch [28/100], Step [10/150], Loss: 14.3750\n",
            "Epoch [28/100], Step [20/150], Loss: 15.3906\n",
            "Epoch [28/100], Step [30/150], Loss: 15.7812\n",
            "Epoch [28/100], Step [40/150], Loss: 15.9375\n",
            "Epoch [28/100], Step [50/150], Loss: 15.9688\n",
            "Epoch [28/100], Step [60/150], Loss: 16.0938\n",
            "Epoch [28/100], Step [70/150], Loss: 16.2277\n",
            "Epoch [28/100], Step [80/150], Loss: 16.4062\n",
            "Epoch [28/100], Step [90/150], Loss: 16.5799\n",
            "Epoch [28/100], Step [100/150], Loss: 16.8438\n",
            "Epoch [28/100], Step [110/150], Loss: 16.7472\n",
            "Epoch [28/100], Step [120/150], Loss: 16.7057\n",
            "Epoch [28/100], Step [130/150], Loss: 16.6346\n",
            "Epoch [28/100], Step [140/150], Loss: 16.5960\n",
            "Epoch [28/100], Step [150/150], Loss: 16.6667\n",
            "Epoch [28/100], Loss: 16.6667\n",
            "Test Accuracy: 83.33%\n",
            "Epoch [29/100], Step [10/150], Loss: 17.9688\n",
            "Epoch [29/100], Step [20/150], Loss: 18.3594\n",
            "Epoch [29/100], Step [30/150], Loss: 18.3333\n",
            "Epoch [29/100], Step [40/150], Loss: 18.4766\n",
            "Epoch [29/100], Step [50/150], Loss: 18.1250\n",
            "Epoch [29/100], Step [60/150], Loss: 17.9688\n",
            "Epoch [29/100], Step [70/150], Loss: 17.9018\n",
            "Epoch [29/100], Step [80/150], Loss: 17.8125\n",
            "Epoch [29/100], Step [90/150], Loss: 17.3958\n",
            "Epoch [29/100], Step [100/150], Loss: 17.2812\n",
            "Epoch [29/100], Step [110/150], Loss: 17.3864\n",
            "Epoch [29/100], Step [120/150], Loss: 17.0443\n",
            "Epoch [29/100], Step [130/150], Loss: 16.9952\n",
            "Epoch [29/100], Step [140/150], Loss: 16.7411\n",
            "Epoch [29/100], Step [150/150], Loss: 16.6667\n",
            "Epoch [29/100], Loss: 16.6667\n",
            "Test Accuracy: 83.33%\n",
            "Epoch [30/100], Step [10/150], Loss: 17.6562\n",
            "Epoch [30/100], Step [20/150], Loss: 16.5625\n",
            "Epoch [30/100], Step [30/150], Loss: 15.2604\n",
            "Epoch [30/100], Step [40/150], Loss: 15.9375\n",
            "Epoch [30/100], Step [50/150], Loss: 15.8125\n",
            "Epoch [30/100], Step [60/150], Loss: 15.8854\n",
            "Epoch [30/100], Step [70/150], Loss: 15.7143\n",
            "Epoch [30/100], Step [80/150], Loss: 15.8789\n",
            "Epoch [30/100], Step [90/150], Loss: 16.1806\n",
            "Epoch [30/100], Step [100/150], Loss: 16.3281\n",
            "Epoch [30/100], Step [110/150], Loss: 16.4205\n",
            "Epoch [30/100], Step [120/150], Loss: 16.4844\n",
            "Epoch [30/100], Step [130/150], Loss: 16.4784\n",
            "Epoch [30/100], Step [140/150], Loss: 16.4286\n",
            "Epoch [30/100], Step [150/150], Loss: 16.6667\n",
            "Epoch [30/100], Loss: 16.6667\n",
            "Test Accuracy: 83.33%\n",
            "Epoch [31/100], Step [10/150], Loss: 16.5625\n",
            "Epoch [31/100], Step [20/150], Loss: 16.1719\n",
            "Epoch [31/100], Step [30/150], Loss: 16.3542\n",
            "Epoch [31/100], Step [40/150], Loss: 16.2109\n",
            "Epoch [31/100], Step [50/150], Loss: 16.5000\n",
            "Epoch [31/100], Step [60/150], Loss: 16.6667\n",
            "Epoch [31/100], Step [70/150], Loss: 16.6295\n",
            "Epoch [31/100], Step [80/150], Loss: 16.5234\n",
            "Epoch [31/100], Step [90/150], Loss: 16.4757\n",
            "Epoch [31/100], Step [100/150], Loss: 16.4375\n",
            "Epoch [31/100], Step [110/150], Loss: 16.4062\n",
            "Epoch [31/100], Step [120/150], Loss: 16.4974\n",
            "Epoch [31/100], Step [130/150], Loss: 16.5986\n",
            "Epoch [31/100], Step [140/150], Loss: 16.6741\n",
            "Epoch [31/100], Step [150/150], Loss: 16.6667\n",
            "Epoch [31/100], Loss: 16.6667\n",
            "Test Accuracy: 83.33%\n",
            "Epoch [32/100], Step [10/150], Loss: 16.7188\n",
            "Epoch [32/100], Step [20/150], Loss: 15.9375\n",
            "Epoch [32/100], Step [30/150], Loss: 16.5625\n",
            "Epoch [32/100], Step [40/150], Loss: 17.2656\n",
            "Epoch [32/100], Step [50/150], Loss: 17.1250\n",
            "Epoch [32/100], Step [60/150], Loss: 17.0833\n",
            "Epoch [32/100], Step [70/150], Loss: 16.7411\n",
            "Epoch [32/100], Step [80/150], Loss: 16.7188\n",
            "Epoch [32/100], Step [90/150], Loss: 16.6319\n",
            "Epoch [32/100], Step [100/150], Loss: 16.7031\n",
            "Epoch [32/100], Step [110/150], Loss: 16.7188\n",
            "Epoch [32/100], Step [120/150], Loss: 16.6927\n",
            "Epoch [32/100], Step [130/150], Loss: 16.6466\n",
            "Epoch [32/100], Step [140/150], Loss: 16.6071\n",
            "Epoch [32/100], Step [150/150], Loss: 16.6667\n",
            "Epoch [32/100], Loss: 16.6667\n",
            "Test Accuracy: 83.33%\n",
            "Epoch [33/100], Step [10/150], Loss: 17.6562\n",
            "Epoch [33/100], Step [20/150], Loss: 17.2656\n",
            "Epoch [33/100], Step [30/150], Loss: 16.3021\n",
            "Epoch [33/100], Step [40/150], Loss: 16.6406\n",
            "Epoch [33/100], Step [50/150], Loss: 16.8125\n",
            "Epoch [33/100], Step [60/150], Loss: 16.7969\n",
            "Epoch [33/100], Step [70/150], Loss: 16.9866\n",
            "Epoch [33/100], Step [80/150], Loss: 16.9727\n",
            "Epoch [33/100], Step [90/150], Loss: 16.9965\n",
            "Epoch [33/100], Step [100/150], Loss: 17.0781\n",
            "Epoch [33/100], Step [110/150], Loss: 16.9744\n",
            "Epoch [33/100], Step [120/150], Loss: 16.8750\n",
            "Epoch [33/100], Step [130/150], Loss: 16.7067\n",
            "Epoch [33/100], Step [140/150], Loss: 16.5960\n",
            "Epoch [33/100], Step [150/150], Loss: 16.6667\n",
            "Epoch [33/100], Loss: 16.6667\n",
            "Test Accuracy: 83.33%\n",
            "Epoch [34/100], Step [10/150], Loss: 15.9375\n",
            "Epoch [34/100], Step [20/150], Loss: 15.0000\n",
            "Epoch [34/100], Step [30/150], Loss: 15.9896\n",
            "Epoch [34/100], Step [40/150], Loss: 16.1328\n",
            "Epoch [34/100], Step [50/150], Loss: 16.5312\n",
            "Epoch [34/100], Step [60/150], Loss: 16.8229\n",
            "Epoch [34/100], Step [70/150], Loss: 16.7188\n",
            "Epoch [34/100], Step [80/150], Loss: 17.0117\n",
            "Epoch [34/100], Step [90/150], Loss: 16.8229\n",
            "Epoch [34/100], Step [100/150], Loss: 16.7969\n",
            "Epoch [34/100], Step [110/150], Loss: 16.6193\n",
            "Epoch [34/100], Step [120/150], Loss: 16.4323\n",
            "Epoch [34/100], Step [130/150], Loss: 16.6346\n",
            "Epoch [34/100], Step [140/150], Loss: 16.5625\n",
            "Epoch [34/100], Step [150/150], Loss: 16.6667\n",
            "Epoch [34/100], Loss: 16.6667\n",
            "Test Accuracy: 83.33%\n",
            "Epoch [35/100], Step [10/150], Loss: 13.7500\n",
            "Epoch [35/100], Step [20/150], Loss: 15.1562\n",
            "Epoch [35/100], Step [30/150], Loss: 15.4688\n",
            "Epoch [35/100], Step [40/150], Loss: 15.8203\n",
            "Epoch [35/100], Step [50/150], Loss: 16.1562\n",
            "Epoch [35/100], Step [60/150], Loss: 16.3281\n",
            "Epoch [35/100], Step [70/150], Loss: 16.4062\n",
            "Epoch [35/100], Step [80/150], Loss: 16.7578\n",
            "Epoch [35/100], Step [90/150], Loss: 16.8056\n",
            "Epoch [35/100], Step [100/150], Loss: 16.7812\n",
            "Epoch [35/100], Step [110/150], Loss: 16.7472\n",
            "Epoch [35/100], Step [120/150], Loss: 16.7448\n",
            "Epoch [35/100], Step [130/150], Loss: 16.8029\n",
            "Epoch [35/100], Step [140/150], Loss: 16.6853\n",
            "Epoch [35/100], Step [150/150], Loss: 16.6667\n",
            "Epoch [35/100], Loss: 16.6667\n",
            "Test Accuracy: 83.33%\n",
            "Epoch [36/100], Step [10/150], Loss: 15.6250\n",
            "Epoch [36/100], Step [20/150], Loss: 16.8750\n",
            "Epoch [36/100], Step [30/150], Loss: 17.0833\n",
            "Epoch [36/100], Step [40/150], Loss: 16.3281\n",
            "Epoch [36/100], Step [50/150], Loss: 16.5000\n",
            "Epoch [36/100], Step [60/150], Loss: 16.4844\n",
            "Epoch [36/100], Step [70/150], Loss: 16.3839\n",
            "Epoch [36/100], Step [80/150], Loss: 16.4258\n",
            "Epoch [36/100], Step [90/150], Loss: 16.7188\n",
            "Epoch [36/100], Step [100/150], Loss: 16.7969\n",
            "Epoch [36/100], Step [110/150], Loss: 16.6193\n",
            "Epoch [36/100], Step [120/150], Loss: 16.8099\n",
            "Epoch [36/100], Step [130/150], Loss: 16.7668\n",
            "Epoch [36/100], Step [140/150], Loss: 16.6518\n",
            "Epoch [36/100], Step [150/150], Loss: 16.6667\n",
            "Epoch [36/100], Loss: 16.6667\n",
            "Test Accuracy: 83.33%\n",
            "Epoch [37/100], Step [10/150], Loss: 17.0312\n",
            "Epoch [37/100], Step [20/150], Loss: 16.3281\n",
            "Epoch [37/100], Step [30/150], Loss: 16.0417\n",
            "Epoch [37/100], Step [40/150], Loss: 15.2344\n",
            "Epoch [37/100], Step [50/150], Loss: 15.5312\n",
            "Epoch [37/100], Step [60/150], Loss: 16.1458\n",
            "Epoch [37/100], Step [70/150], Loss: 16.1161\n",
            "Epoch [37/100], Step [80/150], Loss: 16.3477\n",
            "Epoch [37/100], Step [90/150], Loss: 16.4931\n",
            "Epoch [37/100], Step [100/150], Loss: 16.5625\n",
            "Epoch [37/100], Step [110/150], Loss: 16.6335\n",
            "Epoch [37/100], Step [120/150], Loss: 16.7708\n",
            "Epoch [37/100], Step [130/150], Loss: 16.6587\n",
            "Epoch [37/100], Step [140/150], Loss: 16.5960\n",
            "Epoch [37/100], Step [150/150], Loss: 16.6667\n",
            "Epoch [37/100], Loss: 16.6667\n",
            "Test Accuracy: 83.33%\n",
            "Epoch [38/100], Step [10/150], Loss: 16.2500\n",
            "Epoch [38/100], Step [20/150], Loss: 17.1875\n",
            "Epoch [38/100], Step [30/150], Loss: 16.9792\n",
            "Epoch [38/100], Step [40/150], Loss: 17.1094\n",
            "Epoch [38/100], Step [50/150], Loss: 16.9375\n",
            "Epoch [38/100], Step [60/150], Loss: 17.2917\n",
            "Epoch [38/100], Step [70/150], Loss: 17.1652\n",
            "Epoch [38/100], Step [80/150], Loss: 17.2070\n",
            "Epoch [38/100], Step [90/150], Loss: 16.9097\n",
            "Epoch [38/100], Step [100/150], Loss: 16.6562\n",
            "Epoch [38/100], Step [110/150], Loss: 16.6477\n",
            "Epoch [38/100], Step [120/150], Loss: 16.6276\n",
            "Epoch [38/100], Step [130/150], Loss: 16.6587\n",
            "Epoch [38/100], Step [140/150], Loss: 16.5960\n",
            "Epoch [38/100], Step [150/150], Loss: 16.6667\n",
            "Epoch [38/100], Loss: 16.6667\n",
            "Test Accuracy: 83.33%\n",
            "Epoch [39/100], Step [10/150], Loss: 11.4062\n",
            "Epoch [39/100], Step [20/150], Loss: 13.4375\n",
            "Epoch [39/100], Step [30/150], Loss: 14.6875\n",
            "Epoch [39/100], Step [40/150], Loss: 16.0156\n",
            "Epoch [39/100], Step [50/150], Loss: 16.0625\n",
            "Epoch [39/100], Step [60/150], Loss: 16.3021\n",
            "Epoch [39/100], Step [70/150], Loss: 16.0938\n",
            "Epoch [39/100], Step [80/150], Loss: 16.5430\n",
            "Epoch [39/100], Step [90/150], Loss: 16.3715\n",
            "Epoch [39/100], Step [100/150], Loss: 16.5312\n",
            "Epoch [39/100], Step [110/150], Loss: 16.5483\n",
            "Epoch [39/100], Step [120/150], Loss: 16.5234\n",
            "Epoch [39/100], Step [130/150], Loss: 16.6226\n",
            "Epoch [39/100], Step [140/150], Loss: 16.4732\n",
            "Epoch [39/100], Step [150/150], Loss: 16.6667\n",
            "Epoch [39/100], Loss: 16.6667\n",
            "Test Accuracy: 83.33%\n",
            "Epoch [40/100], Step [10/150], Loss: 15.1562\n",
            "Epoch [40/100], Step [20/150], Loss: 16.8750\n",
            "Epoch [40/100], Step [30/150], Loss: 16.7188\n",
            "Epoch [40/100], Step [40/150], Loss: 16.8359\n",
            "Epoch [40/100], Step [50/150], Loss: 16.5312\n",
            "Epoch [40/100], Step [60/150], Loss: 16.6667\n",
            "Epoch [40/100], Step [70/150], Loss: 16.6741\n",
            "Epoch [40/100], Step [80/150], Loss: 16.9727\n",
            "Epoch [40/100], Step [90/150], Loss: 16.8403\n",
            "Epoch [40/100], Step [100/150], Loss: 16.3438\n",
            "Epoch [40/100], Step [110/150], Loss: 16.2074\n",
            "Epoch [40/100], Step [120/150], Loss: 16.2891\n",
            "Epoch [40/100], Step [130/150], Loss: 16.4904\n",
            "Epoch [40/100], Step [140/150], Loss: 16.5513\n",
            "Epoch [40/100], Step [150/150], Loss: 16.6667\n",
            "Epoch [40/100], Loss: 16.6667\n",
            "Test Accuracy: 83.33%\n",
            "Epoch [41/100], Step [10/150], Loss: 14.6875\n",
            "Epoch [41/100], Step [20/150], Loss: 15.7031\n",
            "Epoch [41/100], Step [30/150], Loss: 15.4688\n",
            "Epoch [41/100], Step [40/150], Loss: 15.5469\n",
            "Epoch [41/100], Step [50/150], Loss: 15.5625\n",
            "Epoch [41/100], Step [60/150], Loss: 15.5990\n",
            "Epoch [41/100], Step [70/150], Loss: 15.8929\n",
            "Epoch [41/100], Step [80/150], Loss: 16.2305\n",
            "Epoch [41/100], Step [90/150], Loss: 16.2500\n",
            "Epoch [41/100], Step [100/150], Loss: 16.3281\n",
            "Epoch [41/100], Step [110/150], Loss: 16.3210\n",
            "Epoch [41/100], Step [120/150], Loss: 16.4583\n",
            "Epoch [41/100], Step [130/150], Loss: 16.5024\n",
            "Epoch [41/100], Step [140/150], Loss: 16.6071\n",
            "Epoch [41/100], Step [150/150], Loss: 16.6667\n",
            "Epoch [41/100], Loss: 16.6667\n",
            "Test Accuracy: 83.33%\n",
            "Epoch [42/100], Step [10/150], Loss: 17.3438\n",
            "Epoch [42/100], Step [20/150], Loss: 17.0312\n",
            "Epoch [42/100], Step [30/150], Loss: 16.5625\n",
            "Epoch [42/100], Step [40/150], Loss: 16.7578\n",
            "Epoch [42/100], Step [50/150], Loss: 16.7812\n",
            "Epoch [42/100], Step [60/150], Loss: 16.1979\n",
            "Epoch [42/100], Step [70/150], Loss: 16.2946\n",
            "Epoch [42/100], Step [80/150], Loss: 16.5430\n",
            "Epoch [42/100], Step [90/150], Loss: 16.8403\n",
            "Epoch [42/100], Step [100/150], Loss: 16.8906\n",
            "Epoch [42/100], Step [110/150], Loss: 16.8892\n",
            "Epoch [42/100], Step [120/150], Loss: 16.8099\n",
            "Epoch [42/100], Step [130/150], Loss: 16.7428\n",
            "Epoch [42/100], Step [140/150], Loss: 16.5513\n",
            "Epoch [42/100], Step [150/150], Loss: 16.6667\n",
            "Epoch [42/100], Loss: 16.6667\n",
            "Test Accuracy: 83.33%\n",
            "Epoch [43/100], Step [10/150], Loss: 17.0312\n",
            "Epoch [43/100], Step [20/150], Loss: 16.4062\n",
            "Epoch [43/100], Step [30/150], Loss: 16.3021\n",
            "Epoch [43/100], Step [40/150], Loss: 16.4844\n",
            "Epoch [43/100], Step [50/150], Loss: 16.4688\n",
            "Epoch [43/100], Step [60/150], Loss: 16.3542\n",
            "Epoch [43/100], Step [70/150], Loss: 16.3393\n",
            "Epoch [43/100], Step [80/150], Loss: 16.4062\n",
            "Epoch [43/100], Step [90/150], Loss: 16.3542\n",
            "Epoch [43/100], Step [100/150], Loss: 16.5312\n",
            "Epoch [43/100], Step [110/150], Loss: 16.3494\n",
            "Epoch [43/100], Step [120/150], Loss: 16.3411\n",
            "Epoch [43/100], Step [130/150], Loss: 16.2981\n",
            "Epoch [43/100], Step [140/150], Loss: 16.5848\n",
            "Epoch [43/100], Step [150/150], Loss: 16.6667\n",
            "Epoch [43/100], Loss: 16.6667\n",
            "Test Accuracy: 83.33%\n",
            "Epoch [44/100], Step [10/150], Loss: 16.0938\n",
            "Epoch [44/100], Step [20/150], Loss: 16.1719\n",
            "Epoch [44/100], Step [30/150], Loss: 15.8854\n",
            "Epoch [44/100], Step [40/150], Loss: 16.2109\n",
            "Epoch [44/100], Step [50/150], Loss: 16.1250\n",
            "Epoch [44/100], Step [60/150], Loss: 15.9896\n",
            "Epoch [44/100], Step [70/150], Loss: 16.4286\n",
            "Epoch [44/100], Step [80/150], Loss: 16.1328\n",
            "Epoch [44/100], Step [90/150], Loss: 16.2153\n",
            "Epoch [44/100], Step [100/150], Loss: 16.3125\n",
            "Epoch [44/100], Step [110/150], Loss: 16.5341\n",
            "Epoch [44/100], Step [120/150], Loss: 16.6927\n",
            "Epoch [44/100], Step [130/150], Loss: 16.6947\n",
            "Epoch [44/100], Step [140/150], Loss: 16.6741\n",
            "Epoch [44/100], Step [150/150], Loss: 16.6667\n",
            "Epoch [44/100], Loss: 16.6667\n",
            "Test Accuracy: 83.33%\n",
            "Epoch [45/100], Step [10/150], Loss: 16.7188\n",
            "Epoch [45/100], Step [20/150], Loss: 16.6406\n",
            "Epoch [45/100], Step [30/150], Loss: 16.3542\n",
            "Epoch [45/100], Step [40/150], Loss: 16.3672\n",
            "Epoch [45/100], Step [50/150], Loss: 16.2500\n",
            "Epoch [45/100], Step [60/150], Loss: 16.0156\n",
            "Epoch [45/100], Step [70/150], Loss: 16.0938\n",
            "Epoch [45/100], Step [80/150], Loss: 16.4258\n",
            "Epoch [45/100], Step [90/150], Loss: 16.5625\n",
            "Epoch [45/100], Step [100/150], Loss: 16.5312\n",
            "Epoch [45/100], Step [110/150], Loss: 16.5199\n",
            "Epoch [45/100], Step [120/150], Loss: 16.7188\n",
            "Epoch [45/100], Step [130/150], Loss: 16.5745\n",
            "Epoch [45/100], Step [140/150], Loss: 16.6295\n",
            "Epoch [45/100], Step [150/150], Loss: 16.6667\n",
            "Epoch [45/100], Loss: 16.6667\n",
            "Test Accuracy: 83.33%\n",
            "Epoch [46/100], Step [10/150], Loss: 17.9688\n",
            "Epoch [46/100], Step [20/150], Loss: 18.1250\n",
            "Epoch [46/100], Step [30/150], Loss: 17.8646\n",
            "Epoch [46/100], Step [40/150], Loss: 17.4219\n",
            "Epoch [46/100], Step [50/150], Loss: 17.4375\n",
            "Epoch [46/100], Step [60/150], Loss: 17.4219\n",
            "Epoch [46/100], Step [70/150], Loss: 16.9420\n",
            "Epoch [46/100], Step [80/150], Loss: 16.8555\n",
            "Epoch [46/100], Step [90/150], Loss: 16.7708\n",
            "Epoch [46/100], Step [100/150], Loss: 16.7656\n",
            "Epoch [46/100], Step [110/150], Loss: 16.7045\n",
            "Epoch [46/100], Step [120/150], Loss: 16.5495\n",
            "Epoch [46/100], Step [130/150], Loss: 16.7548\n",
            "Epoch [46/100], Step [140/150], Loss: 16.5848\n",
            "Epoch [46/100], Step [150/150], Loss: 16.6667\n",
            "Epoch [46/100], Loss: 16.6667\n",
            "Test Accuracy: 83.33%\n",
            "Epoch [47/100], Step [10/150], Loss: 18.4375\n",
            "Epoch [47/100], Step [20/150], Loss: 16.7188\n",
            "Epoch [47/100], Step [30/150], Loss: 16.8750\n",
            "Epoch [47/100], Step [40/150], Loss: 16.3281\n",
            "Epoch [47/100], Step [50/150], Loss: 16.3438\n",
            "Epoch [47/100], Step [60/150], Loss: 16.8490\n",
            "Epoch [47/100], Step [70/150], Loss: 16.8080\n",
            "Epoch [47/100], Step [80/150], Loss: 16.9531\n",
            "Epoch [47/100], Step [90/150], Loss: 16.8576\n",
            "Epoch [47/100], Step [100/150], Loss: 16.7656\n",
            "Epoch [47/100], Step [110/150], Loss: 16.4773\n",
            "Epoch [47/100], Step [120/150], Loss: 16.5495\n",
            "Epoch [47/100], Step [130/150], Loss: 16.5625\n",
            "Epoch [47/100], Step [140/150], Loss: 16.4955\n",
            "Epoch [47/100], Step [150/150], Loss: 16.6667\n",
            "Epoch [47/100], Loss: 16.6667\n",
            "Test Accuracy: 83.33%\n",
            "Epoch [48/100], Step [10/150], Loss: 16.4062\n",
            "Epoch [48/100], Step [20/150], Loss: 17.7344\n",
            "Epoch [48/100], Step [30/150], Loss: 17.0833\n",
            "Epoch [48/100], Step [40/150], Loss: 17.4219\n",
            "Epoch [48/100], Step [50/150], Loss: 17.5000\n",
            "Epoch [48/100], Step [60/150], Loss: 17.3698\n",
            "Epoch [48/100], Step [70/150], Loss: 17.2098\n",
            "Epoch [48/100], Step [80/150], Loss: 17.2266\n",
            "Epoch [48/100], Step [90/150], Loss: 17.1875\n",
            "Epoch [48/100], Step [100/150], Loss: 17.1094\n",
            "Epoch [48/100], Step [110/150], Loss: 16.8608\n",
            "Epoch [48/100], Step [120/150], Loss: 16.8359\n",
            "Epoch [48/100], Step [130/150], Loss: 16.7548\n",
            "Epoch [48/100], Step [140/150], Loss: 16.7969\n",
            "Epoch [48/100], Step [150/150], Loss: 16.6667\n",
            "Epoch [48/100], Loss: 16.6667\n",
            "Test Accuracy: 83.33%\n",
            "Epoch [49/100], Step [10/150], Loss: 17.6562\n",
            "Epoch [49/100], Step [20/150], Loss: 17.0312\n",
            "Epoch [49/100], Step [30/150], Loss: 17.0312\n",
            "Epoch [49/100], Step [40/150], Loss: 16.5625\n",
            "Epoch [49/100], Step [50/150], Loss: 16.6562\n",
            "Epoch [49/100], Step [60/150], Loss: 16.6667\n",
            "Epoch [49/100], Step [70/150], Loss: 16.7634\n",
            "Epoch [49/100], Step [80/150], Loss: 16.8555\n",
            "Epoch [49/100], Step [90/150], Loss: 16.6146\n",
            "Epoch [49/100], Step [100/150], Loss: 16.8438\n",
            "Epoch [49/100], Step [110/150], Loss: 16.6619\n",
            "Epoch [49/100], Step [120/150], Loss: 16.6146\n",
            "Epoch [49/100], Step [130/150], Loss: 16.5986\n",
            "Epoch [49/100], Step [140/150], Loss: 16.7076\n",
            "Epoch [49/100], Step [150/150], Loss: 16.6667\n",
            "Epoch [49/100], Loss: 16.6667\n",
            "Test Accuracy: 83.33%\n",
            "Epoch [50/100], Step [10/150], Loss: 14.8438\n",
            "Epoch [50/100], Step [20/150], Loss: 14.2188\n",
            "Epoch [50/100], Step [30/150], Loss: 15.2083\n",
            "Epoch [50/100], Step [40/150], Loss: 15.7422\n",
            "Epoch [50/100], Step [50/150], Loss: 16.0625\n",
            "Epoch [50/100], Step [60/150], Loss: 16.0938\n",
            "Epoch [50/100], Step [70/150], Loss: 15.9598\n",
            "Epoch [50/100], Step [80/150], Loss: 16.2500\n",
            "Epoch [50/100], Step [90/150], Loss: 16.4410\n",
            "Epoch [50/100], Step [100/150], Loss: 16.5781\n",
            "Epoch [50/100], Step [110/150], Loss: 16.6619\n",
            "Epoch [50/100], Step [120/150], Loss: 16.5625\n",
            "Epoch [50/100], Step [130/150], Loss: 16.6466\n",
            "Epoch [50/100], Step [140/150], Loss: 16.7634\n",
            "Epoch [50/100], Step [150/150], Loss: 16.6667\n",
            "Epoch [50/100], Loss: 16.6667\n",
            "Test Accuracy: 83.33%\n",
            "Epoch [51/100], Step [10/150], Loss: 18.2812\n",
            "Epoch [51/100], Step [20/150], Loss: 18.2812\n",
            "Epoch [51/100], Step [30/150], Loss: 16.8750\n",
            "Epoch [51/100], Step [40/150], Loss: 16.6797\n",
            "Epoch [51/100], Step [50/150], Loss: 16.5625\n",
            "Epoch [51/100], Step [60/150], Loss: 16.5885\n",
            "Epoch [51/100], Step [70/150], Loss: 16.8304\n",
            "Epoch [51/100], Step [80/150], Loss: 17.2070\n",
            "Epoch [51/100], Step [90/150], Loss: 16.7535\n",
            "Epoch [51/100], Step [100/150], Loss: 16.5938\n",
            "Epoch [51/100], Step [110/150], Loss: 16.6193\n",
            "Epoch [51/100], Step [120/150], Loss: 16.7839\n",
            "Epoch [51/100], Step [130/150], Loss: 16.5264\n",
            "Epoch [51/100], Step [140/150], Loss: 16.5067\n",
            "Epoch [51/100], Step [150/150], Loss: 16.6667\n",
            "Epoch [51/100], Loss: 16.6667\n",
            "Test Accuracy: 83.33%\n",
            "Epoch [52/100], Step [10/150], Loss: 18.2812\n",
            "Epoch [52/100], Step [20/150], Loss: 17.2656\n",
            "Epoch [52/100], Step [30/150], Loss: 15.9375\n",
            "Epoch [52/100], Step [40/150], Loss: 16.1328\n",
            "Epoch [52/100], Step [50/150], Loss: 16.0312\n",
            "Epoch [52/100], Step [60/150], Loss: 15.9115\n",
            "Epoch [52/100], Step [70/150], Loss: 16.0714\n",
            "Epoch [52/100], Step [80/150], Loss: 16.2500\n",
            "Epoch [52/100], Step [90/150], Loss: 16.4062\n",
            "Epoch [52/100], Step [100/150], Loss: 16.3438\n",
            "Epoch [52/100], Step [110/150], Loss: 16.4631\n",
            "Epoch [52/100], Step [120/150], Loss: 16.3932\n",
            "Epoch [52/100], Step [130/150], Loss: 16.4303\n",
            "Epoch [52/100], Step [140/150], Loss: 16.6295\n",
            "Epoch [52/100], Step [150/150], Loss: 16.6667\n",
            "Epoch [52/100], Loss: 16.6667\n",
            "Test Accuracy: 83.33%\n",
            "Epoch [53/100], Step [10/150], Loss: 17.1875\n",
            "Epoch [53/100], Step [20/150], Loss: 16.7188\n",
            "Epoch [53/100], Step [30/150], Loss: 17.2396\n",
            "Epoch [53/100], Step [40/150], Loss: 16.4844\n",
            "Epoch [53/100], Step [50/150], Loss: 16.4688\n",
            "Epoch [53/100], Step [60/150], Loss: 16.7188\n",
            "Epoch [53/100], Step [70/150], Loss: 16.5179\n",
            "Epoch [53/100], Step [80/150], Loss: 16.1914\n",
            "Epoch [53/100], Step [90/150], Loss: 16.2326\n",
            "Epoch [53/100], Step [100/150], Loss: 16.2344\n",
            "Epoch [53/100], Step [110/150], Loss: 16.2926\n",
            "Epoch [53/100], Step [120/150], Loss: 16.3932\n",
            "Epoch [53/100], Step [130/150], Loss: 16.4543\n",
            "Epoch [53/100], Step [140/150], Loss: 16.5960\n",
            "Epoch [53/100], Step [150/150], Loss: 16.6667\n",
            "Epoch [53/100], Loss: 16.6667\n",
            "Test Accuracy: 83.33%\n",
            "Epoch [54/100], Step [10/150], Loss: 15.7812\n",
            "Epoch [54/100], Step [20/150], Loss: 15.9375\n",
            "Epoch [54/100], Step [30/150], Loss: 15.9896\n",
            "Epoch [54/100], Step [40/150], Loss: 16.1719\n",
            "Epoch [54/100], Step [50/150], Loss: 16.3125\n",
            "Epoch [54/100], Step [60/150], Loss: 16.4583\n",
            "Epoch [54/100], Step [70/150], Loss: 16.6071\n",
            "Epoch [54/100], Step [80/150], Loss: 16.5625\n",
            "Epoch [54/100], Step [90/150], Loss: 16.6319\n",
            "Epoch [54/100], Step [100/150], Loss: 16.7188\n",
            "Epoch [54/100], Step [110/150], Loss: 16.8466\n",
            "Epoch [54/100], Step [120/150], Loss: 16.8359\n",
            "Epoch [54/100], Step [130/150], Loss: 16.6707\n",
            "Epoch [54/100], Step [140/150], Loss: 16.6406\n",
            "Epoch [54/100], Step [150/150], Loss: 16.6667\n",
            "Epoch [54/100], Loss: 16.6667\n",
            "Test Accuracy: 83.33%\n",
            "Epoch [55/100], Step [10/150], Loss: 17.1875\n",
            "Epoch [55/100], Step [20/150], Loss: 18.1250\n",
            "Epoch [55/100], Step [30/150], Loss: 17.7604\n",
            "Epoch [55/100], Step [40/150], Loss: 17.2656\n",
            "Epoch [55/100], Step [50/150], Loss: 17.2812\n",
            "Epoch [55/100], Step [60/150], Loss: 17.1875\n",
            "Epoch [55/100], Step [70/150], Loss: 16.9643\n",
            "Epoch [55/100], Step [80/150], Loss: 16.6992\n",
            "Epoch [55/100], Step [90/150], Loss: 16.9097\n",
            "Epoch [55/100], Step [100/150], Loss: 17.0469\n",
            "Epoch [55/100], Step [110/150], Loss: 17.1591\n",
            "Epoch [55/100], Step [120/150], Loss: 16.9661\n",
            "Epoch [55/100], Step [130/150], Loss: 16.8750\n",
            "Epoch [55/100], Step [140/150], Loss: 16.6741\n",
            "Epoch [55/100], Step [150/150], Loss: 16.6667\n",
            "Epoch [55/100], Loss: 16.6667\n",
            "Test Accuracy: 83.33%\n",
            "Epoch [56/100], Step [10/150], Loss: 17.1875\n",
            "Epoch [56/100], Step [20/150], Loss: 16.6406\n",
            "Epoch [56/100], Step [30/150], Loss: 16.4583\n",
            "Epoch [56/100], Step [40/150], Loss: 15.8984\n",
            "Epoch [56/100], Step [50/150], Loss: 16.0625\n",
            "Epoch [56/100], Step [60/150], Loss: 16.1198\n",
            "Epoch [56/100], Step [70/150], Loss: 16.4286\n",
            "Epoch [56/100], Step [80/150], Loss: 16.2500\n",
            "Epoch [56/100], Step [90/150], Loss: 16.2674\n",
            "Epoch [56/100], Step [100/150], Loss: 16.4531\n",
            "Epoch [56/100], Step [110/150], Loss: 16.5625\n",
            "Epoch [56/100], Step [120/150], Loss: 16.5234\n",
            "Epoch [56/100], Step [130/150], Loss: 16.6947\n",
            "Epoch [56/100], Step [140/150], Loss: 16.7299\n",
            "Epoch [56/100], Step [150/150], Loss: 16.6667\n",
            "Epoch [56/100], Loss: 16.6667\n",
            "Test Accuracy: 83.33%\n",
            "Epoch [57/100], Step [10/150], Loss: 17.0312\n",
            "Epoch [57/100], Step [20/150], Loss: 16.4062\n",
            "Epoch [57/100], Step [30/150], Loss: 16.6667\n",
            "Epoch [57/100], Step [40/150], Loss: 16.7188\n",
            "Epoch [57/100], Step [50/150], Loss: 16.4688\n",
            "Epoch [57/100], Step [60/150], Loss: 16.5365\n",
            "Epoch [57/100], Step [70/150], Loss: 16.6518\n",
            "Epoch [57/100], Step [80/150], Loss: 16.8164\n",
            "Epoch [57/100], Step [90/150], Loss: 16.7708\n",
            "Epoch [57/100], Step [100/150], Loss: 16.5156\n",
            "Epoch [57/100], Step [110/150], Loss: 16.5199\n",
            "Epoch [57/100], Step [120/150], Loss: 16.5755\n",
            "Epoch [57/100], Step [130/150], Loss: 16.5865\n",
            "Epoch [57/100], Step [140/150], Loss: 16.6741\n",
            "Epoch [57/100], Step [150/150], Loss: 16.6667\n",
            "Epoch [57/100], Loss: 16.6667\n",
            "Test Accuracy: 83.33%\n",
            "Epoch [58/100], Step [10/150], Loss: 16.7188\n",
            "Epoch [58/100], Step [20/150], Loss: 16.5625\n",
            "Epoch [58/100], Step [30/150], Loss: 16.6667\n",
            "Epoch [58/100], Step [40/150], Loss: 17.0312\n",
            "Epoch [58/100], Step [50/150], Loss: 17.0000\n",
            "Epoch [58/100], Step [60/150], Loss: 16.9010\n",
            "Epoch [58/100], Step [70/150], Loss: 16.8973\n",
            "Epoch [58/100], Step [80/150], Loss: 16.7383\n",
            "Epoch [58/100], Step [90/150], Loss: 17.1528\n",
            "Epoch [58/100], Step [100/150], Loss: 17.0156\n",
            "Epoch [58/100], Step [110/150], Loss: 16.8750\n",
            "Epoch [58/100], Step [120/150], Loss: 16.7318\n",
            "Epoch [58/100], Step [130/150], Loss: 16.6466\n",
            "Epoch [58/100], Step [140/150], Loss: 16.7634\n",
            "Epoch [58/100], Step [150/150], Loss: 16.6667\n",
            "Epoch [58/100], Loss: 16.6667\n",
            "Test Accuracy: 83.33%\n",
            "Epoch [59/100], Step [10/150], Loss: 14.8438\n",
            "Epoch [59/100], Step [20/150], Loss: 17.1094\n",
            "Epoch [59/100], Step [30/150], Loss: 17.1875\n",
            "Epoch [59/100], Step [40/150], Loss: 17.2656\n",
            "Epoch [59/100], Step [50/150], Loss: 17.0312\n",
            "Epoch [59/100], Step [60/150], Loss: 16.6406\n",
            "Epoch [59/100], Step [70/150], Loss: 16.5179\n",
            "Epoch [59/100], Step [80/150], Loss: 16.6406\n",
            "Epoch [59/100], Step [90/150], Loss: 16.7708\n",
            "Epoch [59/100], Step [100/150], Loss: 16.7500\n",
            "Epoch [59/100], Step [110/150], Loss: 16.6051\n",
            "Epoch [59/100], Step [120/150], Loss: 16.7708\n",
            "Epoch [59/100], Step [130/150], Loss: 16.7909\n",
            "Epoch [59/100], Step [140/150], Loss: 16.8527\n",
            "Epoch [59/100], Step [150/150], Loss: 16.6667\n",
            "Epoch [59/100], Loss: 16.6667\n",
            "Test Accuracy: 83.33%\n",
            "Epoch [60/100], Step [10/150], Loss: 17.8125\n",
            "Epoch [60/100], Step [20/150], Loss: 16.6406\n",
            "Epoch [60/100], Step [30/150], Loss: 16.0938\n",
            "Epoch [60/100], Step [40/150], Loss: 15.3516\n",
            "Epoch [60/100], Step [50/150], Loss: 15.2812\n",
            "Epoch [60/100], Step [60/150], Loss: 15.7812\n",
            "Epoch [60/100], Step [70/150], Loss: 16.0491\n",
            "Epoch [60/100], Step [80/150], Loss: 16.0938\n",
            "Epoch [60/100], Step [90/150], Loss: 16.3021\n",
            "Epoch [60/100], Step [100/150], Loss: 16.5000\n",
            "Epoch [60/100], Step [110/150], Loss: 16.4631\n",
            "Epoch [60/100], Step [120/150], Loss: 16.4193\n",
            "Epoch [60/100], Step [130/150], Loss: 16.5024\n",
            "Epoch [60/100], Step [140/150], Loss: 16.7411\n",
            "Epoch [60/100], Step [150/150], Loss: 16.6667\n",
            "Epoch [60/100], Loss: 16.6667\n",
            "Test Accuracy: 83.33%\n",
            "Epoch [61/100], Step [10/150], Loss: 17.5000\n",
            "Epoch [61/100], Step [20/150], Loss: 16.7188\n",
            "Epoch [61/100], Step [30/150], Loss: 17.6042\n",
            "Epoch [61/100], Step [40/150], Loss: 17.1094\n",
            "Epoch [61/100], Step [50/150], Loss: 16.7188\n",
            "Epoch [61/100], Step [60/150], Loss: 16.6927\n",
            "Epoch [61/100], Step [70/150], Loss: 16.4509\n",
            "Epoch [61/100], Step [80/150], Loss: 16.5234\n",
            "Epoch [61/100], Step [90/150], Loss: 16.5972\n",
            "Epoch [61/100], Step [100/150], Loss: 16.4688\n",
            "Epoch [61/100], Step [110/150], Loss: 16.5341\n",
            "Epoch [61/100], Step [120/150], Loss: 16.5885\n",
            "Epoch [61/100], Step [130/150], Loss: 16.6587\n",
            "Epoch [61/100], Step [140/150], Loss: 16.6629\n",
            "Epoch [61/100], Step [150/150], Loss: 16.6667\n",
            "Epoch [61/100], Loss: 16.6667\n",
            "Test Accuracy: 83.33%\n",
            "Epoch [62/100], Step [10/150], Loss: 14.3750\n",
            "Epoch [62/100], Step [20/150], Loss: 15.4688\n",
            "Epoch [62/100], Step [30/150], Loss: 16.4062\n",
            "Epoch [62/100], Step [40/150], Loss: 15.5469\n",
            "Epoch [62/100], Step [50/150], Loss: 16.5000\n",
            "Epoch [62/100], Step [60/150], Loss: 16.4583\n",
            "Epoch [62/100], Step [70/150], Loss: 16.5625\n",
            "Epoch [62/100], Step [80/150], Loss: 16.6406\n",
            "Epoch [62/100], Step [90/150], Loss: 16.5278\n",
            "Epoch [62/100], Step [100/150], Loss: 16.5312\n",
            "Epoch [62/100], Step [110/150], Loss: 16.6335\n",
            "Epoch [62/100], Step [120/150], Loss: 16.6927\n",
            "Epoch [62/100], Step [130/150], Loss: 16.6587\n",
            "Epoch [62/100], Step [140/150], Loss: 16.6741\n",
            "Epoch [62/100], Step [150/150], Loss: 16.6667\n",
            "Epoch [62/100], Loss: 16.6667\n",
            "Test Accuracy: 83.33%\n",
            "Epoch [63/100], Step [10/150], Loss: 15.1562\n",
            "Epoch [63/100], Step [20/150], Loss: 16.7188\n",
            "Epoch [63/100], Step [30/150], Loss: 16.3542\n",
            "Epoch [63/100], Step [40/150], Loss: 15.5078\n",
            "Epoch [63/100], Step [50/150], Loss: 15.6250\n",
            "Epoch [63/100], Step [60/150], Loss: 15.9115\n",
            "Epoch [63/100], Step [70/150], Loss: 16.2500\n",
            "Epoch [63/100], Step [80/150], Loss: 16.2109\n",
            "Epoch [63/100], Step [90/150], Loss: 16.2674\n",
            "Epoch [63/100], Step [100/150], Loss: 16.5781\n",
            "Epoch [63/100], Step [110/150], Loss: 16.6051\n",
            "Epoch [63/100], Step [120/150], Loss: 16.6276\n",
            "Epoch [63/100], Step [130/150], Loss: 16.7428\n",
            "Epoch [63/100], Step [140/150], Loss: 16.6406\n",
            "Epoch [63/100], Step [150/150], Loss: 16.6667\n",
            "Epoch [63/100], Loss: 16.6667\n",
            "Test Accuracy: 83.33%\n",
            "Epoch [64/100], Step [10/150], Loss: 16.2500\n",
            "Epoch [64/100], Step [20/150], Loss: 16.4062\n",
            "Epoch [64/100], Step [30/150], Loss: 16.1979\n",
            "Epoch [64/100], Step [40/150], Loss: 16.2891\n",
            "Epoch [64/100], Step [50/150], Loss: 16.0000\n",
            "Epoch [64/100], Step [60/150], Loss: 16.1719\n",
            "Epoch [64/100], Step [70/150], Loss: 16.6518\n",
            "Epoch [64/100], Step [80/150], Loss: 16.4062\n",
            "Epoch [64/100], Step [90/150], Loss: 16.2847\n",
            "Epoch [64/100], Step [100/150], Loss: 16.5781\n",
            "Epoch [64/100], Step [110/150], Loss: 16.4347\n",
            "Epoch [64/100], Step [120/150], Loss: 16.6146\n",
            "Epoch [64/100], Step [130/150], Loss: 16.5144\n",
            "Epoch [64/100], Step [140/150], Loss: 16.7188\n",
            "Epoch [64/100], Step [150/150], Loss: 16.6667\n",
            "Epoch [64/100], Loss: 16.6667\n",
            "Test Accuracy: 83.33%\n",
            "Epoch [65/100], Step [10/150], Loss: 16.0938\n",
            "Epoch [65/100], Step [20/150], Loss: 15.1562\n",
            "Epoch [65/100], Step [30/150], Loss: 15.3646\n",
            "Epoch [65/100], Step [40/150], Loss: 15.8984\n",
            "Epoch [65/100], Step [50/150], Loss: 16.5312\n",
            "Epoch [65/100], Step [60/150], Loss: 16.3802\n",
            "Epoch [65/100], Step [70/150], Loss: 16.6295\n",
            "Epoch [65/100], Step [80/150], Loss: 16.5625\n",
            "Epoch [65/100], Step [90/150], Loss: 16.5799\n",
            "Epoch [65/100], Step [100/150], Loss: 16.4531\n",
            "Epoch [65/100], Step [110/150], Loss: 16.5625\n",
            "Epoch [65/100], Step [120/150], Loss: 16.7578\n",
            "Epoch [65/100], Step [130/150], Loss: 16.7067\n",
            "Epoch [65/100], Step [140/150], Loss: 16.5848\n",
            "Epoch [65/100], Step [150/150], Loss: 16.6667\n",
            "Epoch [65/100], Loss: 16.6667\n",
            "Test Accuracy: 83.33%\n",
            "Epoch [66/100], Step [10/150], Loss: 15.7812\n",
            "Epoch [66/100], Step [20/150], Loss: 16.1719\n",
            "Epoch [66/100], Step [30/150], Loss: 17.0833\n",
            "Epoch [66/100], Step [40/150], Loss: 16.6016\n",
            "Epoch [66/100], Step [50/150], Loss: 17.0312\n",
            "Epoch [66/100], Step [60/150], Loss: 16.6927\n",
            "Epoch [66/100], Step [70/150], Loss: 16.4286\n",
            "Epoch [66/100], Step [80/150], Loss: 16.3672\n",
            "Epoch [66/100], Step [90/150], Loss: 16.5799\n",
            "Epoch [66/100], Step [100/150], Loss: 16.3750\n",
            "Epoch [66/100], Step [110/150], Loss: 16.3494\n",
            "Epoch [66/100], Step [120/150], Loss: 16.6276\n",
            "Epoch [66/100], Step [130/150], Loss: 16.6587\n",
            "Epoch [66/100], Step [140/150], Loss: 16.6071\n",
            "Epoch [66/100], Step [150/150], Loss: 16.6667\n",
            "Epoch [66/100], Loss: 16.6667\n",
            "Test Accuracy: 83.33%\n",
            "Epoch [67/100], Step [10/150], Loss: 17.9688\n",
            "Epoch [67/100], Step [20/150], Loss: 16.6406\n",
            "Epoch [67/100], Step [30/150], Loss: 16.7188\n",
            "Epoch [67/100], Step [40/150], Loss: 16.6797\n",
            "Epoch [67/100], Step [50/150], Loss: 16.8750\n",
            "Epoch [67/100], Step [60/150], Loss: 16.8490\n",
            "Epoch [67/100], Step [70/150], Loss: 16.7857\n",
            "Epoch [67/100], Step [80/150], Loss: 16.8555\n",
            "Epoch [67/100], Step [90/150], Loss: 16.8750\n",
            "Epoch [67/100], Step [100/150], Loss: 16.8750\n",
            "Epoch [67/100], Step [110/150], Loss: 16.7756\n",
            "Epoch [67/100], Step [120/150], Loss: 16.7057\n",
            "Epoch [67/100], Step [130/150], Loss: 16.7428\n",
            "Epoch [67/100], Step [140/150], Loss: 16.7076\n",
            "Epoch [67/100], Step [150/150], Loss: 16.6667\n",
            "Epoch [67/100], Loss: 16.6667\n",
            "Test Accuracy: 83.33%\n",
            "Epoch [68/100], Step [10/150], Loss: 13.7500\n",
            "Epoch [68/100], Step [20/150], Loss: 14.8438\n",
            "Epoch [68/100], Step [30/150], Loss: 14.7396\n",
            "Epoch [68/100], Step [40/150], Loss: 15.1172\n",
            "Epoch [68/100], Step [50/150], Loss: 15.8750\n",
            "Epoch [68/100], Step [60/150], Loss: 16.2760\n",
            "Epoch [68/100], Step [70/150], Loss: 16.1384\n",
            "Epoch [68/100], Step [80/150], Loss: 16.2695\n",
            "Epoch [68/100], Step [90/150], Loss: 16.5451\n",
            "Epoch [68/100], Step [100/150], Loss: 16.5469\n",
            "Epoch [68/100], Step [110/150], Loss: 16.5483\n",
            "Epoch [68/100], Step [120/150], Loss: 16.5885\n",
            "Epoch [68/100], Step [130/150], Loss: 16.6106\n",
            "Epoch [68/100], Step [140/150], Loss: 16.6964\n",
            "Epoch [68/100], Step [150/150], Loss: 16.6667\n",
            "Epoch [68/100], Loss: 16.6667\n",
            "Test Accuracy: 83.33%\n",
            "Epoch [69/100], Step [10/150], Loss: 15.6250\n",
            "Epoch [69/100], Step [20/150], Loss: 15.8594\n",
            "Epoch [69/100], Step [30/150], Loss: 15.7292\n",
            "Epoch [69/100], Step [40/150], Loss: 16.0547\n",
            "Epoch [69/100], Step [50/150], Loss: 15.9375\n",
            "Epoch [69/100], Step [60/150], Loss: 16.1719\n",
            "Epoch [69/100], Step [70/150], Loss: 16.2946\n",
            "Epoch [69/100], Step [80/150], Loss: 16.2500\n",
            "Epoch [69/100], Step [90/150], Loss: 16.0590\n",
            "Epoch [69/100], Step [100/150], Loss: 16.5156\n",
            "Epoch [69/100], Step [110/150], Loss: 16.5199\n",
            "Epoch [69/100], Step [120/150], Loss: 16.7448\n",
            "Epoch [69/100], Step [130/150], Loss: 16.6226\n",
            "Epoch [69/100], Step [140/150], Loss: 16.5625\n",
            "Epoch [69/100], Step [150/150], Loss: 16.6667\n",
            "Epoch [69/100], Loss: 16.6667\n",
            "Test Accuracy: 83.33%\n",
            "Epoch [70/100], Step [10/150], Loss: 16.8750\n",
            "Epoch [70/100], Step [20/150], Loss: 16.8750\n",
            "Epoch [70/100], Step [30/150], Loss: 17.5000\n",
            "Epoch [70/100], Step [40/150], Loss: 16.8359\n",
            "Epoch [70/100], Step [50/150], Loss: 16.4688\n",
            "Epoch [70/100], Step [60/150], Loss: 16.4062\n",
            "Epoch [70/100], Step [70/150], Loss: 16.3170\n",
            "Epoch [70/100], Step [80/150], Loss: 16.1719\n",
            "Epoch [70/100], Step [90/150], Loss: 16.3542\n",
            "Epoch [70/100], Step [100/150], Loss: 16.4844\n",
            "Epoch [70/100], Step [110/150], Loss: 16.4205\n",
            "Epoch [70/100], Step [120/150], Loss: 16.4583\n",
            "Epoch [70/100], Step [130/150], Loss: 16.5024\n",
            "Epoch [70/100], Step [140/150], Loss: 16.6406\n",
            "Epoch [70/100], Step [150/150], Loss: 16.6667\n",
            "Epoch [70/100], Loss: 16.6667\n",
            "Test Accuracy: 83.33%\n",
            "Epoch [71/100], Step [10/150], Loss: 18.1250\n",
            "Epoch [71/100], Step [20/150], Loss: 17.2656\n",
            "Epoch [71/100], Step [30/150], Loss: 17.1354\n",
            "Epoch [71/100], Step [40/150], Loss: 17.6562\n",
            "Epoch [71/100], Step [50/150], Loss: 17.2812\n",
            "Epoch [71/100], Step [60/150], Loss: 17.5781\n",
            "Epoch [71/100], Step [70/150], Loss: 17.4777\n",
            "Epoch [71/100], Step [80/150], Loss: 17.2266\n",
            "Epoch [71/100], Step [90/150], Loss: 17.1354\n",
            "Epoch [71/100], Step [100/150], Loss: 16.8438\n",
            "Epoch [71/100], Step [110/150], Loss: 16.8040\n",
            "Epoch [71/100], Step [120/150], Loss: 16.8490\n",
            "Epoch [71/100], Step [130/150], Loss: 16.7308\n",
            "Epoch [71/100], Step [140/150], Loss: 16.7299\n",
            "Epoch [71/100], Step [150/150], Loss: 16.6667\n",
            "Epoch [71/100], Loss: 16.6667\n",
            "Test Accuracy: 83.33%\n",
            "Epoch [72/100], Step [10/150], Loss: 15.9375\n",
            "Epoch [72/100], Step [20/150], Loss: 14.7656\n",
            "Epoch [72/100], Step [30/150], Loss: 15.1042\n",
            "Epoch [72/100], Step [40/150], Loss: 15.2344\n",
            "Epoch [72/100], Step [50/150], Loss: 15.5938\n",
            "Epoch [72/100], Step [60/150], Loss: 15.9635\n",
            "Epoch [72/100], Step [70/150], Loss: 16.4732\n",
            "Epoch [72/100], Step [80/150], Loss: 16.3086\n",
            "Epoch [72/100], Step [90/150], Loss: 16.4410\n",
            "Epoch [72/100], Step [100/150], Loss: 16.5781\n",
            "Epoch [72/100], Step [110/150], Loss: 16.4347\n",
            "Epoch [72/100], Step [120/150], Loss: 16.5234\n",
            "Epoch [72/100], Step [130/150], Loss: 16.3822\n",
            "Epoch [72/100], Step [140/150], Loss: 16.4732\n",
            "Epoch [72/100], Step [150/150], Loss: 16.6667\n",
            "Epoch [72/100], Loss: 16.6667\n",
            "Test Accuracy: 83.33%\n",
            "Epoch [73/100], Step [10/150], Loss: 15.9375\n",
            "Epoch [73/100], Step [20/150], Loss: 16.7969\n",
            "Epoch [73/100], Step [30/150], Loss: 16.4583\n",
            "Epoch [73/100], Step [40/150], Loss: 16.2500\n",
            "Epoch [73/100], Step [50/150], Loss: 16.4375\n",
            "Epoch [73/100], Step [60/150], Loss: 16.2760\n",
            "Epoch [73/100], Step [70/150], Loss: 16.4732\n",
            "Epoch [73/100], Step [80/150], Loss: 16.8555\n",
            "Epoch [73/100], Step [90/150], Loss: 16.9792\n",
            "Epoch [73/100], Step [100/150], Loss: 17.0156\n",
            "Epoch [73/100], Step [110/150], Loss: 16.7330\n",
            "Epoch [73/100], Step [120/150], Loss: 16.7578\n",
            "Epoch [73/100], Step [130/150], Loss: 16.5505\n",
            "Epoch [73/100], Step [140/150], Loss: 16.5960\n",
            "Epoch [73/100], Step [150/150], Loss: 16.6667\n",
            "Epoch [73/100], Loss: 16.6667\n",
            "Test Accuracy: 83.33%\n",
            "Epoch [74/100], Step [10/150], Loss: 17.3438\n",
            "Epoch [74/100], Step [20/150], Loss: 17.3438\n",
            "Epoch [74/100], Step [30/150], Loss: 17.5521\n",
            "Epoch [74/100], Step [40/150], Loss: 17.1484\n",
            "Epoch [74/100], Step [50/150], Loss: 17.3750\n",
            "Epoch [74/100], Step [60/150], Loss: 17.4479\n",
            "Epoch [74/100], Step [70/150], Loss: 17.4330\n",
            "Epoch [74/100], Step [80/150], Loss: 17.1680\n",
            "Epoch [74/100], Step [90/150], Loss: 17.2222\n",
            "Epoch [74/100], Step [100/150], Loss: 17.1250\n",
            "Epoch [74/100], Step [110/150], Loss: 16.8324\n",
            "Epoch [74/100], Step [120/150], Loss: 16.6927\n",
            "Epoch [74/100], Step [130/150], Loss: 16.8510\n",
            "Epoch [74/100], Step [140/150], Loss: 16.5290\n",
            "Epoch [74/100], Step [150/150], Loss: 16.6667\n",
            "Epoch [74/100], Loss: 16.6667\n",
            "Test Accuracy: 83.33%\n",
            "Epoch [75/100], Step [10/150], Loss: 16.0938\n",
            "Epoch [75/100], Step [20/150], Loss: 16.3281\n",
            "Epoch [75/100], Step [30/150], Loss: 16.3542\n",
            "Epoch [75/100], Step [40/150], Loss: 17.2266\n",
            "Epoch [75/100], Step [50/150], Loss: 17.0938\n",
            "Epoch [75/100], Step [60/150], Loss: 16.9010\n",
            "Epoch [75/100], Step [70/150], Loss: 16.4286\n",
            "Epoch [75/100], Step [80/150], Loss: 16.5234\n",
            "Epoch [75/100], Step [90/150], Loss: 16.5451\n",
            "Epoch [75/100], Step [100/150], Loss: 16.6094\n",
            "Epoch [75/100], Step [110/150], Loss: 16.4489\n",
            "Epoch [75/100], Step [120/150], Loss: 16.4714\n",
            "Epoch [75/100], Step [130/150], Loss: 16.5024\n",
            "Epoch [75/100], Step [140/150], Loss: 16.5848\n",
            "Epoch [75/100], Step [150/150], Loss: 16.6667\n",
            "Epoch [75/100], Loss: 16.6667\n",
            "Test Accuracy: 83.33%\n",
            "Epoch [76/100], Step [10/150], Loss: 14.5312\n",
            "Epoch [76/100], Step [20/150], Loss: 14.3750\n",
            "Epoch [76/100], Step [30/150], Loss: 14.6354\n",
            "Epoch [76/100], Step [40/150], Loss: 15.5469\n",
            "Epoch [76/100], Step [50/150], Loss: 16.4062\n",
            "Epoch [76/100], Step [60/150], Loss: 16.3281\n",
            "Epoch [76/100], Step [70/150], Loss: 16.3839\n",
            "Epoch [76/100], Step [80/150], Loss: 16.8164\n",
            "Epoch [76/100], Step [90/150], Loss: 16.9097\n",
            "Epoch [76/100], Step [100/150], Loss: 16.7344\n",
            "Epoch [76/100], Step [110/150], Loss: 16.6903\n",
            "Epoch [76/100], Step [120/150], Loss: 16.5495\n",
            "Epoch [76/100], Step [130/150], Loss: 16.5024\n",
            "Epoch [76/100], Step [140/150], Loss: 16.5290\n",
            "Epoch [76/100], Step [150/150], Loss: 16.6667\n",
            "Epoch [76/100], Loss: 16.6667\n",
            "Test Accuracy: 83.33%\n",
            "Epoch [77/100], Step [10/150], Loss: 18.5938\n",
            "Epoch [77/100], Step [20/150], Loss: 18.9844\n",
            "Epoch [77/100], Step [30/150], Loss: 18.1250\n",
            "Epoch [77/100], Step [40/150], Loss: 18.3594\n",
            "Epoch [77/100], Step [50/150], Loss: 18.3125\n",
            "Epoch [77/100], Step [60/150], Loss: 17.8906\n",
            "Epoch [77/100], Step [70/150], Loss: 17.3884\n",
            "Epoch [77/100], Step [80/150], Loss: 17.3047\n",
            "Epoch [77/100], Step [90/150], Loss: 17.1701\n",
            "Epoch [77/100], Step [100/150], Loss: 16.8594\n",
            "Epoch [77/100], Step [110/150], Loss: 16.9744\n",
            "Epoch [77/100], Step [120/150], Loss: 16.9271\n",
            "Epoch [77/100], Step [130/150], Loss: 16.6707\n",
            "Epoch [77/100], Step [140/150], Loss: 16.6741\n",
            "Epoch [77/100], Step [150/150], Loss: 16.6667\n",
            "Epoch [77/100], Loss: 16.6667\n",
            "Test Accuracy: 83.33%\n",
            "Epoch [78/100], Step [10/150], Loss: 16.0938\n",
            "Epoch [78/100], Step [20/150], Loss: 16.7188\n",
            "Epoch [78/100], Step [30/150], Loss: 16.2500\n",
            "Epoch [78/100], Step [40/150], Loss: 16.1719\n",
            "Epoch [78/100], Step [50/150], Loss: 16.0938\n",
            "Epoch [78/100], Step [60/150], Loss: 16.2500\n",
            "Epoch [78/100], Step [70/150], Loss: 16.1384\n",
            "Epoch [78/100], Step [80/150], Loss: 16.3867\n",
            "Epoch [78/100], Step [90/150], Loss: 16.2847\n",
            "Epoch [78/100], Step [100/150], Loss: 16.2188\n",
            "Epoch [78/100], Step [110/150], Loss: 16.4205\n",
            "Epoch [78/100], Step [120/150], Loss: 16.2891\n",
            "Epoch [78/100], Step [130/150], Loss: 16.4423\n",
            "Epoch [78/100], Step [140/150], Loss: 16.6406\n",
            "Epoch [78/100], Step [150/150], Loss: 16.6667\n",
            "Epoch [78/100], Loss: 16.6667\n",
            "Test Accuracy: 83.33%\n",
            "Epoch [79/100], Step [10/150], Loss: 18.4375\n",
            "Epoch [79/100], Step [20/150], Loss: 18.5938\n",
            "Epoch [79/100], Step [30/150], Loss: 17.4479\n",
            "Epoch [79/100], Step [40/150], Loss: 17.2656\n",
            "Epoch [79/100], Step [50/150], Loss: 17.4688\n",
            "Epoch [79/100], Step [60/150], Loss: 17.2917\n",
            "Epoch [79/100], Step [70/150], Loss: 17.1875\n",
            "Epoch [79/100], Step [80/150], Loss: 16.6992\n",
            "Epoch [79/100], Step [90/150], Loss: 17.0660\n",
            "Epoch [79/100], Step [100/150], Loss: 17.0469\n",
            "Epoch [79/100], Step [110/150], Loss: 17.0597\n",
            "Epoch [79/100], Step [120/150], Loss: 16.9401\n",
            "Epoch [79/100], Step [130/150], Loss: 16.8029\n",
            "Epoch [79/100], Step [140/150], Loss: 16.6741\n",
            "Epoch [79/100], Step [150/150], Loss: 16.6667\n",
            "Epoch [79/100], Loss: 16.6667\n",
            "Test Accuracy: 83.33%\n",
            "Epoch [80/100], Step [10/150], Loss: 15.9375\n",
            "Epoch [80/100], Step [20/150], Loss: 16.8750\n",
            "Epoch [80/100], Step [30/150], Loss: 16.0938\n",
            "Epoch [80/100], Step [40/150], Loss: 15.8203\n",
            "Epoch [80/100], Step [50/150], Loss: 15.9375\n",
            "Epoch [80/100], Step [60/150], Loss: 15.9896\n",
            "Epoch [80/100], Step [70/150], Loss: 15.9821\n",
            "Epoch [80/100], Step [80/150], Loss: 16.2891\n",
            "Epoch [80/100], Step [90/150], Loss: 16.1806\n",
            "Epoch [80/100], Step [100/150], Loss: 16.2031\n",
            "Epoch [80/100], Step [110/150], Loss: 16.3494\n",
            "Epoch [80/100], Step [120/150], Loss: 16.3672\n",
            "Epoch [80/100], Step [130/150], Loss: 16.5264\n",
            "Epoch [80/100], Step [140/150], Loss: 16.6071\n",
            "Epoch [80/100], Step [150/150], Loss: 16.6667\n",
            "Epoch [80/100], Loss: 16.6667\n",
            "Test Accuracy: 83.33%\n",
            "Epoch [81/100], Step [10/150], Loss: 16.5625\n",
            "Epoch [81/100], Step [20/150], Loss: 17.1875\n",
            "Epoch [81/100], Step [30/150], Loss: 17.1875\n",
            "Epoch [81/100], Step [40/150], Loss: 17.3828\n",
            "Epoch [81/100], Step [50/150], Loss: 17.2812\n",
            "Epoch [81/100], Step [60/150], Loss: 17.0052\n",
            "Epoch [81/100], Step [70/150], Loss: 16.8973\n",
            "Epoch [81/100], Step [80/150], Loss: 16.7188\n",
            "Epoch [81/100], Step [90/150], Loss: 16.5278\n",
            "Epoch [81/100], Step [100/150], Loss: 16.5625\n",
            "Epoch [81/100], Step [110/150], Loss: 16.5483\n",
            "Epoch [81/100], Step [120/150], Loss: 16.4453\n",
            "Epoch [81/100], Step [130/150], Loss: 16.5505\n",
            "Epoch [81/100], Step [140/150], Loss: 16.6406\n",
            "Epoch [81/100], Step [150/150], Loss: 16.6667\n",
            "Epoch [81/100], Loss: 16.6667\n",
            "Test Accuracy: 83.33%\n",
            "Epoch [82/100], Step [10/150], Loss: 18.4375\n",
            "Epoch [82/100], Step [20/150], Loss: 16.5625\n",
            "Epoch [82/100], Step [30/150], Loss: 16.5104\n",
            "Epoch [82/100], Step [40/150], Loss: 17.1094\n",
            "Epoch [82/100], Step [50/150], Loss: 17.3750\n",
            "Epoch [82/100], Step [60/150], Loss: 16.9792\n",
            "Epoch [82/100], Step [70/150], Loss: 17.0312\n",
            "Epoch [82/100], Step [80/150], Loss: 16.7969\n",
            "Epoch [82/100], Step [90/150], Loss: 16.8576\n",
            "Epoch [82/100], Step [100/150], Loss: 16.6094\n",
            "Epoch [82/100], Step [110/150], Loss: 16.6619\n",
            "Epoch [82/100], Step [120/150], Loss: 16.5755\n",
            "Epoch [82/100], Step [130/150], Loss: 16.5625\n",
            "Epoch [82/100], Step [140/150], Loss: 16.5290\n",
            "Epoch [82/100], Step [150/150], Loss: 16.6667\n",
            "Epoch [82/100], Loss: 16.6667\n",
            "Test Accuracy: 83.33%\n",
            "Epoch [83/100], Step [10/150], Loss: 17.3438\n",
            "Epoch [83/100], Step [20/150], Loss: 18.5938\n",
            "Epoch [83/100], Step [30/150], Loss: 18.0729\n",
            "Epoch [83/100], Step [40/150], Loss: 17.2266\n",
            "Epoch [83/100], Step [50/150], Loss: 17.0000\n",
            "Epoch [83/100], Step [60/150], Loss: 16.6667\n",
            "Epoch [83/100], Step [70/150], Loss: 16.9196\n",
            "Epoch [83/100], Step [80/150], Loss: 17.2070\n",
            "Epoch [83/100], Step [90/150], Loss: 17.0312\n",
            "Epoch [83/100], Step [100/150], Loss: 16.9531\n",
            "Epoch [83/100], Step [110/150], Loss: 16.8608\n",
            "Epoch [83/100], Step [120/150], Loss: 16.8620\n",
            "Epoch [83/100], Step [130/150], Loss: 16.7668\n",
            "Epoch [83/100], Step [140/150], Loss: 16.7857\n",
            "Epoch [83/100], Step [150/150], Loss: 16.6667\n",
            "Epoch [83/100], Loss: 16.6667\n",
            "Test Accuracy: 83.33%\n",
            "Epoch [84/100], Step [10/150], Loss: 17.6562\n",
            "Epoch [84/100], Step [20/150], Loss: 16.5625\n",
            "Epoch [84/100], Step [30/150], Loss: 16.2500\n",
            "Epoch [84/100], Step [40/150], Loss: 16.5625\n",
            "Epoch [84/100], Step [50/150], Loss: 16.0312\n",
            "Epoch [84/100], Step [60/150], Loss: 16.3281\n",
            "Epoch [84/100], Step [70/150], Loss: 16.4062\n",
            "Epoch [84/100], Step [80/150], Loss: 16.0742\n",
            "Epoch [84/100], Step [90/150], Loss: 16.2153\n",
            "Epoch [84/100], Step [100/150], Loss: 16.3594\n",
            "Epoch [84/100], Step [110/150], Loss: 16.5483\n",
            "Epoch [84/100], Step [120/150], Loss: 16.5625\n",
            "Epoch [84/100], Step [130/150], Loss: 16.7909\n",
            "Epoch [84/100], Step [140/150], Loss: 16.7746\n",
            "Epoch [84/100], Step [150/150], Loss: 16.6667\n",
            "Epoch [84/100], Loss: 16.6667\n",
            "Test Accuracy: 83.33%\n",
            "Epoch [85/100], Step [10/150], Loss: 16.0938\n",
            "Epoch [85/100], Step [20/150], Loss: 16.1719\n",
            "Epoch [85/100], Step [30/150], Loss: 16.7188\n",
            "Epoch [85/100], Step [40/150], Loss: 16.3672\n",
            "Epoch [85/100], Step [50/150], Loss: 15.7188\n",
            "Epoch [85/100], Step [60/150], Loss: 16.2760\n",
            "Epoch [85/100], Step [70/150], Loss: 16.6964\n",
            "Epoch [85/100], Step [80/150], Loss: 16.7188\n",
            "Epoch [85/100], Step [90/150], Loss: 16.8576\n",
            "Epoch [85/100], Step [100/150], Loss: 16.6719\n",
            "Epoch [85/100], Step [110/150], Loss: 16.5625\n",
            "Epoch [85/100], Step [120/150], Loss: 16.5495\n",
            "Epoch [85/100], Step [130/150], Loss: 16.5264\n",
            "Epoch [85/100], Step [140/150], Loss: 16.6853\n",
            "Epoch [85/100], Step [150/150], Loss: 16.6667\n",
            "Epoch [85/100], Loss: 16.6667\n",
            "Test Accuracy: 83.33%\n",
            "Epoch [86/100], Step [10/150], Loss: 16.8750\n",
            "Epoch [86/100], Step [20/150], Loss: 17.0312\n",
            "Epoch [86/100], Step [30/150], Loss: 16.7188\n",
            "Epoch [86/100], Step [40/150], Loss: 16.9531\n",
            "Epoch [86/100], Step [50/150], Loss: 17.1875\n",
            "Epoch [86/100], Step [60/150], Loss: 16.6146\n",
            "Epoch [86/100], Step [70/150], Loss: 16.9196\n",
            "Epoch [86/100], Step [80/150], Loss: 16.6211\n",
            "Epoch [86/100], Step [90/150], Loss: 16.5625\n",
            "Epoch [86/100], Step [100/150], Loss: 16.3594\n",
            "Epoch [86/100], Step [110/150], Loss: 16.4062\n",
            "Epoch [86/100], Step [120/150], Loss: 16.5104\n",
            "Epoch [86/100], Step [130/150], Loss: 16.5625\n",
            "Epoch [86/100], Step [140/150], Loss: 16.6518\n",
            "Epoch [86/100], Step [150/150], Loss: 16.6667\n",
            "Epoch [86/100], Loss: 16.6667\n",
            "Test Accuracy: 83.33%\n",
            "Epoch [87/100], Step [10/150], Loss: 18.2812\n",
            "Epoch [87/100], Step [20/150], Loss: 17.0312\n",
            "Epoch [87/100], Step [30/150], Loss: 16.9271\n",
            "Epoch [87/100], Step [40/150], Loss: 16.7969\n",
            "Epoch [87/100], Step [50/150], Loss: 16.4375\n",
            "Epoch [87/100], Step [60/150], Loss: 16.5365\n",
            "Epoch [87/100], Step [70/150], Loss: 16.6741\n",
            "Epoch [87/100], Step [80/150], Loss: 16.3477\n",
            "Epoch [87/100], Step [90/150], Loss: 16.5625\n",
            "Epoch [87/100], Step [100/150], Loss: 16.6094\n",
            "Epoch [87/100], Step [110/150], Loss: 16.6193\n",
            "Epoch [87/100], Step [120/150], Loss: 16.6667\n",
            "Epoch [87/100], Step [130/150], Loss: 16.5986\n",
            "Epoch [87/100], Step [140/150], Loss: 16.5737\n",
            "Epoch [87/100], Step [150/150], Loss: 16.6667\n",
            "Epoch [87/100], Loss: 16.6667\n",
            "Test Accuracy: 83.33%\n",
            "Epoch [88/100], Step [10/150], Loss: 16.8750\n",
            "Epoch [88/100], Step [20/150], Loss: 15.7031\n",
            "Epoch [88/100], Step [30/150], Loss: 16.1979\n",
            "Epoch [88/100], Step [40/150], Loss: 16.0547\n",
            "Epoch [88/100], Step [50/150], Loss: 16.0625\n",
            "Epoch [88/100], Step [60/150], Loss: 16.5104\n",
            "Epoch [88/100], Step [70/150], Loss: 16.6741\n",
            "Epoch [88/100], Step [80/150], Loss: 16.8164\n",
            "Epoch [88/100], Step [90/150], Loss: 16.7188\n",
            "Epoch [88/100], Step [100/150], Loss: 16.5625\n",
            "Epoch [88/100], Step [110/150], Loss: 16.4773\n",
            "Epoch [88/100], Step [120/150], Loss: 16.5625\n",
            "Epoch [88/100], Step [130/150], Loss: 16.5385\n",
            "Epoch [88/100], Step [140/150], Loss: 16.5737\n",
            "Epoch [88/100], Step [150/150], Loss: 16.6667\n",
            "Epoch [88/100], Loss: 16.6667\n",
            "Test Accuracy: 83.33%\n",
            "Epoch [89/100], Step [10/150], Loss: 14.0625\n",
            "Epoch [89/100], Step [20/150], Loss: 14.6875\n",
            "Epoch [89/100], Step [30/150], Loss: 15.5729\n",
            "Epoch [89/100], Step [40/150], Loss: 16.6016\n",
            "Epoch [89/100], Step [50/150], Loss: 16.3438\n",
            "Epoch [89/100], Step [60/150], Loss: 16.1458\n",
            "Epoch [89/100], Step [70/150], Loss: 16.2723\n",
            "Epoch [89/100], Step [80/150], Loss: 16.2305\n",
            "Epoch [89/100], Step [90/150], Loss: 16.4931\n",
            "Epoch [89/100], Step [100/150], Loss: 16.3438\n",
            "Epoch [89/100], Step [110/150], Loss: 16.3920\n",
            "Epoch [89/100], Step [120/150], Loss: 16.4844\n",
            "Epoch [89/100], Step [130/150], Loss: 16.3822\n",
            "Epoch [89/100], Step [140/150], Loss: 16.6183\n",
            "Epoch [89/100], Step [150/150], Loss: 16.6667\n",
            "Epoch [89/100], Loss: 16.6667\n",
            "Test Accuracy: 83.33%\n",
            "Epoch [90/100], Step [10/150], Loss: 18.7500\n",
            "Epoch [90/100], Step [20/150], Loss: 16.5625\n",
            "Epoch [90/100], Step [30/150], Loss: 15.8333\n",
            "Epoch [90/100], Step [40/150], Loss: 16.2891\n",
            "Epoch [90/100], Step [50/150], Loss: 16.4688\n",
            "Epoch [90/100], Step [60/150], Loss: 16.7969\n",
            "Epoch [90/100], Step [70/150], Loss: 16.6518\n",
            "Epoch [90/100], Step [80/150], Loss: 16.3867\n",
            "Epoch [90/100], Step [90/150], Loss: 16.4931\n",
            "Epoch [90/100], Step [100/150], Loss: 16.6250\n",
            "Epoch [90/100], Step [110/150], Loss: 16.4773\n",
            "Epoch [90/100], Step [120/150], Loss: 16.7057\n",
            "Epoch [90/100], Step [130/150], Loss: 16.6587\n",
            "Epoch [90/100], Step [140/150], Loss: 16.7076\n",
            "Epoch [90/100], Step [150/150], Loss: 16.6667\n",
            "Epoch [90/100], Loss: 16.6667\n",
            "Test Accuracy: 83.33%\n",
            "Epoch [91/100], Step [10/150], Loss: 16.2500\n",
            "Epoch [91/100], Step [20/150], Loss: 17.1875\n",
            "Epoch [91/100], Step [30/150], Loss: 16.7708\n",
            "Epoch [91/100], Step [40/150], Loss: 16.9922\n",
            "Epoch [91/100], Step [50/150], Loss: 16.8125\n",
            "Epoch [91/100], Step [60/150], Loss: 16.7188\n",
            "Epoch [91/100], Step [70/150], Loss: 16.8527\n",
            "Epoch [91/100], Step [80/150], Loss: 16.9141\n",
            "Epoch [91/100], Step [90/150], Loss: 16.8576\n",
            "Epoch [91/100], Step [100/150], Loss: 16.6406\n",
            "Epoch [91/100], Step [110/150], Loss: 16.5057\n",
            "Epoch [91/100], Step [120/150], Loss: 16.5365\n",
            "Epoch [91/100], Step [130/150], Loss: 16.8510\n",
            "Epoch [91/100], Step [140/150], Loss: 16.6518\n",
            "Epoch [91/100], Step [150/150], Loss: 16.6667\n",
            "Epoch [91/100], Loss: 16.6667\n",
            "Test Accuracy: 83.33%\n",
            "Epoch [92/100], Step [10/150], Loss: 15.3125\n",
            "Epoch [92/100], Step [20/150], Loss: 14.6875\n",
            "Epoch [92/100], Step [30/150], Loss: 15.0521\n",
            "Epoch [92/100], Step [40/150], Loss: 15.8594\n",
            "Epoch [92/100], Step [50/150], Loss: 16.2188\n",
            "Epoch [92/100], Step [60/150], Loss: 16.7969\n",
            "Epoch [92/100], Step [70/150], Loss: 16.5179\n",
            "Epoch [92/100], Step [80/150], Loss: 16.7188\n",
            "Epoch [92/100], Step [90/150], Loss: 16.7882\n",
            "Epoch [92/100], Step [100/150], Loss: 16.7344\n",
            "Epoch [92/100], Step [110/150], Loss: 16.6903\n",
            "Epoch [92/100], Step [120/150], Loss: 16.7839\n",
            "Epoch [92/100], Step [130/150], Loss: 16.6947\n",
            "Epoch [92/100], Step [140/150], Loss: 16.6741\n",
            "Epoch [92/100], Step [150/150], Loss: 16.6667\n",
            "Epoch [92/100], Loss: 16.6667\n",
            "Test Accuracy: 83.33%\n",
            "Epoch [93/100], Step [10/150], Loss: 16.8750\n",
            "Epoch [93/100], Step [20/150], Loss: 16.7188\n",
            "Epoch [93/100], Step [30/150], Loss: 16.5104\n",
            "Epoch [93/100], Step [40/150], Loss: 16.4844\n",
            "Epoch [93/100], Step [50/150], Loss: 16.2812\n",
            "Epoch [93/100], Step [60/150], Loss: 16.4062\n",
            "Epoch [93/100], Step [70/150], Loss: 16.5625\n",
            "Epoch [93/100], Step [80/150], Loss: 16.7578\n",
            "Epoch [93/100], Step [90/150], Loss: 17.1181\n",
            "Epoch [93/100], Step [100/150], Loss: 16.8125\n",
            "Epoch [93/100], Step [110/150], Loss: 16.7898\n",
            "Epoch [93/100], Step [120/150], Loss: 16.6536\n",
            "Epoch [93/100], Step [130/150], Loss: 16.5745\n",
            "Epoch [93/100], Step [140/150], Loss: 16.7522\n",
            "Epoch [93/100], Step [150/150], Loss: 16.6667\n",
            "Epoch [93/100], Loss: 16.6667\n",
            "Test Accuracy: 83.33%\n",
            "Epoch [94/100], Step [10/150], Loss: 16.5625\n",
            "Epoch [94/100], Step [20/150], Loss: 14.8438\n",
            "Epoch [94/100], Step [30/150], Loss: 15.6771\n",
            "Epoch [94/100], Step [40/150], Loss: 15.6641\n",
            "Epoch [94/100], Step [50/150], Loss: 16.0938\n",
            "Epoch [94/100], Step [60/150], Loss: 16.3802\n",
            "Epoch [94/100], Step [70/150], Loss: 16.4955\n",
            "Epoch [94/100], Step [80/150], Loss: 16.6211\n",
            "Epoch [94/100], Step [90/150], Loss: 16.7014\n",
            "Epoch [94/100], Step [100/150], Loss: 16.7344\n",
            "Epoch [94/100], Step [110/150], Loss: 16.7898\n",
            "Epoch [94/100], Step [120/150], Loss: 16.9141\n",
            "Epoch [94/100], Step [130/150], Loss: 16.8029\n",
            "Epoch [94/100], Step [140/150], Loss: 16.7188\n",
            "Epoch [94/100], Step [150/150], Loss: 16.6667\n",
            "Epoch [94/100], Loss: 16.6667\n",
            "Test Accuracy: 83.33%\n",
            "Epoch [95/100], Step [10/150], Loss: 13.9062\n",
            "Epoch [95/100], Step [20/150], Loss: 15.3906\n",
            "Epoch [95/100], Step [30/150], Loss: 15.6250\n",
            "Epoch [95/100], Step [40/150], Loss: 16.0938\n",
            "Epoch [95/100], Step [50/150], Loss: 16.5312\n",
            "Epoch [95/100], Step [60/150], Loss: 16.3281\n",
            "Epoch [95/100], Step [70/150], Loss: 16.4509\n",
            "Epoch [95/100], Step [80/150], Loss: 16.6016\n",
            "Epoch [95/100], Step [90/150], Loss: 16.7188\n",
            "Epoch [95/100], Step [100/150], Loss: 16.7344\n",
            "Epoch [95/100], Step [110/150], Loss: 16.8040\n",
            "Epoch [95/100], Step [120/150], Loss: 16.8229\n",
            "Epoch [95/100], Step [130/150], Loss: 16.7788\n",
            "Epoch [95/100], Step [140/150], Loss: 16.7299\n",
            "Epoch [95/100], Step [150/150], Loss: 16.6667\n",
            "Epoch [95/100], Loss: 16.6667\n",
            "Test Accuracy: 83.33%\n",
            "Epoch [96/100], Step [10/150], Loss: 18.2812\n",
            "Epoch [96/100], Step [20/150], Loss: 15.5469\n",
            "Epoch [96/100], Step [30/150], Loss: 16.1458\n",
            "Epoch [96/100], Step [40/150], Loss: 16.6797\n",
            "Epoch [96/100], Step [50/150], Loss: 16.4688\n",
            "Epoch [96/100], Step [60/150], Loss: 16.9271\n",
            "Epoch [96/100], Step [70/150], Loss: 16.4286\n",
            "Epoch [96/100], Step [80/150], Loss: 16.4258\n",
            "Epoch [96/100], Step [90/150], Loss: 16.5972\n",
            "Epoch [96/100], Step [100/150], Loss: 16.8281\n",
            "Epoch [96/100], Step [110/150], Loss: 16.8182\n",
            "Epoch [96/100], Step [120/150], Loss: 16.6797\n",
            "Epoch [96/100], Step [130/150], Loss: 16.6226\n",
            "Epoch [96/100], Step [140/150], Loss: 16.6406\n",
            "Epoch [96/100], Step [150/150], Loss: 16.6667\n",
            "Epoch [96/100], Loss: 16.6667\n",
            "Test Accuracy: 83.33%\n",
            "Epoch [97/100], Step [10/150], Loss: 17.6562\n",
            "Epoch [97/100], Step [20/150], Loss: 15.7031\n",
            "Epoch [97/100], Step [30/150], Loss: 15.1562\n",
            "Epoch [97/100], Step [40/150], Loss: 15.4297\n",
            "Epoch [97/100], Step [50/150], Loss: 15.6250\n",
            "Epoch [97/100], Step [60/150], Loss: 15.8854\n",
            "Epoch [97/100], Step [70/150], Loss: 16.0714\n",
            "Epoch [97/100], Step [80/150], Loss: 16.3477\n",
            "Epoch [97/100], Step [90/150], Loss: 16.6840\n",
            "Epoch [97/100], Step [100/150], Loss: 16.7031\n",
            "Epoch [97/100], Step [110/150], Loss: 16.6619\n",
            "Epoch [97/100], Step [120/150], Loss: 16.8229\n",
            "Epoch [97/100], Step [130/150], Loss: 16.7188\n",
            "Epoch [97/100], Step [140/150], Loss: 16.6071\n",
            "Epoch [97/100], Step [150/150], Loss: 16.6667\n",
            "Epoch [97/100], Loss: 16.6667\n",
            "Test Accuracy: 83.33%\n",
            "Epoch [98/100], Step [10/150], Loss: 15.1562\n",
            "Epoch [98/100], Step [20/150], Loss: 15.3906\n",
            "Epoch [98/100], Step [30/150], Loss: 15.7292\n",
            "Epoch [98/100], Step [40/150], Loss: 15.9375\n",
            "Epoch [98/100], Step [50/150], Loss: 15.7500\n",
            "Epoch [98/100], Step [60/150], Loss: 16.0156\n",
            "Epoch [98/100], Step [70/150], Loss: 15.9375\n",
            "Epoch [98/100], Step [80/150], Loss: 16.5430\n",
            "Epoch [98/100], Step [90/150], Loss: 16.4931\n",
            "Epoch [98/100], Step [100/150], Loss: 16.5469\n",
            "Epoch [98/100], Step [110/150], Loss: 16.4773\n",
            "Epoch [98/100], Step [120/150], Loss: 16.3281\n",
            "Epoch [98/100], Step [130/150], Loss: 16.5264\n",
            "Epoch [98/100], Step [140/150], Loss: 16.6183\n",
            "Epoch [98/100], Step [150/150], Loss: 16.6667\n",
            "Epoch [98/100], Loss: 16.6667\n",
            "Test Accuracy: 83.33%\n",
            "Epoch [99/100], Step [10/150], Loss: 17.9688\n",
            "Epoch [99/100], Step [20/150], Loss: 19.1406\n",
            "Epoch [99/100], Step [30/150], Loss: 18.3333\n",
            "Epoch [99/100], Step [40/150], Loss: 17.6562\n",
            "Epoch [99/100], Step [50/150], Loss: 17.4688\n",
            "Epoch [99/100], Step [60/150], Loss: 17.5521\n",
            "Epoch [99/100], Step [70/150], Loss: 17.2768\n",
            "Epoch [99/100], Step [80/150], Loss: 17.0117\n",
            "Epoch [99/100], Step [90/150], Loss: 17.0312\n",
            "Epoch [99/100], Step [100/150], Loss: 17.2344\n",
            "Epoch [99/100], Step [110/150], Loss: 17.0881\n",
            "Epoch [99/100], Step [120/150], Loss: 16.9531\n",
            "Epoch [99/100], Step [130/150], Loss: 16.9471\n",
            "Epoch [99/100], Step [140/150], Loss: 16.8527\n",
            "Epoch [99/100], Step [150/150], Loss: 16.6667\n",
            "Epoch [99/100], Loss: 16.6667\n",
            "Test Accuracy: 83.33%\n",
            "Epoch [100/100], Step [10/150], Loss: 18.5938\n",
            "Epoch [100/100], Step [20/150], Loss: 18.0469\n",
            "Epoch [100/100], Step [30/150], Loss: 17.3438\n",
            "Epoch [100/100], Step [40/150], Loss: 17.3438\n",
            "Epoch [100/100], Step [50/150], Loss: 16.9688\n",
            "Epoch [100/100], Step [60/150], Loss: 16.6146\n",
            "Epoch [100/100], Step [70/150], Loss: 16.7188\n",
            "Epoch [100/100], Step [80/150], Loss: 16.6406\n",
            "Epoch [100/100], Step [90/150], Loss: 16.3889\n",
            "Epoch [100/100], Step [100/150], Loss: 16.4844\n",
            "Epoch [100/100], Step [110/150], Loss: 16.4631\n",
            "Epoch [100/100], Step [120/150], Loss: 16.5755\n",
            "Epoch [100/100], Step [130/150], Loss: 16.5865\n",
            "Epoch [100/100], Step [140/150], Loss: 16.6518\n",
            "Epoch [100/100], Step [150/150], Loss: 16.6667\n",
            "Epoch [100/100], Loss: 16.6667\n",
            "Test Accuracy: 83.33%\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "class CNNClassifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNNClassifier, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(30, 64, kernel_size=(3, 3), stride=1, padding=1)\n",
        "        self.conv2 = nn.Conv2d(64, 128, kernel_size=(3, 3), stride=1, padding=1)\n",
        "        self.fc1 = nn.Linear(6144, 256)\n",
        "        self.fc2 = nn.Linear(256, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = nn.ReLU()(self.conv1(x))\n",
        "        x = nn.MaxPool2d(kernel_size=(2, 2))(x)\n",
        "        x = nn.ReLU()(self.conv2(x))\n",
        "        x = nn.MaxPool2d(kernel_size=(2, 2))(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = nn.ReLU()(self.fc1(x))\n",
        "        x = self.sigmoid(self.fc2(x))\n",
        "        return x\n",
        "\n",
        "model = CNNClassifier().to(device)\n",
        "\n",
        "# Funzione di perdita e ottimizzatore\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "# Ciclo di addestramento\n",
        "num_epochs = 100\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for i, (inputs, labels) in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        inputs = inputs.permute(0, 2, 3, 1).contiguous()\n",
        "        outputs = model(inputs)\n",
        "        labels = labels.view(-1, 1)\n",
        "        \n",
        "        loss = criterion(outputs, labels)\n",
        "        \n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        running_loss += loss.item()\n",
        "        \n",
        "        if (i + 1) % 10 == 0:\n",
        "            print(f'Epoch [{epoch + 1}/{num_epochs}], Step [{i + 1}/{len(train_loader)}], Loss: {running_loss / (i + 1):.4f}')\n",
        "    \n",
        "    print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {running_loss / len(train_loader):.4f}')\n",
        "\n",
        "# Valutazione del modello\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs = inputs.permute(0, 2, 3, 1).contiguous()\n",
        "            outputs = model(inputs)\n",
        "            predicted = (outputs > 0.5).float()\n",
        "            labels = labels.view(-1, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "        print(f'Test Accuracy: {100 * correct / total:.2f}%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/20], Step [10/16], Loss: 0.5643\n",
            "Epoch [1/20], Loss: 0.5269\n",
            "Test Accuracy: 84.50%\n",
            "Epoch [2/20], Step [10/16], Loss: 0.4600\n",
            "Epoch [2/20], Loss: 0.4603\n",
            "Test Accuracy: 84.50%\n",
            "Epoch [3/20], Step [10/16], Loss: 0.4546\n",
            "Epoch [3/20], Loss: 0.4470\n",
            "Test Accuracy: 84.50%\n",
            "Epoch [4/20], Step [10/16], Loss: 0.4525\n",
            "Epoch [4/20], Loss: 0.4418\n",
            "Test Accuracy: 84.50%\n",
            "Epoch [5/20], Step [10/16], Loss: 0.4694\n",
            "Epoch [5/20], Loss: 0.4448\n",
            "Test Accuracy: 84.50%\n",
            "Epoch [6/20], Step [10/16], Loss: 0.4084\n",
            "Epoch [6/20], Loss: 0.4399\n",
            "Test Accuracy: 84.00%\n",
            "Epoch [7/20], Step [10/16], Loss: 0.4398\n",
            "Epoch [7/20], Loss: 0.4373\n",
            "Test Accuracy: 84.00%\n",
            "Epoch [8/20], Step [10/16], Loss: 0.4189\n",
            "Epoch [8/20], Loss: 0.4415\n",
            "Test Accuracy: 84.00%\n",
            "Epoch [9/20], Step [10/16], Loss: 0.4332\n",
            "Epoch [9/20], Loss: 0.4349\n",
            "Test Accuracy: 84.00%\n",
            "Epoch [10/20], Step [10/16], Loss: 0.4289\n",
            "Epoch [10/20], Loss: 0.4340\n",
            "Test Accuracy: 84.50%\n",
            "Epoch [11/20], Step [10/16], Loss: 0.4224\n",
            "Epoch [11/20], Loss: 0.4250\n",
            "Test Accuracy: 84.00%\n",
            "Epoch [12/20], Step [10/16], Loss: 0.4222\n",
            "Epoch [12/20], Loss: 0.4246\n",
            "Test Accuracy: 84.00%\n",
            "Epoch [13/20], Step [10/16], Loss: 0.4298\n",
            "Epoch [13/20], Loss: 0.4238\n",
            "Test Accuracy: 84.00%\n",
            "Epoch [14/20], Step [10/16], Loss: 0.4334\n",
            "Epoch [14/20], Loss: 0.4238\n",
            "Test Accuracy: 84.00%\n",
            "Epoch [15/20], Step [10/16], Loss: 0.4174\n",
            "Epoch [15/20], Loss: 0.4146\n",
            "Test Accuracy: 83.50%\n",
            "Epoch [16/20], Step [10/16], Loss: 0.3985\n",
            "Epoch [16/20], Loss: 0.4138\n",
            "Test Accuracy: 83.50%\n",
            "Epoch [17/20], Step [10/16], Loss: 0.3637\n",
            "Epoch [17/20], Loss: 0.4127\n",
            "Test Accuracy: 83.50%\n",
            "Epoch [18/20], Step [10/16], Loss: 0.4081\n",
            "Epoch [18/20], Loss: 0.4139\n",
            "Test Accuracy: 84.00%\n",
            "Epoch [19/20], Step [10/16], Loss: 0.4016\n",
            "Epoch [19/20], Loss: 0.4236\n",
            "Test Accuracy: 81.00%\n",
            "Epoch [20/20], Step [10/16], Loss: 0.4384\n",
            "Epoch [20/20], Loss: 0.4326\n",
            "Test Accuracy: 84.00%\n"
          ]
        }
      ],
      "source": [
        "X_train_tensor = torch.tensor(X_train_small.reshape((X_train_small.shape[0], -1, 5)), dtype=torch.float32).to(device)\n",
        "y_train_tensor = torch.tensor(y_train_small, dtype=torch.float32).to(device)\n",
        "X_test_tensor = torch.tensor(X_test_small.reshape((X_test_small.shape[0], -1, 5)), dtype=torch.float32).to(device)\n",
        "y_test_tensor = torch.tensor(y_test_small, dtype=torch.float32).to(device)\n",
        "\n",
        "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "# Definizione del modello LSTM\n",
        "class LSTMClassifier(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim):\n",
        "        super(LSTMClassifier, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.layer_dim = layer_dim\n",
        "        self.lstm = nn.LSTM(input_dim, hidden_dim, layer_dim, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "    \n",
        "    def forward(self, x):\n",
        "        h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).to(x.device)\n",
        "        c0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).to(x.device)\n",
        "        \n",
        "        out, _ = self.lstm(x, (h0, c0))\n",
        "        \n",
        "        out = self.fc(out[:, -1, :])\n",
        "        out = self.sigmoid(out)\n",
        "        return out\n",
        "\n",
        "input_dim = 5\n",
        "hidden_dim = 128\n",
        "layer_dim = 2\n",
        "output_dim = 1\n",
        "\n",
        "model = LSTMClassifier(input_dim, hidden_dim, layer_dim, output_dim).to(device)\n",
        "\n",
        "# Definizione della funzione di perdita e dell'ottimizzatore\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Ciclo di addestramento\n",
        "num_epochs = 20\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for i, (inputs, labels) in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        outputs = model(inputs)\n",
        "        labels = labels.view(-1, 1)\n",
        "        \n",
        "        loss = criterion(outputs, labels)\n",
        "        \n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        running_loss += loss.item()\n",
        "        \n",
        "        if (i + 1) % 10 == 0:\n",
        "            print(f'Epoch [{epoch + 1}/{num_epochs}], Step [{i + 1}/{len(train_loader)}], Loss: {running_loss / (i + 1):.4f}')\n",
        "    \n",
        "    print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {running_loss / len(train_loader):.4f}')\n",
        "\n",
        "# Valutazione del modello\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for inputs, labels in test_loader:\n",
        "            outputs = model(inputs)\n",
        "            predicted = (outputs > 0.5).float()\n",
        "            labels = labels.view(-1, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "        print(f'Test Accuracy: {100 * correct / total:.2f}%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[[ 3.30937095e-02 -4.96353908e-03 -1.52720032e+01 -3.96207733e+01\n",
            "    3.12500000e-02]\n",
            "  [-8.89009908e-02  9.58854891e-03  2.03179493e+01  3.79275589e+01\n",
            "    9.89583358e-02]\n",
            "  [ 4.81614470e-02  7.81319104e-05  2.25106564e+01  3.38749657e+01\n",
            "    9.89583358e-02]\n",
            "  ...\n",
            "  [-1.15359358e-01  7.73958722e-03  1.24924879e+01 -9.38453436e-01\n",
            "    4.68750000e-02]\n",
            "  [-1.13427095e-01  6.99999882e-03  1.20373526e+01 -9.66182411e-01\n",
            "    4.68750000e-02]\n",
            "  [-1.06697895e-01  5.17187966e-03  1.17136459e+01 -9.78239834e-01\n",
            "    4.68750000e-02]]\n",
            "\n",
            " [[ 3.58750112e-02  1.41458316e-02  1.94359932e+01 -1.62199485e+00\n",
            "    9.37500000e-02]\n",
            "  [ 9.52760577e-02  7.95833115e-03  2.62669621e+01 -2.04067683e+00\n",
            "    1.04166664e-01]\n",
            "  [ 6.74749613e-01  6.36354238e-02  3.88198586e+01  1.39930165e+00\n",
            "    1.56250000e-02]\n",
            "  ...\n",
            "  [ 9.04536724e-01  6.17187321e-02  2.13479786e+01 -5.61791718e-01\n",
            "    7.29166642e-02]\n",
            "  [ 8.67734492e-01  4.40156199e-02  1.86060371e+01 -8.93166959e-01\n",
            "    7.29166642e-02]\n",
            "  [ 4.36104268e-01  7.89062027e-03  3.15551090e+01 -3.14036489e-01\n",
            "    2.08333340e-02]]\n",
            "\n",
            " [[ 8.33959654e-02 -2.44791638e-02  2.05586987e+01 -2.04470277e+00\n",
            "    4.16666679e-02]\n",
            "  [-2.37186793e-02 -4.73645777e-02  2.10075207e+01 -1.93177569e+00\n",
            "    4.16666679e-02]\n",
            "  [-6.93218708e-01  1.53125264e-03 -2.65564270e+01 -3.42814064e+00\n",
            "    6.77083358e-02]\n",
            "  ...\n",
            "  [ 6.36458583e-03  5.32291690e-03  1.60685978e+01 -2.66911447e-01\n",
            "    4.16666679e-02]\n",
            "  [ 7.68228807e-03  4.44270764e-03  1.53815355e+01 -2.81921893e-01\n",
            "    4.16666679e-02]\n",
            "  [ 1.00885397e-02  2.50520860e-03  1.47059107e+01 -2.90880263e-01\n",
            "    4.16666679e-02]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-6.05026007e-01  1.10937119e-03  1.22345314e+01 -4.96913433e+00\n",
            "    4.68750000e-02]\n",
            "  [-6.05015576e-01  1.13019475e-03  1.36594687e+01 -4.91499090e+00\n",
            "    4.68750000e-02]\n",
            "  [-6.05026007e-01  1.10937119e-03  1.56092148e+01 -4.83593082e+00\n",
            "    4.68750000e-02]\n",
            "  ...\n",
            "  [ 4.23140734e-01  7.58906081e-02  1.03829565e+01 -8.27654648e+00\n",
            "    7.81250000e-02]\n",
            "  [ 4.70000118e-01  7.41979033e-02  9.40138721e+00 -8.53924370e+00\n",
            "    7.29166642e-02]\n",
            "  [ 2.38734484e-01  7.58749917e-02  7.71146250e+00 -9.36296940e+00\n",
            "    5.72916679e-02]]\n",
            "\n",
            " [[-8.34828317e-01 -6.10416336e-03 -2.07496109e+01 -3.28262019e+00\n",
            "    2.60416660e-02]\n",
            "  [-8.26760530e-01 -8.07812251e-03 -2.02596302e+01 -3.33298469e+00\n",
            "    2.60416660e-02]\n",
            "  [-8.15041780e-01 -6.97916187e-03 -1.96718979e+01 -3.38631749e+00\n",
            "    2.60416660e-02]\n",
            "  ...\n",
            "  [ 7.57281542e-01  5.65885454e-02  7.01931763e+01 -4.20788002e+00\n",
            "    2.60416660e-02]\n",
            "  [-2.64979184e-01  1.95833878e-03  3.37105727e+00 -3.12529254e+00\n",
            "    4.68750000e-02]\n",
            "  [-3.74041706e-01  7.55729293e-03  4.09711266e+00 -3.19520283e+00\n",
            "    4.68750000e-02]]\n",
            "\n",
            " [[ 7.30999947e-01  3.84374964e-03 -1.30381250e+01 -8.39015782e-01\n",
            "    1.56250000e-02]\n",
            "  [ 1.74411476e-01  1.47552108e-02  1.03269796e+01  9.80948210e-01\n",
            "    2.60416660e-02]\n",
            "  [-6.17255211e-01  3.48750018e-02  3.51693840e+01 -4.83723927e+00\n",
            "    5.20833349e-03]\n",
            "  ...\n",
            "  [ 9.81406271e-02 -1.73437456e-03  3.53492355e+01 -4.34542799e+00\n",
            "    5.20833349e-03]\n",
            "  [-1.30802095e-01 -8.85416288e-04 -1.84085560e+01 -5.73703051e+00\n",
            "    1.56250000e-02]\n",
            "  [ 1.56093732e-01  4.74478956e-03  5.43406439e+00 -1.95510387e-01\n",
            "    1.04166670e-02]]]\n",
            "[[ 3.30937095e-02 -4.96353908e-03 -1.52720032e+01 -3.96207733e+01\n",
            "   3.12500000e-02]\n",
            " [-8.89009908e-02  9.58854891e-03  2.03179493e+01  3.79275589e+01\n",
            "   9.89583358e-02]\n",
            " [ 4.81614470e-02  7.81319104e-05  2.25106564e+01  3.38749657e+01\n",
            "   9.89583358e-02]\n",
            " [-8.15000013e-02  1.99114624e-02 -2.25800443e+00 -1.29760420e+00\n",
            "   4.16666679e-02]\n",
            " [-8.28749985e-02  1.87968779e-02 -2.15396786e+00 -1.41384900e+00\n",
            "   4.16666679e-02]\n",
            " [ 7.84385443e-01  2.81249988e-03 -2.91387806e+01 -4.63051796e-01\n",
            "   2.08333340e-02]\n",
            " [ 3.47505003e-01 -3.65416817e-02  1.86489239e+01 -3.33413529e+00\n",
            "   6.77083358e-02]\n",
            " [ 3.10005218e-01 -3.63281332e-02  1.71435337e+01 -3.32584286e+00\n",
            "   7.81250000e-02]\n",
            " [ 2.68744797e-01 -4.32239659e-02  1.59583368e+01 -3.30270195e+00\n",
            "   7.81250000e-02]\n",
            " [ 2.19791695e-01 -5.02916761e-02  1.48630610e+01 -3.25895214e+00\n",
            "   7.81250000e-02]\n",
            " [-2.03651011e-01  1.11666648e-02 -1.74029160e+00 -2.35178161e+00\n",
            "   6.77083358e-02]\n",
            " [-2.07968876e-01  4.57812753e-03 -2.08121848e+00 -2.35580778e+00\n",
            "   7.29166642e-02]\n",
            " [-1.76703095e-01  2.69791973e-03 -2.81775022e+00 -2.31608367e+00\n",
            "   8.33333358e-02]\n",
            " [-1.75677061e-01  4.75521153e-03 -3.13665080e+00 -2.30360436e+00\n",
            "   8.85416642e-02]\n",
            " [-1.71078265e-01  9.60417558e-03  1.17213144e+01 -2.75764012e+00\n",
            "   1.14583336e-01]\n",
            " [-1.70713708e-01  7.48438062e-03  1.15975914e+01 -2.75325513e+00\n",
            "   1.14583336e-01]\n",
            " [-1.60619751e-01  8.35416652e-03  1.13917656e+01 -2.74681759e+00\n",
            "   1.14583336e-01]\n",
            " [-1.62906215e-01  1.04166660e-02  1.11595421e+01 -2.74908829e+00\n",
            "   1.14583336e-01]\n",
            " [-3.44895236e-02  1.26874894e-02  1.10091865e+00 -4.24073029e+00\n",
            "   7.81250000e-02]\n",
            " [-1.90328076e-01  3.00468672e-02  1.80385932e-01 -5.06626177e+00\n",
            "   7.81250000e-02]\n",
            " [-2.50296801e-01 -2.77447868e-02 -1.30473146e+01 -4.92700148e+00\n",
            "   4.68750000e-02]\n",
            " [-1.98114559e-01 -3.18854265e-02 -1.20194101e+01 -4.88397932e+00\n",
            "   4.68750000e-02]\n",
            " [-1.88156173e-01 -3.26822959e-02 -1.15566521e+01 -4.85286999e+00\n",
            "   4.68750000e-02]\n",
            " [-1.67557195e-01 -3.29114608e-02 -1.11303606e+01 -4.81408358e+00\n",
            "   4.68750000e-02]\n",
            " [-5.75625040e-02  6.35625198e-02  5.92758942e+00 -2.75225472e+00\n",
            "   6.77083358e-02]\n",
            " [-3.29312533e-01 -2.25001201e-03  6.78788424e+00 -2.62061453e+00\n",
            "   1.04166664e-01]\n",
            " [-1.95802152e-01 -1.05520906e-02  9.37450504e+00 -2.56105137e+00\n",
            "   1.04166664e-01]\n",
            " [-1.15359358e-01  7.73958722e-03  1.24924879e+01 -9.38453436e-01\n",
            "   4.68750000e-02]\n",
            " [-1.13427095e-01  6.99999882e-03  1.20373526e+01 -9.66182411e-01\n",
            "   4.68750000e-02]\n",
            " [-1.06697895e-01  5.17187966e-03  1.17136459e+01 -9.78239834e-01\n",
            "   4.68750000e-02]]\n",
            "300\n"
          ]
        }
      ],
      "source": [
        "medie = np.mean(data, axis=(2))\n",
        "varianza = np.mean(data, axis=(2))\n",
        "medie_cheaters = np.mean(data_cheater, axis=(2))\n",
        "varianza_cheaters = np.mean(data_cheater, axis=(2))\n",
        "\n",
        "print(medie)\n",
        "print(medie[0])\n",
        "#concatenazione \n",
        "result=np.stack((medie,varianza), axis=1)\n",
        "result_cheater=np.stack((medie_cheaters,varianza_cheaters), axis=1)\n",
        "total = np.concatenate((result, result_cheater), axis=0)\n",
        "\n",
        "labels_legit = np.zeros(len(result))\n",
        "labels_cheater=np.ones(len(result_cheater))\n",
        "labels = np.concatenate((labels_legit, labels_cheater))\n",
        "\n",
        "#reshape dei dati per la SVM\n",
        "X = total.reshape(12000, -1)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.20, random_state=42, stratify=labels)\n",
        "\n",
        "#rescaling dei dati\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = X_train\n",
        "X_test_scaled = X_test\n",
        "\n",
        "print(X_train_scaled.shape[1])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([9600, 1, 300])\n",
            "torch.Size([9600])\n"
          ]
        }
      ],
      "source": [
        "X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32).view(-1,1,X_test_scaled.shape[1])\n",
        "X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32).view(-1,1,X_test_scaled.shape[1])\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
        "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "train_data = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
        "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "print(X_train_tensor.shape)\n",
        "print(y_train_tensor.shape)\n",
        "def get_accuracy(logit, target):\n",
        "    corrects = (torch.max(logit, 1)[1].view(target.size()).data == target.data).sum()\n",
        "    accuracy = 100.0 * corrects / target.size(0)\n",
        "    return accuracy.item()\n",
        "\n",
        "def compute_weight_norm(model):\n",
        "    norm = 0.0\n",
        "    for name, param in model.named_parameters():\n",
        "      if \"weight\" in name:\n",
        "        norm += torch.norm(param.data, p= 2)\n",
        "    return norm.cpu().item()\n",
        "\n",
        "def train_model(model, num_epochs, trainloader, criterion, optimizer):\n",
        "\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "    model = model.to(device)\n",
        "\n",
        "    losses = []\n",
        "    accs = []\n",
        "\n",
        "    norms = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        train_running_loss = 0.0\n",
        "        train_acc = 0.0\n",
        "\n",
        "        # Set the model to training mode\n",
        "        model.train()\n",
        "        ## training step\n",
        "        for i, (inputs, labels) in enumerate(trainloader):\n",
        "\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            ## forward + backprop + loss\n",
        "            logits = model(inputs)\n",
        "            loss = criterion(logits, labels)\n",
        "\n",
        "            # Reset the gradients to zero\n",
        "\n",
        "            loss.backward()\n",
        "\n",
        "            ## update model params\n",
        "            optimizer.step()\n",
        "\n",
        "            train_running_loss += loss.item()\n",
        "            train_acc += get_accuracy(logits, labels)\n",
        "\n",
        "\n",
        "        losses.append(train_running_loss / i)\n",
        "        accs.append(train_acc/i)\n",
        "\n",
        "        norms.append(compute_weight_norm(model))\n",
        "        model.eval()\n",
        "        print(f\"Epoch: {epoch+1} | Loss: {train_running_loss / i:.4f} | Train Accuracy: {train_acc/i:.4f}\")\n",
        "\n",
        "    return losses, accs, norms\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MLP, self).__init__()\n",
        "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=6, kernel_size=10, padding=1)\n",
        "        self.dropout1 = nn.Dropout(p=0.20)\n",
        "\n",
        "        self.conv2 = nn.Conv1d(in_channels=6, out_channels=18, kernel_size=10, padding=1)\n",
        "        self.dropout2 = nn.Dropout(p=0.20)\n",
        "\n",
        "\n",
        "        self.conv3 = nn.Conv1d(in_channels=18, out_channels=30, kernel_size=10, padding=1)\n",
        "        self.dropout3 = nn.Dropout(p=0.20)\n",
        "\n",
        "        self.flattern = nn.Flatten()\n",
        "\n",
        "        self.dropout4 = nn.Dropout(p=0.20)\n",
        "\n",
        "\n",
        "        self.fc1 = nn.Linear(8370, 1000)\n",
        "\n",
        "        self.fc2 = nn.Linear(1000, 500)\n",
        "\n",
        "        self.fc3 = nn.Linear(500,64)\n",
        "\n",
        "        self.fc4 = nn.Linear(64, 2)\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = self.dropout1(x)\n",
        "\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = self.dropout2(x)\n",
        "\n",
        "        x = F.relu(self.conv3(x))\n",
        "        x = self.dropout3(x)\n",
        "\n",
        "        x = x.flatten(start_dim = 1)\n",
        "\n",
        "        #x = self.dropout4\n",
        "\n",
        "        x = torch.relu(self.fc1(x))\n",
        "\n",
        "        x = torch.relu(self.fc2(x))\n",
        "\n",
        "        x = torch.relu(self.fc3(x))\n",
        "\n",
        "        x = (self.fc4(x))\n",
        "        return x\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 1 | Loss: 0.9374 | Train Accuracy: 83.5911\n",
            "Epoch: 2 | Loss: 0.4565 | Train Accuracy: 83.6120\n",
            "Epoch: 3 | Loss: 0.4531 | Train Accuracy: 83.6120\n",
            "Epoch: 4 | Loss: 0.4527 | Train Accuracy: 83.6120\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[88], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[0;32m      9\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m)\n\u001b[1;32m---> 11\u001b[0m _, _, W_adam \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[1;32mIn[86], line 61\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, num_epochs, trainloader, criterion, optimizer)\u001b[0m\n\u001b[0;32m     58\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     60\u001b[0m \u001b[38;5;66;03m## update model params\u001b[39;00m\n\u001b[1;32m---> 61\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     63\u001b[0m train_running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     64\u001b[0m train_acc \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m get_accuracy(logits, labels)\n",
            "File \u001b[1;32mc:\\Users\\gabri\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\optim\\optimizer.py:391\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    386\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    387\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    388\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    389\u001b[0m             )\n\u001b[1;32m--> 391\u001b[0m out \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    392\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    394\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\gabri\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\optim\\optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[1;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
            "File \u001b[1;32mc:\\Users\\gabri\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\optim\\adam.py:168\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    157\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m    159\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[0;32m    160\u001b[0m         group,\n\u001b[0;32m    161\u001b[0m         params_with_grad,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    165\u001b[0m         max_exp_avg_sqs,\n\u001b[0;32m    166\u001b[0m         state_steps)\n\u001b[1;32m--> 168\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    169\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    170\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    171\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    172\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    175\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    176\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    177\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    178\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    179\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    180\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    181\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    182\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    183\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    184\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    185\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    186\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    187\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    188\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    189\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
            "File \u001b[1;32mc:\\Users\\gabri\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\optim\\adam.py:318\u001b[0m, in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    315\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    316\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[1;32m--> 318\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    319\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    320\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    321\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    322\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    323\u001b[0m \u001b[43m     \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    324\u001b[0m \u001b[43m     \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    325\u001b[0m \u001b[43m     \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    326\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    327\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    328\u001b[0m \u001b[43m     \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    329\u001b[0m \u001b[43m     \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    330\u001b[0m \u001b[43m     \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    331\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    332\u001b[0m \u001b[43m     \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    333\u001b[0m \u001b[43m     \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    334\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    335\u001b[0m \u001b[43m     \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\gabri\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\optim\\adam.py:443\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[0;32m    440\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    441\u001b[0m         denom \u001b[38;5;241m=\u001b[39m (exp_avg_sq\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[1;32m--> 443\u001b[0m     \u001b[43mparam\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maddcdiv_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexp_avg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdenom\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mstep_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    445\u001b[0m \u001b[38;5;66;03m# Lastly, switch back to complex view\u001b[39;00m\n\u001b[0;32m    446\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m amsgrad \u001b[38;5;129;01mand\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mis_complex(params[i]):\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "input_size = X_train.shape[1]\n",
        "output_size = 1\n",
        "\n",
        "model = MLP()\n",
        "\n",
        "# class_weights = torch.tensor([1.0, 5.0])\n",
        "# criterion = nn.BCEWithLogitsLoss(pos_weight=class_weights[1])\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "_, _, W_adam = train_model(model, 100, train_loader, criterion, optimizer)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.71375\n",
            "0.16666666666666666\n"
          ]
        }
      ],
      "source": [
        "from sklearn.ensemble import IsolationForest\n",
        "\n",
        "modelisolationforest = IsolationForest(contamination=0.18, random_state=42)\n",
        "modelisolationforest.fit(X_train_scaled)\n",
        "\n",
        "y_train_pred_if = modelisolationforest.predict(X_train_scaled)\n",
        "y_test_pred_if = modelisolationforest.predict(X_test_scaled)\n",
        "\n",
        "y_train_pred_if = np.where(y_train_pred_if == -1,1,0)\n",
        "y_test_pred_if = np.where(y_test_pred_if == -1,1,0)\n",
        "\n",
        "train_accuracy_if = accuracy_score(y_train,y_train_pred_if)\n",
        "test_accuracy_if = accuracy_score(y_test,y_test_pred_if)\n",
        "\n",
        "print(train_accuracy_if)\n",
        "print(test_accuracy_if)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[[ 3.30937095e-02 -4.96353908e-03 -1.52720032e+01 -3.96207733e+01\n",
            "    3.12500000e-02]\n",
            "  [-8.89009908e-02  9.58854891e-03  2.03179493e+01  3.79275589e+01\n",
            "    9.89583358e-02]\n",
            "  [ 4.81614470e-02  7.81319104e-05  2.25106564e+01  3.38749657e+01\n",
            "    9.89583358e-02]\n",
            "  ...\n",
            "  [-1.15359358e-01  7.73958722e-03  1.24924879e+01 -9.38453436e-01\n",
            "    4.68750000e-02]\n",
            "  [-1.13427095e-01  6.99999882e-03  1.20373526e+01 -9.66182411e-01\n",
            "    4.68750000e-02]\n",
            "  [-1.06697895e-01  5.17187966e-03  1.17136459e+01 -9.78239834e-01\n",
            "    4.68750000e-02]]\n",
            "\n",
            " [[ 3.58750112e-02  1.41458316e-02  1.94359932e+01 -1.62199485e+00\n",
            "    9.37500000e-02]\n",
            "  [ 9.52760577e-02  7.95833115e-03  2.62669621e+01 -2.04067683e+00\n",
            "    1.04166664e-01]\n",
            "  [ 6.74749613e-01  6.36354238e-02  3.88198586e+01  1.39930165e+00\n",
            "    1.56250000e-02]\n",
            "  ...\n",
            "  [ 9.04536724e-01  6.17187321e-02  2.13479786e+01 -5.61791718e-01\n",
            "    7.29166642e-02]\n",
            "  [ 8.67734492e-01  4.40156199e-02  1.86060371e+01 -8.93166959e-01\n",
            "    7.29166642e-02]\n",
            "  [ 4.36104268e-01  7.89062027e-03  3.15551090e+01 -3.14036489e-01\n",
            "    2.08333340e-02]]\n",
            "\n",
            " [[ 8.33959654e-02 -2.44791638e-02  2.05586987e+01 -2.04470277e+00\n",
            "    4.16666679e-02]\n",
            "  [-2.37186793e-02 -4.73645777e-02  2.10075207e+01 -1.93177569e+00\n",
            "    4.16666679e-02]\n",
            "  [-6.93218708e-01  1.53125264e-03 -2.65564270e+01 -3.42814064e+00\n",
            "    6.77083358e-02]\n",
            "  ...\n",
            "  [ 6.36458583e-03  5.32291690e-03  1.60685978e+01 -2.66911447e-01\n",
            "    4.16666679e-02]\n",
            "  [ 7.68228807e-03  4.44270764e-03  1.53815355e+01 -2.81921893e-01\n",
            "    4.16666679e-02]\n",
            "  [ 1.00885397e-02  2.50520860e-03  1.47059107e+01 -2.90880263e-01\n",
            "    4.16666679e-02]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-6.05026007e-01  1.10937119e-03  1.22345314e+01 -4.96913433e+00\n",
            "    4.68750000e-02]\n",
            "  [-6.05015576e-01  1.13019475e-03  1.36594687e+01 -4.91499090e+00\n",
            "    4.68750000e-02]\n",
            "  [-6.05026007e-01  1.10937119e-03  1.56092148e+01 -4.83593082e+00\n",
            "    4.68750000e-02]\n",
            "  ...\n",
            "  [ 4.23140734e-01  7.58906081e-02  1.03829565e+01 -8.27654648e+00\n",
            "    7.81250000e-02]\n",
            "  [ 4.70000118e-01  7.41979033e-02  9.40138721e+00 -8.53924370e+00\n",
            "    7.29166642e-02]\n",
            "  [ 2.38734484e-01  7.58749917e-02  7.71146250e+00 -9.36296940e+00\n",
            "    5.72916679e-02]]\n",
            "\n",
            " [[-8.34828317e-01 -6.10416336e-03 -2.07496109e+01 -3.28262019e+00\n",
            "    2.60416660e-02]\n",
            "  [-8.26760530e-01 -8.07812251e-03 -2.02596302e+01 -3.33298469e+00\n",
            "    2.60416660e-02]\n",
            "  [-8.15041780e-01 -6.97916187e-03 -1.96718979e+01 -3.38631749e+00\n",
            "    2.60416660e-02]\n",
            "  ...\n",
            "  [ 7.57281542e-01  5.65885454e-02  7.01931763e+01 -4.20788002e+00\n",
            "    2.60416660e-02]\n",
            "  [-2.64979184e-01  1.95833878e-03  3.37105727e+00 -3.12529254e+00\n",
            "    4.68750000e-02]\n",
            "  [-3.74041706e-01  7.55729293e-03  4.09711266e+00 -3.19520283e+00\n",
            "    4.68750000e-02]]\n",
            "\n",
            " [[ 7.30999947e-01  3.84374964e-03 -1.30381250e+01 -8.39015782e-01\n",
            "    1.56250000e-02]\n",
            "  [ 1.74411476e-01  1.47552108e-02  1.03269796e+01  9.80948210e-01\n",
            "    2.60416660e-02]\n",
            "  [-6.17255211e-01  3.48750018e-02  3.51693840e+01 -4.83723927e+00\n",
            "    5.20833349e-03]\n",
            "  ...\n",
            "  [ 9.81406271e-02 -1.73437456e-03  3.53492355e+01 -4.34542799e+00\n",
            "    5.20833349e-03]\n",
            "  [-1.30802095e-01 -8.85416288e-04 -1.84085560e+01 -5.73703051e+00\n",
            "    1.56250000e-02]\n",
            "  [ 1.56093732e-01  4.74478956e-03  5.43406439e+00 -1.95510387e-01\n",
            "    1.04166670e-02]]]\n",
            "[[ 3.30937095e-02 -4.96353908e-03 -1.52720032e+01 -3.96207733e+01\n",
            "   3.12500000e-02]\n",
            " [-8.89009908e-02  9.58854891e-03  2.03179493e+01  3.79275589e+01\n",
            "   9.89583358e-02]\n",
            " [ 4.81614470e-02  7.81319104e-05  2.25106564e+01  3.38749657e+01\n",
            "   9.89583358e-02]\n",
            " [-8.15000013e-02  1.99114624e-02 -2.25800443e+00 -1.29760420e+00\n",
            "   4.16666679e-02]\n",
            " [-8.28749985e-02  1.87968779e-02 -2.15396786e+00 -1.41384900e+00\n",
            "   4.16666679e-02]\n",
            " [ 7.84385443e-01  2.81249988e-03 -2.91387806e+01 -4.63051796e-01\n",
            "   2.08333340e-02]\n",
            " [ 3.47505003e-01 -3.65416817e-02  1.86489239e+01 -3.33413529e+00\n",
            "   6.77083358e-02]\n",
            " [ 3.10005218e-01 -3.63281332e-02  1.71435337e+01 -3.32584286e+00\n",
            "   7.81250000e-02]\n",
            " [ 2.68744797e-01 -4.32239659e-02  1.59583368e+01 -3.30270195e+00\n",
            "   7.81250000e-02]\n",
            " [ 2.19791695e-01 -5.02916761e-02  1.48630610e+01 -3.25895214e+00\n",
            "   7.81250000e-02]\n",
            " [-2.03651011e-01  1.11666648e-02 -1.74029160e+00 -2.35178161e+00\n",
            "   6.77083358e-02]\n",
            " [-2.07968876e-01  4.57812753e-03 -2.08121848e+00 -2.35580778e+00\n",
            "   7.29166642e-02]\n",
            " [-1.76703095e-01  2.69791973e-03 -2.81775022e+00 -2.31608367e+00\n",
            "   8.33333358e-02]\n",
            " [-1.75677061e-01  4.75521153e-03 -3.13665080e+00 -2.30360436e+00\n",
            "   8.85416642e-02]\n",
            " [-1.71078265e-01  9.60417558e-03  1.17213144e+01 -2.75764012e+00\n",
            "   1.14583336e-01]\n",
            " [-1.70713708e-01  7.48438062e-03  1.15975914e+01 -2.75325513e+00\n",
            "   1.14583336e-01]\n",
            " [-1.60619751e-01  8.35416652e-03  1.13917656e+01 -2.74681759e+00\n",
            "   1.14583336e-01]\n",
            " [-1.62906215e-01  1.04166660e-02  1.11595421e+01 -2.74908829e+00\n",
            "   1.14583336e-01]\n",
            " [-3.44895236e-02  1.26874894e-02  1.10091865e+00 -4.24073029e+00\n",
            "   7.81250000e-02]\n",
            " [-1.90328076e-01  3.00468672e-02  1.80385932e-01 -5.06626177e+00\n",
            "   7.81250000e-02]\n",
            " [-2.50296801e-01 -2.77447868e-02 -1.30473146e+01 -4.92700148e+00\n",
            "   4.68750000e-02]\n",
            " [-1.98114559e-01 -3.18854265e-02 -1.20194101e+01 -4.88397932e+00\n",
            "   4.68750000e-02]\n",
            " [-1.88156173e-01 -3.26822959e-02 -1.15566521e+01 -4.85286999e+00\n",
            "   4.68750000e-02]\n",
            " [-1.67557195e-01 -3.29114608e-02 -1.11303606e+01 -4.81408358e+00\n",
            "   4.68750000e-02]\n",
            " [-5.75625040e-02  6.35625198e-02  5.92758942e+00 -2.75225472e+00\n",
            "   6.77083358e-02]\n",
            " [-3.29312533e-01 -2.25001201e-03  6.78788424e+00 -2.62061453e+00\n",
            "   1.04166664e-01]\n",
            " [-1.95802152e-01 -1.05520906e-02  9.37450504e+00 -2.56105137e+00\n",
            "   1.04166664e-01]\n",
            " [-1.15359358e-01  7.73958722e-03  1.24924879e+01 -9.38453436e-01\n",
            "   4.68750000e-02]\n",
            " [-1.13427095e-01  6.99999882e-03  1.20373526e+01 -9.66182411e-01\n",
            "   4.68750000e-02]\n",
            " [-1.06697895e-01  5.17187966e-03  1.17136459e+01 -9.78239834e-01\n",
            "   4.68750000e-02]]\n",
            "300\n"
          ]
        }
      ],
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "medie = np.mean(data, axis=(2))\n",
        "varianza = np.mean(data, axis=(2))\n",
        "medie_cheaters = np.mean(data_cheater, axis=(2))\n",
        "varianza_cheaters = np.mean(data_cheater, axis=(2))\n",
        "\n",
        "print(medie)\n",
        "print(medie[0])\n",
        "#concatenazione \n",
        "result=np.stack((medie,varianza), axis=1)\n",
        "result_cheater=np.stack((medie_cheaters,varianza_cheaters), axis=1)\n",
        "total = np.concatenate((result, result_cheater), axis=0)\n",
        "\n",
        "labels_legit = np.zeros(len(result))\n",
        "labels_cheater=np.ones(len(result_cheater))\n",
        "labels = np.concatenate((labels_legit, labels_cheater))\n",
        "\n",
        "#reshape dei dati per la SVM\n",
        "X = total.reshape(12000, -1)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.20, random_state=42, stratify=labels)\n",
        "\n",
        "#rescaling dei dati\n",
        "scaler = MinMaxScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.fit_transform(X_test)\n",
        "\n",
        "print(X_train_scaled.shape[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([9600, 300])\n",
            "torch.Size([9600])\n"
          ]
        }
      ],
      "source": [
        "X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
        "X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
        "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "train_data = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
        "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "print(X_train_tensor.shape)\n",
        "print(y_train_tensor.shape)\n",
        "def get_accuracy(logit, target):\n",
        "    corrects = (torch.max(logit, 1)[1].view(target.size()).data == target.data).sum()\n",
        "    accuracy = 100.0 * corrects / target.size(0)\n",
        "    return accuracy.item()\n",
        "\n",
        "def compute_weight_norm(model):\n",
        "    norm = 0.0\n",
        "    for name, param in model.named_parameters():\n",
        "      if \"weight\" in name:\n",
        "        norm += torch.norm(param.data, p= 2)\n",
        "    return norm.cpu().item()\n",
        "\n",
        "def train_model(model, num_epochs, trainloader, criterion, optimizer):\n",
        "\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Rich mac user\n",
        "    # device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
        "\n",
        "    model = model.to(device)\n",
        "\n",
        "    losses = []\n",
        "    accs = []\n",
        "\n",
        "    norms = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        train_running_loss = 0.0\n",
        "        train_acc = 0.0\n",
        "\n",
        "        # Set the model to training mode\n",
        "        model.train()\n",
        "        ## training step\n",
        "        for i, (inputs, labels) in enumerate(trainloader):\n",
        "\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            ## forward + backprop + loss\n",
        "            logits = model(inputs)\n",
        "            loss = criterion(logits, labels)\n",
        "\n",
        "            # Reset the gradients to zero\n",
        "\n",
        "            loss.backward()\n",
        "\n",
        "            ## update model params\n",
        "            optimizer.step()\n",
        "\n",
        "            train_running_loss += loss.item()\n",
        "            train_acc += get_accuracy(logits, labels)\n",
        "\n",
        "\n",
        "        losses.append(train_running_loss / i)\n",
        "        accs.append(train_acc/i)\n",
        "\n",
        "        norms.append(compute_weight_norm(model))\n",
        "        model.eval()\n",
        "        print(f\"Epoch: {epoch+1} | Loss: {train_running_loss / i:.4f} | Train Accuracy: {train_acc/i:.4f}\")\n",
        "\n",
        "    return losses, accs, norms\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MLP, self).__init__()\n",
        "        self.fc1 = nn.Linear(300, 1024)\n",
        "\n",
        "        self.fc2 = nn.Linear(1024, 500)\n",
        "\n",
        "        self.fc3 = nn.Linear(500, 256)\n",
        "\n",
        "        self.fc4 = nn.Linear(256, 128)\n",
        "\n",
        "        self.fc5 = nn.Linear(128, 64)\n",
        "\n",
        "        self.fc6 = nn.Linear(64, 32)\n",
        "\n",
        "        self.fc7 = nn.Linear(32, 8)\n",
        "\n",
        "        self.fc8 = nn.Linear(8, 2)\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = torch.relu(self.fc1(x))\n",
        "\n",
        "        x = torch.relu(self.fc2(x))\n",
        "\n",
        "        x = torch.relu(self.fc3(x))\n",
        "\n",
        "        x = torch.relu(self.fc4(x))\n",
        "\n",
        "        x = torch.relu(self.fc5(x))\n",
        "\n",
        "        x = torch.relu(self.fc6(x))\n",
        "\n",
        "        x = torch.relu(self.fc7(x))\n",
        "\n",
        "        x = torch.sigmoid(self.fc8(x))\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "300\n",
            "Epoch: 1 | Loss: 0.4820 | Train Accuracy: 83.6120\n",
            "Epoch: 2 | Loss: 0.4815 | Train Accuracy: 83.6120\n",
            "Epoch: 3 | Loss: 0.4815 | Train Accuracy: 83.6120\n",
            "Epoch: 4 | Loss: 0.4815 | Train Accuracy: 83.6120\n",
            "Epoch: 5 | Loss: 0.4815 | Train Accuracy: 83.6120\n",
            "Epoch: 6 | Loss: 0.4815 | Train Accuracy: 83.6120\n",
            "Epoch: 7 | Loss: 0.4815 | Train Accuracy: 83.6120\n",
            "Epoch: 8 | Loss: 0.4815 | Train Accuracy: 83.6120\n",
            "Epoch: 9 | Loss: 0.4815 | Train Accuracy: 83.6120\n",
            "Epoch: 10 | Loss: 0.4815 | Train Accuracy: 83.6120\n",
            "Epoch: 11 | Loss: 0.4815 | Train Accuracy: 83.6120\n",
            "Epoch: 12 | Loss: 0.4815 | Train Accuracy: 83.6120\n",
            "Epoch: 13 | Loss: 0.4815 | Train Accuracy: 83.6120\n",
            "Epoch: 14 | Loss: 0.4815 | Train Accuracy: 83.6120\n",
            "Epoch: 15 | Loss: 0.4815 | Train Accuracy: 83.6120\n",
            "Epoch: 16 | Loss: 0.4815 | Train Accuracy: 83.6120\n",
            "Epoch: 17 | Loss: 0.4815 | Train Accuracy: 83.6120\n",
            "Epoch: 18 | Loss: 0.4815 | Train Accuracy: 83.6120\n",
            "Epoch: 19 | Loss: 0.4815 | Train Accuracy: 83.6120\n",
            "Epoch: 20 | Loss: 0.4815 | Train Accuracy: 83.6120\n",
            "Epoch: 21 | Loss: 0.4815 | Train Accuracy: 83.6120\n",
            "Epoch: 22 | Loss: 0.4815 | Train Accuracy: 83.6120\n",
            "Epoch: 23 | Loss: 0.4815 | Train Accuracy: 83.6120\n",
            "Epoch: 24 | Loss: 0.4815 | Train Accuracy: 83.6120\n",
            "Epoch: 25 | Loss: 0.4815 | Train Accuracy: 83.6120\n",
            "Epoch: 26 | Loss: 0.4815 | Train Accuracy: 83.6120\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[74], line 15\u001b[0m\n\u001b[0;32m     10\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[0;32m     11\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 15\u001b[0m _, _, W_adam \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[1;32mIn[70], line 61\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, num_epochs, trainloader, criterion, optimizer)\u001b[0m\n\u001b[0;32m     58\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     60\u001b[0m \u001b[38;5;66;03m## update model params\u001b[39;00m\n\u001b[1;32m---> 61\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     63\u001b[0m train_running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     64\u001b[0m train_acc \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m get_accuracy(logits, labels)\n",
            "File \u001b[1;32mc:\\Users\\gabri\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\optim\\optimizer.py:391\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    386\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    387\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    388\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    389\u001b[0m             )\n\u001b[1;32m--> 391\u001b[0m out \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    392\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    394\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\gabri\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\optim\\optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[1;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
            "File \u001b[1;32mc:\\Users\\gabri\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\optim\\adam.py:168\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    157\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m    159\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[0;32m    160\u001b[0m         group,\n\u001b[0;32m    161\u001b[0m         params_with_grad,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    165\u001b[0m         max_exp_avg_sqs,\n\u001b[0;32m    166\u001b[0m         state_steps)\n\u001b[1;32m--> 168\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    169\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    170\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    171\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    172\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    175\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    176\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    177\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    178\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    179\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    180\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    181\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    182\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    183\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    184\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    185\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    186\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    187\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    188\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    189\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
            "File \u001b[1;32mc:\\Users\\gabri\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\optim\\adam.py:318\u001b[0m, in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    315\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    316\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[1;32m--> 318\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    319\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    320\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    321\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    322\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    323\u001b[0m \u001b[43m     \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    324\u001b[0m \u001b[43m     \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    325\u001b[0m \u001b[43m     \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    326\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    327\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    328\u001b[0m \u001b[43m     \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    329\u001b[0m \u001b[43m     \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    330\u001b[0m \u001b[43m     \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    331\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    332\u001b[0m \u001b[43m     \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    333\u001b[0m \u001b[43m     \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    334\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    335\u001b[0m \u001b[43m     \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\gabri\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\optim\\adam.py:393\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[0;32m    390\u001b[0m     param \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mview_as_real(param)\n\u001b[0;32m    392\u001b[0m \u001b[38;5;66;03m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[1;32m--> 393\u001b[0m \u001b[43mexp_avg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlerp_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    394\u001b[0m exp_avg_sq\u001b[38;5;241m.\u001b[39mmul_(beta2)\u001b[38;5;241m.\u001b[39maddcmul_(grad, grad\u001b[38;5;241m.\u001b[39mconj(), value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta2)\n\u001b[0;32m    396\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m capturable \u001b[38;5;129;01mor\u001b[39;00m differentiable:\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "input_size = X_train.shape[1]\n",
        "output_size = 1\n",
        "print(input_size)\n",
        "\n",
        "model = MLP()\n",
        "\n",
        "class_weights = torch.tensor([1.0, 5.0])\n",
        "#criterion = nn.BCELoss()\n",
        "#criterion = nn.BCEWithLogitsLoss(pos_weight=class_weights[1])\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1)\n",
        "\n",
        "\n",
        "\n",
        "_, _, W_adam = train_model(model, 100, train_loader, criterion, optimizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MS3gME1w-3xv",
        "outputId": "ca0088e3-ace5-49d0-e743-07ec886fad37"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "30\n",
            "(9600, 30, 192, 5)\n",
            "torch.Size([9600, 30, 192, 5])\n",
            "torch.Size([9600, 1])\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "total = np.concatenate((data, data_cheater), axis=0)\n",
        "\n",
        "labels_legit = np.zeros(len(data))\n",
        "labels_cheater=np.ones(len(data_cheater))\n",
        "labels = np.concatenate((labels_legit, labels_cheater))\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(total, labels, test_size=0.20, random_state=42, stratify=labels)\n",
        "\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train.reshape(-1, X_train.shape[-1])).reshape(X_train.shape)\n",
        "X_test_scaled = scaler.fit_transform(X_test.reshape(-1, X_test.shape[-1])).reshape(X_test.shape)\n",
        "\n",
        "print(X_train_scaled.shape[1])\n",
        "print(X_train_scaled.shape)\n",
        "\n",
        "X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
        "X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)\n",
        "y_test_tensor = torch.tensor(y_test, dtype=torch.float32).unsqueeze(1)\n",
        "\n",
        "train_data = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
        "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "print(X_train_tensor.shape)\n",
        "print(y_train_tensor.shape)\n",
        "\n",
        "class MLP(nn.Module):\n",
        "  def __init__(self):\n",
        "        super(MLP, self).__init__()\n",
        "        self.conv1 = nn.Conv3d(in_channels=1, out_channels=64, kernel_size=(3,3,3), padding=1)\n",
        "        self.conv2 = nn.Conv3d(in_channels=64, out_channels=128, kernel_size=(3,3,3), padding=1)\n",
        "        self.pool = nn.MaxPool3d(kernel_size=(2,2,2), stride=2)\n",
        "        self.dropout = nn.Dropout(p=0.5)\n",
        "        self.fc1 = nn.Linear(43008, 256)\n",
        "        self.fc2 = nn.Linear(256, 1)\n",
        "        self.relu = nn.ReLU()\n",
        "  def forward(self, x):\n",
        "        x = self.relu(self.conv1(x))\n",
        "        x = self.pool(x)\n",
        "        x = self.relu(self.conv2(x))\n",
        "        x = self.pool(x)\n",
        "        x = x.view(x.size(0), -1)  # Flatten the tensor\n",
        "        x = self.dropout(self.relu(self.fc1(x)))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "model = MLP()\n",
        "\n",
        "class_weights = torch.tensor([1.0, 5.0])\n",
        "criterion = nn.BCEWithLogitsLoss(pos_weight=class_weights[1])\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "lHSNz0-DD7n_",
        "outputId": "b84b6105-5893-4dbf-b541-ca56e269be47"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-068122f4f551>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    523\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m             )\n\u001b[0;32m--> 525\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    526\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 267\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    268\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    743\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 744\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    745\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    746\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "num_epochs = 100\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs = inputs.unsqueeze(1)\n",
        "        labels = labels.squeeze(1)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(inputs).squeeze(1)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    if (epoch+1) % 10 == 0:\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# Valutare il modello\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "      correct = 0\n",
        "      total = 0\n",
        "      all_labels = []\n",
        "      all_predicted = []\n",
        "      for inputs, labels in test_loader:\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        predicted = torch.sigmoid(outputs) > 0.5\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "        all_labels.extend(labels.numpy())\n",
        "        all_predicted.extend(predicted.numpy())\n",
        "\n",
        "    accuracy = correct / total\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Accuracy: {accuracy:.4f}')\n",
        "    print(classification_report(all_labels, all_predicted, target_names=['legit', 'cheater']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GYSOcsdJrVmG",
        "outputId": "7e58825d-0f55-4e72-c1e9-eeb0ab4875e0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "28800\n",
            "torch.Size([9600, 1, 28800])\n",
            "torch.Size([9600])\n",
            "Epoch: 1 | Loss: 0.4823 | Train Accuracy: 83.4971\n",
            "Epoch: 2 | Loss: 0.4815 | Train Accuracy: 83.6120\n",
            "Epoch: 3 | Loss: 0.4815 | Train Accuracy: 83.6120\n",
            "Epoch: 4 | Loss: 0.4815 | Train Accuracy: 83.6120\n",
            "Epoch: 5 | Loss: 0.4815 | Train Accuracy: 83.6120\n",
            "Epoch: 6 | Loss: 0.4815 | Train Accuracy: 83.6120\n",
            "Epoch: 7 | Loss: 0.4815 | Train Accuracy: 83.6120\n",
            "Epoch: 8 | Loss: 0.4815 | Train Accuracy: 83.6120\n",
            "Epoch: 9 | Loss: 0.4815 | Train Accuracy: 83.6120\n",
            "Epoch: 10 | Loss: 0.4815 | Train Accuracy: 83.6120\n",
            "Epoch: 11 | Loss: 0.4815 | Train Accuracy: 83.6120\n",
            "Epoch: 12 | Loss: 0.4815 | Train Accuracy: 83.6120\n",
            "Epoch: 13 | Loss: 0.4815 | Train Accuracy: 83.6120\n",
            "Epoch: 14 | Loss: 0.4815 | Train Accuracy: 83.6120\n",
            "Epoch: 15 | Loss: 0.4815 | Train Accuracy: 83.6120\n",
            "Epoch: 16 | Loss: 0.4815 | Train Accuracy: 83.6120\n",
            "Epoch: 17 | Loss: 0.4815 | Train Accuracy: 83.6120\n",
            "Epoch: 18 | Loss: 0.4815 | Train Accuracy: 83.6120\n",
            "Epoch: 19 | Loss: 0.4815 | Train Accuracy: 83.6120\n",
            "Epoch: 20 | Loss: 0.4815 | Train Accuracy: 83.6120\n",
            "Epoch: 21 | Loss: 0.4815 | Train Accuracy: 83.6120\n",
            "Epoch: 22 | Loss: 0.4815 | Train Accuracy: 83.6120\n",
            "Epoch: 23 | Loss: 0.4815 | Train Accuracy: 83.6120\n",
            "Epoch: 24 | Loss: 0.4815 | Train Accuracy: 83.6120\n",
            "Epoch: 25 | Loss: 0.4815 | Train Accuracy: 83.6120\n",
            "Epoch: 26 | Loss: 0.4815 | Train Accuracy: 83.6120\n",
            "Epoch: 27 | Loss: 0.4815 | Train Accuracy: 83.6120\n",
            "Epoch: 28 | Loss: 0.4815 | Train Accuracy: 83.6120\n",
            "Epoch: 29 | Loss: 0.4815 | Train Accuracy: 83.6120\n",
            "Epoch: 30 | Loss: 0.4815 | Train Accuracy: 83.6120\n",
            "Epoch: 31 | Loss: 0.4815 | Train Accuracy: 83.6120\n",
            "Epoch: 32 | Loss: 0.4815 | Train Accuracy: 83.6120\n",
            "Epoch: 33 | Loss: 0.4815 | Train Accuracy: 83.6120\n",
            "Epoch: 34 | Loss: 0.4815 | Train Accuracy: 83.6120\n",
            "Epoch: 35 | Loss: 0.4815 | Train Accuracy: 83.6120\n",
            "Epoch: 36 | Loss: 0.4815 | Train Accuracy: 83.6120\n",
            "Epoch: 37 | Loss: 0.4815 | Train Accuracy: 83.6120\n",
            "Epoch: 38 | Loss: 0.4815 | Train Accuracy: 83.6120\n",
            "Epoch: 39 | Loss: 0.4815 | Train Accuracy: 83.6120\n",
            "Epoch: 40 | Loss: 0.4815 | Train Accuracy: 83.6120\n",
            "Epoch: 41 | Loss: 0.4815 | Train Accuracy: 83.6120\n",
            "Epoch: 42 | Loss: 0.4815 | Train Accuracy: 83.6120\n",
            "Epoch: 43 | Loss: 0.4815 | Train Accuracy: 83.6120\n",
            "Epoch: 44 | Loss: 0.4815 | Train Accuracy: 83.6120\n",
            "Epoch: 45 | Loss: 0.4815 | Train Accuracy: 83.6120\n",
            "Epoch: 46 | Loss: 0.4815 | Train Accuracy: 83.6120\n",
            "Epoch: 47 | Loss: 0.4815 | Train Accuracy: 83.6120\n",
            "Epoch: 48 | Loss: 0.4815 | Train Accuracy: 83.6120\n",
            "Epoch: 49 | Loss: 0.4815 | Train Accuracy: 83.6120\n",
            "Epoch: 50 | Loss: 0.4815 | Train Accuracy: 83.6120\n",
            "Epoch: 51 | Loss: 0.4815 | Train Accuracy: 83.6120\n",
            "Epoch: 52 | Loss: 0.4815 | Train Accuracy: 83.6120\n",
            "Epoch: 53 | Loss: 0.4815 | Train Accuracy: 83.6120\n",
            "Epoch: 54 | Loss: 0.4815 | Train Accuracy: 83.6120\n",
            "Epoch: 55 | Loss: 0.4815 | Train Accuracy: 83.6120\n",
            "Epoch: 56 | Loss: 0.4815 | Train Accuracy: 83.6120\n",
            "Epoch: 57 | Loss: 0.4815 | Train Accuracy: 83.6120\n",
            "Epoch: 58 | Loss: 0.4815 | Train Accuracy: 83.6120\n",
            "Epoch: 59 | Loss: 0.4815 | Train Accuracy: 83.6120\n",
            "Epoch: 60 | Loss: 0.4815 | Train Accuracy: 83.6120\n",
            "Epoch: 61 | Loss: 0.4815 | Train Accuracy: 83.6120\n",
            "Epoch: 62 | Loss: 0.4815 | Train Accuracy: 83.6120\n",
            "Epoch: 63 | Loss: 0.4815 | Train Accuracy: 83.6120\n",
            "Epoch: 64 | Loss: 0.4815 | Train Accuracy: 83.6120\n",
            "Epoch: 65 | Loss: 0.4815 | Train Accuracy: 83.6120\n",
            "Epoch: 66 | Loss: 0.4815 | Train Accuracy: 83.6120\n",
            "Epoch: 67 | Loss: 0.4815 | Train Accuracy: 83.6120\n",
            "Epoch: 68 | Loss: 0.4815 | Train Accuracy: 83.6120\n",
            "Epoch: 69 | Loss: 0.4815 | Train Accuracy: 83.6120\n",
            "Epoch: 70 | Loss: 0.4815 | Train Accuracy: 83.6120\n",
            "Epoch: 71 | Loss: 0.4815 | Train Accuracy: 83.6120\n",
            "Epoch: 72 | Loss: 0.4815 | Train Accuracy: 83.6120\n",
            "Epoch: 73 | Loss: 0.4815 | Train Accuracy: 83.6120\n",
            "Epoch: 74 | Loss: 0.4815 | Train Accuracy: 83.6120\n",
            "Epoch: 75 | Loss: 0.4815 | Train Accuracy: 83.6120\n",
            "Epoch: 76 | Loss: 0.4815 | Train Accuracy: 83.6120\n",
            "Epoch: 77 | Loss: 0.4815 | Train Accuracy: 83.6120\n",
            "Epoch: 78 | Loss: 0.4815 | Train Accuracy: 83.6120\n",
            "Epoch: 79 | Loss: 0.4815 | Train Accuracy: 83.6120\n",
            "Epoch: 80 | Loss: 0.4815 | Train Accuracy: 83.6120\n",
            "Epoch: 81 | Loss: 0.4815 | Train Accuracy: 83.6120\n",
            "Epoch: 82 | Loss: 0.4815 | Train Accuracy: 83.6120\n",
            "Epoch: 83 | Loss: 0.4815 | Train Accuracy: 83.6120\n",
            "Epoch: 84 | Loss: 0.4815 | Train Accuracy: 83.6120\n",
            "Epoch: 85 | Loss: 0.4815 | Train Accuracy: 83.6120\n",
            "Epoch: 86 | Loss: 0.4815 | Train Accuracy: 83.6120\n",
            "Epoch: 87 | Loss: 0.4815 | Train Accuracy: 83.6120\n",
            "Epoch: 88 | Loss: 0.4815 | Train Accuracy: 83.6120\n",
            "Epoch: 89 | Loss: 0.4815 | Train Accuracy: 83.6120\n",
            "Epoch: 90 | Loss: 0.4815 | Train Accuracy: 83.6120\n",
            "Epoch: 91 | Loss: 0.4815 | Train Accuracy: 83.6120\n",
            "Epoch: 92 | Loss: 0.4815 | Train Accuracy: 83.6120\n",
            "Epoch: 93 | Loss: 0.4815 | Train Accuracy: 83.6120\n",
            "Epoch: 94 | Loss: 0.4815 | Train Accuracy: 83.6120\n",
            "Epoch: 95 | Loss: 0.4815 | Train Accuracy: 83.6120\n",
            "Epoch: 96 | Loss: 0.4815 | Train Accuracy: 83.6120\n",
            "Epoch: 97 | Loss: 0.4815 | Train Accuracy: 83.6120\n",
            "Epoch: 98 | Loss: 0.4815 | Train Accuracy: 83.6120\n"
          ]
        }
      ],
      "source": [
        "#tesi\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random as rnd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "\n",
        "\n",
        "#caricamento dati\n",
        "data = np.load(\"/content/drive/My Drive/Colab Notebooks/legit.npy\")\n",
        "data_cheater = np.load(\"/content/drive/My Drive/Colab Notebooks/cheaters.npy\")\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "#calcolo medie e varianza per legit e cheater\n",
        "medie = np.mean(data, axis=(1,2))\n",
        "varianza = np.mean(data, axis=(1,2))\n",
        "medie_cheaters = np.mean(data_cheater, axis=(1,2))\n",
        "varianza_cheaters = np.mean(data_cheater, axis=(1,2))\n",
        "\n",
        "#concatenazione\n",
        "result=np.stack((medie,varianza), axis=1)\n",
        "result_cheater=np.stack((medie_cheaters,varianza_cheaters), axis=1)\n",
        "total = np.concatenate((data, data_cheater), axis=0)\n",
        "\n",
        "labels_legit = np.zeros(len(result))\n",
        "labels_cheater=np.ones(len(result_cheater))\n",
        "labels = np.concatenate((labels_legit, labels_cheater))\n",
        "\n",
        "#reshape dei dati per la SVM\n",
        "X = total.reshape(12000, -1)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.20, random_state=42, stratify=labels)\n",
        "\n",
        "#rescaling dei dati\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.fit_transform(X_test)\n",
        "\n",
        "print(X_train_scaled.shape[1])\n",
        "#modello svm linear\n",
        "# svm = SVC(kernel = 'linear')\n",
        "# svm.fit(X_train_scaled, y_train)\n",
        "# #predicition\n",
        "# y_pred=svm.predict(X_test_scaled)\n",
        "\n",
        "#accuracy\n",
        "# acc = accuracy_score(y_test, y_pred)\n",
        "# print(acc)\n",
        "# cm_lin = confusion_matrix(y_test, y_pred)\n",
        "# disp_lin = ConfusionMatrixDisplay(confusion_matrix=cm_lin, display_labels=[\"Non cheater\", \"cheater\"])\n",
        "# disp_lin.plot()\n",
        "# plt.show()\n",
        "\n",
        "#modello svm gaussian\n",
        "# svm_rbf = SVC(kernel = 'rbf', gamma=20)\n",
        "# svm_rbf.fit(X_train_scaled, y_train)\n",
        "# #predicition\n",
        "# y_pred_rbf=svm_rbf.predict(X_test_scaled)\n",
        "\n",
        "#accuracy\n",
        "# acc_rbf = accuracy_score(y_test, y_pred_rbf)\n",
        "# print(acc_rbf)\n",
        "# cm_rbf = confusion_matrix(y_test, y_pred_rbf)\n",
        "# disp = ConfusionMatrixDisplay(confusion_matrix=cm_rbf, display_labels=[\"Non cheater\", \"cheater\"])\n",
        "# disp.plot()\n",
        "# plt.show()\n",
        "\n",
        "\n",
        "X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32).view(-1,1,X_test_scaled.shape[1])\n",
        "X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32).view(-1,1,X_test_scaled.shape[1])\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
        "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "train_data = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
        "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "print(X_train_tensor.shape)\n",
        "print(y_train_tensor.shape)\n",
        "def get_accuracy(logit, target):\n",
        "    corrects = (torch.max(logit, 1)[1].view(target.size()).data == target.data).sum()\n",
        "    accuracy = 100.0 * corrects / target.size(0)\n",
        "    return accuracy.item()\n",
        "\n",
        "def compute_weight_norm(model):\n",
        "    norm = 0.0\n",
        "    for name, param in model.named_parameters():\n",
        "      if \"weight\" in name:\n",
        "        norm += torch.norm(param.data, p= 2)\n",
        "    return norm.cpu().item()\n",
        "\n",
        "def train_model(model, num_epochs, trainloader, criterion, optimizer):\n",
        "\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Rich mac user\n",
        "    # device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
        "\n",
        "    model = model.to(device)\n",
        "\n",
        "    losses = []\n",
        "    accs = []\n",
        "\n",
        "    norms = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        train_running_loss = 0.0\n",
        "        train_acc = 0.0\n",
        "\n",
        "        # Set the model to training mode\n",
        "        model.train()\n",
        "        ## training step\n",
        "        for i, (inputs, labels) in enumerate(trainloader):\n",
        "\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            ## forward + backprop + loss\n",
        "            logits = model(inputs)\n",
        "            loss = criterion(logits, labels)\n",
        "\n",
        "            # Reset the gradients to zero\n",
        "\n",
        "            loss.backward()\n",
        "\n",
        "            ## update model params\n",
        "            optimizer.step()\n",
        "\n",
        "            train_running_loss += loss.item()\n",
        "            train_acc += get_accuracy(logits, labels)\n",
        "\n",
        "\n",
        "        losses.append(train_running_loss / i)\n",
        "        accs.append(train_acc/i)\n",
        "\n",
        "        norms.append(compute_weight_norm(model))\n",
        "        model.eval()\n",
        "        print(f\"Epoch: {epoch+1} | Loss: {train_running_loss / i:.4f} | Train Accuracy: {train_acc/i:.4f}\")\n",
        "\n",
        "    return losses, accs, norms\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MLP, self).__init__()\n",
        "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=6, kernel_size=10)\n",
        "        self.dropout1 = nn.Dropout(p=0.75)\n",
        "\n",
        "        self.conv2 = nn.Conv1d(in_channels=6, out_channels=6, kernel_size=10)\n",
        "        self.dropout2 = nn.Dropout(p=0.75)\n",
        "\n",
        "\n",
        "        self.conv3 = nn.Conv1d(in_channels=6, out_channels=6, kernel_size=10)\n",
        "        self.dropout3 = nn.Dropout(p=0.75)\n",
        "\n",
        "        self.flattern = nn.Flatten()\n",
        "\n",
        "        self.dropout4 = nn.Dropout(p=0.75)\n",
        "\n",
        "\n",
        "        self.fc1 = nn.Linear(172638, 2)\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = self.dropout1(x)\n",
        "\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = self.dropout2(x)\n",
        "\n",
        "        x = F.relu(self.conv3(x))\n",
        "        x = self.dropout3(x)\n",
        "\n",
        "        x = x.flatten(start_dim = 1)\n",
        "\n",
        "        #x = self.dropout4\n",
        "\n",
        "        x = torch.sigmoid(self.fc1(x))\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "input_size = X_train.shape[1]\n",
        "output_size = 1\n",
        "\n",
        "model = MLP()\n",
        "\n",
        "# class_weights = torch.tensor([1.0, 5.0])\n",
        "# criterion = nn.BCEWithLogitsLoss(pos_weight=class_weights[1])\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
        "\n",
        "_, _, W_adam = train_model(model, 100, train_loader, criterion, optimizer)\n",
        "\n",
        "\n",
        "# Addestrare il modello\n",
        "# num_epochs = 100\n",
        "# for epoch in range(num_epochs):\n",
        "#     model.train()\n",
        "#     for inputs, labels in train_loader:\n",
        "\n",
        "#         optimizer.zero_grad()\n",
        "\n",
        "#         outputs = model(inputs)\n",
        "#         loss = criterion(outputs, labels)\n",
        "\n",
        "\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "\n",
        "#     if (epoch+1) % 10 == 0:\n",
        "#         print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# # Valutare il modello\n",
        "#     model.eval()\n",
        "\n",
        "#     with torch.no_grad():\n",
        "#       correct = 0\n",
        "#       total = 0\n",
        "#       all_labels = []\n",
        "#       all_predicted = []\n",
        "#       for inputs, labels in test_loader:\n",
        "\n",
        "#         outputs = model(inputs)\n",
        "#         predicted = torch.sigmoid(outputs) > 0.5\n",
        "#         total += labels.size(0)\n",
        "#         correct += (predicted == labels).sum().item()\n",
        "#         all_labels.extend(labels.numpy())\n",
        "#         all_predicted.extend(predicted.numpy())\n",
        "\n",
        "#     accuracy = correct / total\n",
        "#     print(f'Epoch [{epoch+1}/{num_epochs}], Accuracy: {accuracy:.4f}')\n",
        "#     print(classification_report(all_labels, all_predicted, target_names=['legit', 'cheater']))\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
