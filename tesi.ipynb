{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Import and Data loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6SDvJoPLupUl"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random as rnd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import classification_report\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "\n",
        "\n",
        "#data loading\n",
        "data = np.load(\"C:/Users/gabri/Desktop/uni/tesi/csgo_archive/legit/legit.npy\")\n",
        "data_cheater = np.load(\"C:/Users/gabri/Desktop/uni/tesi/csgo_archive/cheaters/cheaters.npy\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## setting the device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2D4jAqBrxlmp",
        "outputId": "7a177793-b5c5-49ec-ed78-175deaa330bc"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data preprocessing \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "## Concatenation, adding labels, splitting in training and test, standarizing the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "total = np.concatenate((data, data_cheater), axis=0)\n",
        "labels_legit = np.zeros(len(data))\n",
        "labels_cheater=np.ones(len(data_cheater))\n",
        "labels = np.concatenate((labels_legit, labels_cheater))\n",
        "\n",
        "#reshape dei dati per la SVM\n",
        "X_train, X_test, y_train, y_test = train_test_split(total, labels, test_size=0.20, random_state=42, stratify=labels)\n",
        "\n",
        "#rescaling dei dati\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = X_train\n",
        "X_test_scaled = X_test\n",
        "\n",
        "X_train_small = X_train_scaled[:1000, :10, 96:192]\n",
        "y_train_small = y_train[:1000]\n",
        "X_test_small = X_test_scaled[:200, :10, 96:192]\n",
        "y_test_small = y_test[:200]\n",
        "\n",
        "print(X_train_small.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## transformation of data into tensor format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32).permute(0, 3, 1, 2)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).to(device)\n",
        "X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32).permute(0, 3, 1, 2)\n",
        "y_test_tensor = torch.tensor(y_test, dtype=torch.float32).to(device)\n",
        "\n",
        "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# CNN classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "class CNNClassifier(nn.Module):\n",
        "    #initialization of the convolutional network defining the different layers\n",
        "    def __init__(self):\n",
        "        super(CNNClassifier, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(30, 64, kernel_size=(3, 3), stride=1, padding=1)\n",
        "        self.conv2 = nn.Conv2d(64, 128, kernel_size=(3, 3), stride=1, padding=1)\n",
        "        self.fc1 = nn.Linear(6144, 256)\n",
        "        self.fc2 = nn.Linear(256, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "    \n",
        "    #activation function between layers\n",
        "    def forward(self, x):\n",
        "        x = nn.ReLU()(self.conv1(x))\n",
        "        x = nn.MaxPool2d(kernel_size=(2, 2))(x)\n",
        "        x = nn.ReLU()(self.conv2(x))\n",
        "        x = nn.MaxPool2d(kernel_size=(2, 2))(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = nn.ReLU()(self.fc1(x))\n",
        "        x = self.sigmoid(self.fc2(x))\n",
        "        return x\n",
        "\n",
        "model = CNNClassifier().to(device)\n",
        "\n",
        "# Loss function and optimization\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "# Training cycle\n",
        "num_epochs = 100\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for i, (inputs, labels) in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        inputs = inputs.permute(0, 2, 3, 1).contiguous()\n",
        "        outputs = model(inputs)\n",
        "        labels = labels.view(-1, 1)\n",
        "        \n",
        "        loss = criterion(outputs, labels)\n",
        "        \n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        running_loss += loss.item()\n",
        "        \n",
        "        if (i + 1) % 10 == 0:\n",
        "            print(f'Epoch [{epoch + 1}/{num_epochs}], Step [{i + 1}/{len(train_loader)}], Loss: {running_loss / (i + 1):.4f}')\n",
        "    \n",
        "    print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {running_loss / len(train_loader):.4f}')\n",
        "\n",
        "# Model evaluation\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs = inputs.permute(0, 2, 3, 1).contiguous()\n",
        "            outputs = model(inputs)\n",
        "            predicted = (outputs > 0.5).float()\n",
        "            labels = labels.view(-1, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "        print(f'Test Accuracy: {100 * correct / total:.2f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LSTM model classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train_tensor = torch.tensor(X_train_small.reshape((X_train_small.shape[0], -1, 5)), dtype=torch.float32).to(device)\n",
        "y_train_tensor = torch.tensor(y_train_small, dtype=torch.float32).to(device)\n",
        "X_test_tensor = torch.tensor(X_test_small.reshape((X_test_small.shape[0], -1, 5)), dtype=torch.float32).to(device)\n",
        "y_test_tensor = torch.tensor(y_test_small, dtype=torch.float32).to(device)\n",
        "\n",
        "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "# Creation of LSTM \n",
        "class LSTMClassifier(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim):\n",
        "        super(LSTMClassifier, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.layer_dim = layer_dim\n",
        "        self.lstm = nn.LSTM(input_dim, hidden_dim, layer_dim, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "    \n",
        "    def forward(self, x):\n",
        "        h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).to(x.device)\n",
        "        c0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).to(x.device)\n",
        "        \n",
        "        out, _ = self.lstm(x, (h0, c0))\n",
        "        \n",
        "        out = self.fc(out[:, -1, :])\n",
        "        out = self.sigmoid(out)\n",
        "        return out\n",
        "\n",
        "input_dim = 5\n",
        "hidden_dim = 128\n",
        "layer_dim = 2\n",
        "output_dim = 1\n",
        "\n",
        "model = LSTMClassifier(input_dim, hidden_dim, layer_dim, output_dim).to(device)\n",
        "\n",
        "# Loss function and optimization\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training cycle\n",
        "num_epochs = 20\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for i, (inputs, labels) in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        outputs = model(inputs)\n",
        "        labels = labels.view(-1, 1)\n",
        "        \n",
        "        loss = criterion(outputs, labels)\n",
        "        \n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        running_loss += loss.item()\n",
        "        \n",
        "        if (i + 1) % 10 == 0:\n",
        "            print(f'Epoch [{epoch + 1}/{num_epochs}], Step [{i + 1}/{len(train_loader)}], Loss: {running_loss / (i + 1):.4f}')\n",
        "    \n",
        "    print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {running_loss / len(train_loader):.4f}')\n",
        "\n",
        "# Model evaluation\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for inputs, labels in test_loader:\n",
        "            outputs = model(inputs)\n",
        "            predicted = (outputs > 0.5).float()\n",
        "            labels = labels.view(-1, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "        print(f'Test Accuracy: {100 * correct / total:.2f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Key feature dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " Creation of dataset containing important key feature of the entire dataset such as mean, variance for all the data and for only the cheaters or the legit players"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "medie = np.mean(data, axis=(2))\n",
        "varianza = np.mean(data, axis=(2))\n",
        "medie_cheaters = np.mean(data_cheater, axis=(2))\n",
        "varianza_cheaters = np.mean(data_cheater, axis=(2))\n",
        "\n",
        "print(medie)\n",
        "print(medie[0])\n",
        "#concatenazione \n",
        "result=np.stack((medie,varianza), axis=1)\n",
        "result_cheater=np.stack((medie_cheaters,varianza_cheaters), axis=1)\n",
        "total = np.concatenate((result, result_cheater), axis=0)\n",
        "\n",
        "labels_legit = np.zeros(len(result))\n",
        "labels_cheater=np.ones(len(result_cheater))\n",
        "labels = np.concatenate((labels_legit, labels_cheater))\n",
        "\n",
        "#reshape dei dati per la SVM\n",
        "X = total.reshape(12000, -1)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.20, random_state=42, stratify=labels)\n",
        "\n",
        "#rescaling dei dati\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = X_train\n",
        "X_test_scaled = X_test\n",
        "\n",
        "print(X_train_scaled.shape[1])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# MLP model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32).view(-1,1,X_test_scaled.shape[1])\n",
        "X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32).view(-1,1,X_test_scaled.shape[1])\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
        "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "train_data = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
        "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "print(X_train_tensor.shape)\n",
        "print(y_train_tensor.shape)\n",
        "def get_accuracy(logit, target):\n",
        "    corrects = (torch.max(logit, 1)[1].view(target.size()).data == target.data).sum()\n",
        "    accuracy = 100.0 * corrects / target.size(0)\n",
        "    return accuracy.item()\n",
        "\n",
        "def compute_weight_norm(model):\n",
        "    norm = 0.0\n",
        "    for name, param in model.named_parameters():\n",
        "      if \"weight\" in name:\n",
        "        norm += torch.norm(param.data, p= 2)\n",
        "    return norm.cpu().item()\n",
        "\n",
        "def train_model(model, num_epochs, trainloader, criterion, optimizer):\n",
        "\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "    model = model.to(device)\n",
        "\n",
        "    losses = []\n",
        "    accs = []\n",
        "\n",
        "    norms = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        train_running_loss = 0.0\n",
        "        train_acc = 0.0\n",
        "\n",
        "        \n",
        "        model.train()\n",
        "        ## training step\n",
        "        for i, (inputs, labels) in enumerate(trainloader):\n",
        "\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            # forward + backprop + loss\n",
        "            logits = model(inputs)\n",
        "            loss = criterion(logits, labels)\n",
        "\n",
        "            # Reset the gradients to zero\n",
        "\n",
        "            loss.backward()\n",
        "\n",
        "            # update model params\n",
        "            optimizer.step()\n",
        "\n",
        "            train_running_loss += loss.item()\n",
        "            train_acc += get_accuracy(logits, labels)\n",
        "\n",
        "\n",
        "        losses.append(train_running_loss / i)\n",
        "        accs.append(train_acc/i)\n",
        "\n",
        "        norms.append(compute_weight_norm(model))\n",
        "        model.eval()\n",
        "        print(f\"Epoch: {epoch+1} | Loss: {train_running_loss / i:.4f} | Train Accuracy: {train_acc/i:.4f}\")\n",
        "\n",
        "    return losses, accs, norms\n",
        "\n",
        "#creation of layers and activation function for MLP\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MLP, self).__init__()\n",
        "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=6, kernel_size=10, padding=1)\n",
        "        self.dropout1 = nn.Dropout(p=0.20)\n",
        "\n",
        "        self.conv2 = nn.Conv1d(in_channels=6, out_channels=18, kernel_size=10, padding=1)\n",
        "        self.dropout2 = nn.Dropout(p=0.20)\n",
        "\n",
        "\n",
        "        self.conv3 = nn.Conv1d(in_channels=18, out_channels=30, kernel_size=10, padding=1)\n",
        "        self.dropout3 = nn.Dropout(p=0.20)\n",
        "\n",
        "        self.flattern = nn.Flatten()\n",
        "\n",
        "        self.dropout4 = nn.Dropout(p=0.20)\n",
        "\n",
        "\n",
        "        self.fc1 = nn.Linear(8370, 1000)\n",
        "\n",
        "        self.fc2 = nn.Linear(1000, 500)\n",
        "\n",
        "        self.fc3 = nn.Linear(500,64)\n",
        "\n",
        "        self.fc4 = nn.Linear(64, 2)\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = self.dropout1(x)\n",
        "\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = self.dropout2(x)\n",
        "\n",
        "        x = F.relu(self.conv3(x))\n",
        "        x = self.dropout3(x)\n",
        "\n",
        "        x = x.flatten(start_dim = 1)\n",
        "\n",
        "        x = torch.relu(self.fc1(x))\n",
        "\n",
        "        x = torch.relu(self.fc2(x))\n",
        "\n",
        "        x = torch.relu(self.fc3(x))\n",
        "\n",
        "        x = (self.fc4(x))\n",
        "        return x\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "input_size = X_train.shape[1]\n",
        "output_size = 1\n",
        "\n",
        "model = MLP()\n",
        "\n",
        "#Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "_, _, W_adam = train_model(model, 100, train_loader, criterion, optimizer)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.ensemble import IsolationForest\n",
        "\n",
        "modelisolationforest = IsolationForest(contamination=0.18, random_state=42)\n",
        "modelisolationforest.fit(X_train_scaled)\n",
        "\n",
        "y_train_pred_if = modelisolationforest.predict(X_train_scaled)\n",
        "y_test_pred_if = modelisolationforest.predict(X_test_scaled)\n",
        "\n",
        "y_train_pred_if = np.where(y_train_pred_if == -1,1,0)\n",
        "y_test_pred_if = np.where(y_test_pred_if == -1,1,0)\n",
        "\n",
        "train_accuracy_if = accuracy_score(y_train,y_train_pred_if)\n",
        "test_accuracy_if = accuracy_score(y_test,y_test_pred_if)\n",
        "\n",
        "print(train_accuracy_if)\n",
        "print(test_accuracy_if)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "medie = np.mean(data, axis=(2))\n",
        "varianza = np.mean(data, axis=(2))\n",
        "medie_cheaters = np.mean(data_cheater, axis=(2))\n",
        "varianza_cheaters = np.mean(data_cheater, axis=(2))\n",
        "\n",
        "print(medie)\n",
        "print(medie[0])\n",
        "#concatenazione \n",
        "result=np.stack((medie,varianza), axis=1)\n",
        "result_cheater=np.stack((medie_cheaters,varianza_cheaters), axis=1)\n",
        "total = np.concatenate((result, result_cheater), axis=0)\n",
        "\n",
        "labels_legit = np.zeros(len(result))\n",
        "labels_cheater=np.ones(len(result_cheater))\n",
        "labels = np.concatenate((labels_legit, labels_cheater))\n",
        "\n",
        "#reshape dei dati per la SVM\n",
        "X = total.reshape(12000, -1)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.20, random_state=42, stratify=labels)\n",
        "\n",
        "#rescaling dei dati\n",
        "scaler = MinMaxScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.fit_transform(X_test)\n",
        "\n",
        "print(X_train_scaled.shape[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
        "X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
        "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "train_data = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
        "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "print(X_train_tensor.shape)\n",
        "print(y_train_tensor.shape)\n",
        "def get_accuracy(logit, target):\n",
        "    corrects = (torch.max(logit, 1)[1].view(target.size()).data == target.data).sum()\n",
        "    accuracy = 100.0 * corrects / target.size(0)\n",
        "    return accuracy.item()\n",
        "\n",
        "def compute_weight_norm(model):\n",
        "    norm = 0.0\n",
        "    for name, param in model.named_parameters():\n",
        "      if \"weight\" in name:\n",
        "        norm += torch.norm(param.data, p= 2)\n",
        "    return norm.cpu().item()\n",
        "\n",
        "def train_model(model, num_epochs, trainloader, criterion, optimizer):\n",
        "\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    model = model.to(device)\n",
        "\n",
        "    losses = []\n",
        "    accs = []\n",
        "\n",
        "    norms = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        train_running_loss = 0.0\n",
        "        train_acc = 0.0\n",
        "\n",
        "        # Set the model to training mode\n",
        "        model.train()\n",
        "        ## training step\n",
        "        for i, (inputs, labels) in enumerate(trainloader):\n",
        "\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            ## forward + backprop + loss\n",
        "            logits = model(inputs)\n",
        "            loss = criterion(logits, labels)\n",
        "\n",
        "            # Reset the gradients to zero\n",
        "\n",
        "            loss.backward()\n",
        "\n",
        "            ## update model params\n",
        "            optimizer.step()\n",
        "\n",
        "            train_running_loss += loss.item()\n",
        "            train_acc += get_accuracy(logits, labels)\n",
        "\n",
        "\n",
        "        losses.append(train_running_loss / i)\n",
        "        accs.append(train_acc/i)\n",
        "\n",
        "        norms.append(compute_weight_norm(model))\n",
        "        model.eval()\n",
        "        print(f\"Epoch: {epoch+1} | Loss: {train_running_loss / i:.4f} | Train Accuracy: {train_acc/i:.4f}\")\n",
        "\n",
        "    return losses, accs, norms\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MLP, self).__init__()\n",
        "        self.fc1 = nn.Linear(300, 1024)\n",
        "\n",
        "        self.fc2 = nn.Linear(1024, 500)\n",
        "\n",
        "        self.fc3 = nn.Linear(500, 256)\n",
        "\n",
        "        self.fc4 = nn.Linear(256, 128)\n",
        "\n",
        "        self.fc5 = nn.Linear(128, 64)\n",
        "\n",
        "        self.fc6 = nn.Linear(64, 32)\n",
        "\n",
        "        self.fc7 = nn.Linear(32, 8)\n",
        "\n",
        "        self.fc8 = nn.Linear(8, 2)\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = torch.relu(self.fc1(x))\n",
        "\n",
        "        x = torch.relu(self.fc2(x))\n",
        "\n",
        "        x = torch.relu(self.fc3(x))\n",
        "\n",
        "        x = torch.relu(self.fc4(x))\n",
        "\n",
        "        x = torch.relu(self.fc5(x))\n",
        "\n",
        "        x = torch.relu(self.fc6(x))\n",
        "\n",
        "        x = torch.relu(self.fc7(x))\n",
        "\n",
        "        x = torch.sigmoid(self.fc8(x))\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "input_size = X_train.shape[1]\n",
        "output_size = 1\n",
        "print(input_size)\n",
        "\n",
        "model = MLP()\n",
        "\n",
        "class_weights = torch.tensor([1.0, 5.0])\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1)\n",
        "\n",
        "\n",
        "\n",
        "_, _, W_adam = train_model(model, 100, train_loader, criterion, optimizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MS3gME1w-3xv",
        "outputId": "ca0088e3-ace5-49d0-e743-07ec886fad37"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "total = np.concatenate((data, data_cheater), axis=0)\n",
        "\n",
        "labels_legit = np.zeros(len(data))\n",
        "labels_cheater=np.ones(len(data_cheater))\n",
        "labels = np.concatenate((labels_legit, labels_cheater))\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(total, labels, test_size=0.20, random_state=42, stratify=labels)\n",
        "\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train.reshape(-1, X_train.shape[-1])).reshape(X_train.shape)\n",
        "X_test_scaled = scaler.fit_transform(X_test.reshape(-1, X_test.shape[-1])).reshape(X_test.shape)\n",
        "\n",
        "print(X_train_scaled.shape[1])\n",
        "print(X_train_scaled.shape)\n",
        "\n",
        "X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
        "X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)\n",
        "y_test_tensor = torch.tensor(y_test, dtype=torch.float32).unsqueeze(1)\n",
        "\n",
        "train_data = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
        "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "print(X_train_tensor.shape)\n",
        "print(y_train_tensor.shape)\n",
        "\n",
        "class MLP(nn.Module):\n",
        "  def __init__(self):\n",
        "        super(MLP, self).__init__()\n",
        "        self.conv1 = nn.Conv3d(in_channels=1, out_channels=64, kernel_size=(3,3,3), padding=1)\n",
        "        self.conv2 = nn.Conv3d(in_channels=64, out_channels=128, kernel_size=(3,3,3), padding=1)\n",
        "        self.pool = nn.MaxPool3d(kernel_size=(2,2,2), stride=2)\n",
        "        self.dropout = nn.Dropout(p=0.5)\n",
        "        self.fc1 = nn.Linear(43008, 256)\n",
        "        self.fc2 = nn.Linear(256, 1)\n",
        "        self.relu = nn.ReLU()\n",
        "  def forward(self, x):\n",
        "        x = self.relu(self.conv1(x))\n",
        "        x = self.pool(x)\n",
        "        x = self.relu(self.conv2(x))\n",
        "        x = self.pool(x)\n",
        "        x = x.view(x.size(0), -1)  # Flatten the tensor\n",
        "        x = self.dropout(self.relu(self.fc1(x)))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "model = MLP()\n",
        "\n",
        "class_weights = torch.tensor([1.0, 5.0])\n",
        "criterion = nn.BCEWithLogitsLoss(pos_weight=class_weights[1])\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "lHSNz0-DD7n_",
        "outputId": "b84b6105-5893-4dbf-b541-ca56e269be47"
      },
      "outputs": [],
      "source": [
        "num_epochs = 100\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs = inputs.unsqueeze(1)\n",
        "        labels = labels.squeeze(1)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(inputs).squeeze(1)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    if (epoch+1) % 10 == 0:\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# Valutare il modello\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "      correct = 0\n",
        "      total = 0\n",
        "      all_labels = []\n",
        "      all_predicted = []\n",
        "      for inputs, labels in test_loader:\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        predicted = torch.sigmoid(outputs) > 0.5\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "        all_labels.extend(labels.numpy())\n",
        "        all_predicted.extend(predicted.numpy())\n",
        "\n",
        "    accuracy = correct / total\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Accuracy: {accuracy:.4f}')\n",
        "    print(classification_report(all_labels, all_predicted, target_names=['legit', 'cheater']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GYSOcsdJrVmG",
        "outputId": "7e58825d-0f55-4e72-c1e9-eeb0ab4875e0"
      },
      "outputs": [],
      "source": [
        "#tesi\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random as rnd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "\n",
        "\n",
        "#caricamento dati\n",
        "data = np.load(\"/content/drive/My Drive/Colab Notebooks/legit.npy\")\n",
        "data_cheater = np.load(\"/content/drive/My Drive/Colab Notebooks/cheaters.npy\")\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "#calcolo medie e varianza per legit e cheater\n",
        "medie = np.mean(data, axis=(1,2))\n",
        "varianza = np.mean(data, axis=(1,2))\n",
        "medie_cheaters = np.mean(data_cheater, axis=(1,2))\n",
        "varianza_cheaters = np.mean(data_cheater, axis=(1,2))\n",
        "\n",
        "#concatenazione\n",
        "result=np.stack((medie,varianza), axis=1)\n",
        "result_cheater=np.stack((medie_cheaters,varianza_cheaters), axis=1)\n",
        "total = np.concatenate((data, data_cheater), axis=0)\n",
        "\n",
        "labels_legit = np.zeros(len(result))\n",
        "labels_cheater=np.ones(len(result_cheater))\n",
        "labels = np.concatenate((labels_legit, labels_cheater))\n",
        "\n",
        "#reshape dei dati per la SVM\n",
        "X = total.reshape(12000, -1)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.20, random_state=42, stratify=labels)\n",
        "\n",
        "#rescaling dei dati\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.fit_transform(X_test)\n",
        "\n",
        "print(X_train_scaled.shape[1])\n",
        "#modello svm linear\n",
        "# svm = SVC(kernel = 'linear')\n",
        "# svm.fit(X_train_scaled, y_train)\n",
        "# #predicition\n",
        "# y_pred=svm.predict(X_test_scaled)\n",
        "\n",
        "#accuracy\n",
        "# acc = accuracy_score(y_test, y_pred)\n",
        "# print(acc)\n",
        "# cm_lin = confusion_matrix(y_test, y_pred)\n",
        "# disp_lin = ConfusionMatrixDisplay(confusion_matrix=cm_lin, display_labels=[\"Non cheater\", \"cheater\"])\n",
        "# disp_lin.plot()\n",
        "# plt.show()\n",
        "\n",
        "#modello svm gaussian\n",
        "# svm_rbf = SVC(kernel = 'rbf', gamma=20)\n",
        "# svm_rbf.fit(X_train_scaled, y_train)\n",
        "# #predicition\n",
        "# y_pred_rbf=svm_rbf.predict(X_test_scaled)\n",
        "\n",
        "#accuracy\n",
        "# acc_rbf = accuracy_score(y_test, y_pred_rbf)\n",
        "# print(acc_rbf)\n",
        "# cm_rbf = confusion_matrix(y_test, y_pred_rbf)\n",
        "# disp = ConfusionMatrixDisplay(confusion_matrix=cm_rbf, display_labels=[\"Non cheater\", \"cheater\"])\n",
        "# disp.plot()\n",
        "# plt.show()\n",
        "\n",
        "\n",
        "X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32).view(-1,1,X_test_scaled.shape[1])\n",
        "X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32).view(-1,1,X_test_scaled.shape[1])\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
        "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "train_data = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
        "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "print(X_train_tensor.shape)\n",
        "print(y_train_tensor.shape)\n",
        "def get_accuracy(logit, target):\n",
        "    corrects = (torch.max(logit, 1)[1].view(target.size()).data == target.data).sum()\n",
        "    accuracy = 100.0 * corrects / target.size(0)\n",
        "    return accuracy.item()\n",
        "\n",
        "def compute_weight_norm(model):\n",
        "    norm = 0.0\n",
        "    for name, param in model.named_parameters():\n",
        "      if \"weight\" in name:\n",
        "        norm += torch.norm(param.data, p= 2)\n",
        "    return norm.cpu().item()\n",
        "\n",
        "def train_model(model, num_epochs, trainloader, criterion, optimizer):\n",
        "\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Rich mac user\n",
        "    # device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
        "\n",
        "    model = model.to(device)\n",
        "\n",
        "    losses = []\n",
        "    accs = []\n",
        "\n",
        "    norms = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        train_running_loss = 0.0\n",
        "        train_acc = 0.0\n",
        "\n",
        "        # Set the model to training mode\n",
        "        model.train()\n",
        "        ## training step\n",
        "        for i, (inputs, labels) in enumerate(trainloader):\n",
        "\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            ## forward + backprop + loss\n",
        "            logits = model(inputs)\n",
        "            loss = criterion(logits, labels)\n",
        "\n",
        "            # Reset the gradients to zero\n",
        "\n",
        "            loss.backward()\n",
        "\n",
        "            ## update model params\n",
        "            optimizer.step()\n",
        "\n",
        "            train_running_loss += loss.item()\n",
        "            train_acc += get_accuracy(logits, labels)\n",
        "\n",
        "\n",
        "        losses.append(train_running_loss / i)\n",
        "        accs.append(train_acc/i)\n",
        "\n",
        "        norms.append(compute_weight_norm(model))\n",
        "        model.eval()\n",
        "        print(f\"Epoch: {epoch+1} | Loss: {train_running_loss / i:.4f} | Train Accuracy: {train_acc/i:.4f}\")\n",
        "\n",
        "    return losses, accs, norms\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MLP, self).__init__()\n",
        "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=6, kernel_size=10)\n",
        "        self.dropout1 = nn.Dropout(p=0.75)\n",
        "\n",
        "        self.conv2 = nn.Conv1d(in_channels=6, out_channels=6, kernel_size=10)\n",
        "        self.dropout2 = nn.Dropout(p=0.75)\n",
        "\n",
        "\n",
        "        self.conv3 = nn.Conv1d(in_channels=6, out_channels=6, kernel_size=10)\n",
        "        self.dropout3 = nn.Dropout(p=0.75)\n",
        "\n",
        "        self.flattern = nn.Flatten()\n",
        "\n",
        "        self.dropout4 = nn.Dropout(p=0.75)\n",
        "\n",
        "\n",
        "        self.fc1 = nn.Linear(172638, 2)\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = self.dropout1(x)\n",
        "\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = self.dropout2(x)\n",
        "\n",
        "        x = F.relu(self.conv3(x))\n",
        "        x = self.dropout3(x)\n",
        "\n",
        "        x = x.flatten(start_dim = 1)\n",
        "\n",
        "        #x = self.dropout4\n",
        "\n",
        "        x = torch.sigmoid(self.fc1(x))\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "input_size = X_train.shape[1]\n",
        "output_size = 1\n",
        "\n",
        "model = MLP()\n",
        "\n",
        "# class_weights = torch.tensor([1.0, 5.0])\n",
        "# criterion = nn.BCEWithLogitsLoss(pos_weight=class_weights[1])\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
        "\n",
        "_, _, W_adam = train_model(model, 100, train_loader, criterion, optimizer)\n",
        "\n",
        "\n",
        "# Addestrare il modello\n",
        "# num_epochs = 100\n",
        "# for epoch in range(num_epochs):\n",
        "#     model.train()\n",
        "#     for inputs, labels in train_loader:\n",
        "\n",
        "#         optimizer.zero_grad()\n",
        "\n",
        "#         outputs = model(inputs)\n",
        "#         loss = criterion(outputs, labels)\n",
        "\n",
        "\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "\n",
        "#     if (epoch+1) % 10 == 0:\n",
        "#         print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# # Valutare il modello\n",
        "#     model.eval()\n",
        "\n",
        "#     with torch.no_grad():\n",
        "#       correct = 0\n",
        "#       total = 0\n",
        "#       all_labels = []\n",
        "#       all_predicted = []\n",
        "#       for inputs, labels in test_loader:\n",
        "\n",
        "#         outputs = model(inputs)\n",
        "#         predicted = torch.sigmoid(outputs) > 0.5\n",
        "#         total += labels.size(0)\n",
        "#         correct += (predicted == labels).sum().item()\n",
        "#         all_labels.extend(labels.numpy())\n",
        "#         all_predicted.extend(predicted.numpy())\n",
        "\n",
        "#     accuracy = correct / total\n",
        "#     print(f'Epoch [{epoch+1}/{num_epochs}], Accuracy: {accuracy:.4f}')\n",
        "#     print(classification_report(all_labels, all_predicted, target_names=['legit', 'cheater']))\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
